{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running main.py\n",
      "path to XLE data: historical_data/XLE/XLE.csv\n",
      "Historical data XLE is already available on disk. Therefore not downloading.\n",
      "Calculating RSI\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36d08f1336b4dc0973c410f1d867ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcserafin/OneDrive/Business-Analytics_WS2021_OneDrive/Master Thesis/cnnForTrading/cnn-for-trading/technical_indicators.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['rsi_' + str(period)][1:] = res\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of RSI Done 4.0 mins, 0.0 secs\n",
      "Calculating WilliamR\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0281c47d31a04a8bb8f5d32c3b96fe13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of WilliamR Done 0.0 mins, 0.0 secs\n",
      "Calculating MFI\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da1b6f9ae524c2e92866969d4b29a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of MFI done 0.0 mins, 5.0 secs\n",
      "Calculating ROC\n",
      "Calculation of ROC done 0.0 mins, 15.0 secs\n",
      "Calculating CMF\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbd92c1b25849e7be23ae412b8c61d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of CMF done 0.0 mins, 0.0 secs\n",
      "Calculating CMO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcserafin/OneDrive/Business-Analytics_WS2021_OneDrive/Master Thesis/cnnForTrading/cnn-for-trading/technical_indicators.py:493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['cmf_' + str(i)] = chaikin_money_flow(df['high'], df['low'], df['close'], df['volume'], window=i, fillna=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907e750c71d640f6b127e3332dee4cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcserafin/OneDrive/Business-Analytics_WS2021_OneDrive/Master Thesis/cnnForTrading/cnn-for-trading/technical_indicators.py:366: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['cmo_' + str(period)] = np.nan\n",
      "/Users/marcserafin/OneDrive/Business-Analytics_WS2021_OneDrive/Master Thesis/cnnForTrading/cnn-for-trading/technical_indicators.py:368: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cmo_' + str(period)][1:] = res\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of CMO Done 2.0 mins, 17.0 secs\n",
      "Calculating SMA\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a3f7d2a7d440c6ac3ef748d2d018d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of SMA Done 0.0 mins, 0.0 secs\n",
      "Calculating SMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/stockstats.py:782: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df[column].rolling(min_periods=1, window=window,\n",
      "/Users/marcserafin/OneDrive/Business-Analytics_WS2021_OneDrive/Master Thesis/cnnForTrading/cnn-for-trading/technical_indicators.py:167: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name + '_sma_' + str(i)] = df_ss[col_name + '_' + str(i) + '_sma']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cde380d79ed40259bf48b6ad7c75fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of SMA Done 0.0 mins, 0.0 secs\n",
      "Calculating EMA\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db0bef4784b4c6eb5f4723666e56829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of EMA Done 0.0 mins, 0.0 secs\n",
      "Calculating WMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/stockstats.py:797: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = df[column].ewm(\n",
      "/Users/marcserafin/OneDrive/Business-Analytics_WS2021_OneDrive/Master Thesis/cnnForTrading/cnn-for-trading/technical_indicators.py:182: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ema_' + str(i)] = df_ss[col_name + '_' + str(i) + '_ema']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab51dd61c9b4582b59a29e2dcec0536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcserafin/OneDrive/Business-Analytics_WS2021_OneDrive/Master Thesis/cnnForTrading/cnn-for-trading/technical_indicators.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['wma_' + str(i)] = res\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of WMA Done 0.0 mins, 42.0 secs\n",
      "Calculating HMA\n",
      "WMA calculated already. Proceed with HMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcserafin/OneDrive/Business-Analytics_WS2021_OneDrive/Master Thesis/cnnForTrading/cnn-for-trading/technical_indicators.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['hma_wma_' + str(i) + '_' + str(temp_col_count_dict['hma_wma_' + str(i)])] = 2 * res\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da0a94799c844f8a893b080b235a502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of HMA Done 1.0 mins, 22.0 secs\n",
      "Calculating TRIX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffe6f77c0d546358c87b8596637457f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of TRIX Done 0.0 mins, 0.0 secs\n",
      "Calculating CCI\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58156ef84ff4b3a8c59c4e57d7032e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of CCI Done 0.0 mins, 4.0 secs\n",
      "Calculating DPO\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985c29effb25446ba78e6d633d24ff99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of DPO done 0.0 mins, 0.0 secs\n",
      "Calculating KST\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628b0b2d91604a0db023b715df402c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of KST done 0.0 mins, 0.0 secs\n",
      "Calculating DMI\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9c07a2a196493a9fa9885f45ca916b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of DMI done 0.0 mins, 0.0 secs\n",
      "Calculating Bollinger Band MAV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcserafin/OneDrive/Business-Analytics_WS2021_OneDrive/Master Thesis/cnnForTrading/cnn-for-trading/technical_indicators.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['dmi_' + str(i)] = df_ss['adx_' + str(i) + '_ema']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75df7b01835f4307bd3d18c2705d5814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of Bollinger Band MAV done 0.0 mins, 0.0 secs\n",
      "Calculating Force Index\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d1d622902444f98877dbab8bee310e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of Force Index done 0.0 mins, 0.0 secs\n",
      "Calculating KDJK, RSV\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d94f7a58b2043cf80be5c7805686598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of EMA Done 0.0 mins, 0.0 secs\n",
      "Calculating EOM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b8b4d1ee3240bfb63fe2636da20ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of EOM done 0.0 mins, 0.0 secs\n",
      "Saving dataframe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcserafin/OneDrive/Business-Analytics_WS2021_OneDrive/Master Thesis/cnnForTrading/cnn-for-trading/technical_indicators.py:513: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['eom_' + str(i)] = ease_of_movement(df['high'], df['low'], df['volume'], window=i, fillna=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 26 nan rows before label calculation\n",
      "Dropped 0 nan rows after label calculation\n",
      "Number of Technical indicator columns for train/test are 463\n",
      "path to CL=F data: historical_data/CL=F/CL=F.csv\n",
      "Auxiliary data CL=F is already available on disk. Therefore not downloading.\n",
      "path to EURUSD=X data: historical_data/EURUSD=X/EURUSD=X.csv\n",
      "Auxiliary data EURUSD=X is already available on disk. Therefore not downloading.\n",
      "Merging dataframes from XLE and auxiliary CL=F\n",
      "Technical indicators for auxiliary data CL=F already calculated. Will load from disk.\n",
      "Merging dataframes from XLE and auxiliary EURUSD=X\n",
      "Technical indicators for auxiliary data EURUSD=X already calculated. Will load from disk.\n",
      "df_XLE_aux written to disk.\n",
      "common selected featues:291, ['kdjk_26', 'cci_23', 'kdjk_19', 'roc_16', 'kdjk_7', 'wr_21', 'trix_25', 'fi_17', 'dpo_9', 'wma_26', 'trix_10', 'dmi_20', 'fi_15', 'cmo_6', 'rsv_10', 'cmo_17', 'open_sma_13', 'eom_9', 'trix_15', 'rsv_16', 'rsv_26', 'kdjk_12', 'cci_7', 'kdjk_15', 'rsv_24', 'kst_14', 'eom_15', 'dmi_12', 'dpo_17', 'dpo_12', 'kst_11', 'kst_24', 'cmf_19', 'open_23_sma', 'trix_22', 'wr_15', 'eom_8', 'mfi_10', 'open_sma_15', 'dpo_26', 'open_sma_18', 'trix_6', 'kst_6', 'fi_26', 'rsi_9', 'bb_23', 'dmi_18', 'dpo_15', 'open_13_sma', 'open_sma_22', 'cci_18', 'wr_24', 'dmi_24', 'bb_22', 'cmf_20', 'dpo_25', 'bb_17', 'dmi_6', 'dpo_7', 'open_sma_17', 'rsv_8', 'open_sma_23', 'bb_16', 'roc_20', 'rsi_16', 'dmi_17', 'eom_12', 'dmi_26', 'eom_18', 'roc_7', 'dmi_21', 'cci_20', 'wma_24', 'wr_19', 'kst_19', 'bb_21', 'eom_11', 'dpo_16', 'wr_26', 'dmi_15', 'dpo_23', 'trix_18', 'rsi_21', 'cci_8', 'dmi_10', 'fi_20', 'cmo_12', 'wma_19', 'rsv_7', 'bb_20', 'trix_16', 'open_19_sma', 'kst_10', 'cci_24', 'eom_24', 'rsv_12', 'dpo_11', 'eom_6', 'rsv_14', 'wr_12', 'kdjk_8', 'dpo_21', 'cmo_11', 'kst_16', 'open_12_sma', 'dmi_14', 'cmo_10', 'trix_9', 'dmi_7', 'fi_10', 'fi_6', 'trix_26', 'open_16_sma', 'trix_24', 'cci_21', 'rsv_22', 'trix_19', 'kst_8', 'cci_11', 'mfi_11', 'wr_25', 'dmi_8', 'kst_25', 'kst_23', 'roc_26', 'rsv_23', 'eom_22', 'cci_13', 'cmf_10', 'trix_21', 'open_18_sma', 'fi_11', 'dmi_11', 'wma_23', 'kdjk_24', 'kdjk_23', 'cci_15', 'fi_14', 'roc_9', 'dpo_6', 'trix_20', 'wr_11', 'eom_13', 'open_11_sma', 'open_sma_14', 'kdjk_22', 'rsv_6', 'fi_8', 'fi_12', 'open_sma_24', 'fi_22', 'mfi_16', 'open_14_sma', 'mfi_8', 'dpo_13', 'rsv_17', 'rsv_9', 'mfi_9', 'dpo_14', 'dpo_19', 'cci_17', 'cci_26', 'fi_18', 'rsv_18', 'dmi_22', 'fi_25', 'cci_9', 'rsi_14', 'mfi_14', 'cci_16', 'cci_12', 'kdjk_21', 'open_15_sma', 'dmi_13', 'kdjk_6', 'rsv_13', 'roc_19', 'fi_21', 'kst_18', 'cci_6', 'bb_15', 'wma_18', 'trix_23', 'cci_25', 'cci_14', 'wr_8', 'wr_9', 'mfi_6', 'roc_14', 'eom_23', 'cmf_22', 'eom_26', 'kst_7', 'kdjk_9', 'roc_8', 'wma_25', 'bb_13', 'roc_15', 'wr_23', 'open_sma_12', 'wr_18', 'kst_13', 'fi_24', 'wr_22', 'eom_10', 'cmf_12', 'open_17_sma', 'cmo_7', 'cci_10', 'dpo_18', 'kst_9', 'eom_19', 'wr_13', 'kst_17', 'cmf_23', 'kst_12', 'cmo_15', 'dmi_23', 'fi_23', 'cmf_13', 'eom_14', 'dmi_25', 'bb_18', 'open_24_sma', 'rsv_11', 'kdjk_13', 'bb_14', 'eom_16', 'kst_21', 'kdjk_18', 'kdjk_20', 'cmo_8', 'open_sma_19', 'dpo_8', 'kst_26', 'rsi_7', 'rsv_15', 'rsv_19', 'wr_16', 'open_sma_11', 'kst_20', 'roc_11', 'cmf_7', 'eom_7', 'fi_7', 'dmi_16', 'dmi_19', 'wma_22', 'kdjk_11', 'trix_14', 'dmi_9', 'kdjk_17', 'open_22_sma', 'rsi_8', 'open_sma_16', 'fi_9', 'fi_19', 'kst_22', 'kdjk_25', 'wr_17', 'cci_22', 'trix_13', 'kdjk_14', 'wr_14', 'trix_12', 'rsi_6', 'fi_13', 'rsv_25', 'eom_17', 'rsv_21', 'cmf_18', 'bb_19', 'eom_21', 'rsi_10', 'cmf_24', 'eom_20', 'trix_17', 'dpo_22', 'wr_10', 'dpo_24', 'fi_16', 'rsv_20', 'kdjk_16', 'kdjk_10', 'mfi_13', 'wma_20', 'dpo_20', 'kst_15', 'dpo_10', 'cci_19', 'eom_25']\n",
      "[8, 13, 15, 20, 28, 29, 31, 32, 33, 35, 38, 39, 41, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 55, 57, 69, 70, 71, 76, 77, 78, 81, 82, 88, 93, 95, 96, 102, 103, 105, 106, 110, 111, 114, 115, 116, 119, 121, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 164, 165, 166, 167, 168, 227, 228, 232, 233, 234, 235, 257, 260, 261, 266, 267, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 292, 293, 295, 296, 297, 298, 299, 300, 302, 304, 305, 306, 307, 308, 309, 310, 311, 312, 314, 316, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 330, 331, 332, 333, 337, 338, 339, 341, 342, 343, 345, 346, 347, 348, 349, 350, 352, 353, 355, 356, 357, 358, 359, 360, 361, 369, 371, 372, 373, 374, 376, 377, 378, 379, 383, 385, 387, 388, 389, 391, 392, 394, 395, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 414, 416, 417, 418, 420, 423, 424, 426, 428, 431, 435, 436, 437, 438, 439, 440, 441, 444, 445, 446, 448, 449, 450, 451, 452, 453, 454, 455, 458, 459, 462, 463, 464, 466]\n",
      "historical_data has data for 2004-01-06 00:00:00 to 2021-09-29 00:00:00\n"
     ]
    }
   ],
   "source": [
    "%run ./main.py\n",
    "\n",
    "# uncomment to run main.py to generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>unnamed: 0</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi_6</th>\n",
       "      <th>rsi_7</th>\n",
       "      <th>...</th>\n",
       "      <th>eom_18_EURUSD=X</th>\n",
       "      <th>eom_19_EURUSD=X</th>\n",
       "      <th>eom_20_EURUSD=X</th>\n",
       "      <th>eom_21_EURUSD=X</th>\n",
       "      <th>eom_22_EURUSD=X</th>\n",
       "      <th>eom_23_EURUSD=X</th>\n",
       "      <th>eom_24_EURUSD=X</th>\n",
       "      <th>eom_25_EURUSD=X</th>\n",
       "      <th>eom_26_EURUSD=X</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>810</td>\n",
       "      <td>1265</td>\n",
       "      <td>2004-01-06</td>\n",
       "      <td>17.814430</td>\n",
       "      <td>17.833448</td>\n",
       "      <td>17.662278</td>\n",
       "      <td>17.776392</td>\n",
       "      <td>691100</td>\n",
       "      <td>91.334101</td>\n",
       "      <td>81.891705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>811</td>\n",
       "      <td>1266</td>\n",
       "      <td>2004-01-07</td>\n",
       "      <td>17.681286</td>\n",
       "      <td>17.681286</td>\n",
       "      <td>17.484757</td>\n",
       "      <td>17.592531</td>\n",
       "      <td>1676800</td>\n",
       "      <td>72.556789</td>\n",
       "      <td>77.477096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>812</td>\n",
       "      <td>1267</td>\n",
       "      <td>2004-01-08</td>\n",
       "      <td>17.611557</td>\n",
       "      <td>17.674953</td>\n",
       "      <td>17.459406</td>\n",
       "      <td>17.643255</td>\n",
       "      <td>1334900</td>\n",
       "      <td>63.748235</td>\n",
       "      <td>68.320334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>813</td>\n",
       "      <td>1268</td>\n",
       "      <td>2004-01-09</td>\n",
       "      <td>17.592536</td>\n",
       "      <td>17.877821</td>\n",
       "      <td>17.510121</td>\n",
       "      <td>17.732008</td>\n",
       "      <td>2369800</td>\n",
       "      <td>51.689542</td>\n",
       "      <td>63.117682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>814</td>\n",
       "      <td>1269</td>\n",
       "      <td>2004-01-12</td>\n",
       "      <td>17.789070</td>\n",
       "      <td>17.839787</td>\n",
       "      <td>17.719335</td>\n",
       "      <td>17.782730</td>\n",
       "      <td>820000</td>\n",
       "      <td>61.661799</td>\n",
       "      <td>64.749230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>815</td>\n",
       "      <td>1270</td>\n",
       "      <td>2004-01-13</td>\n",
       "      <td>17.846125</td>\n",
       "      <td>17.960239</td>\n",
       "      <td>17.751031</td>\n",
       "      <td>17.789068</td>\n",
       "      <td>1003000</td>\n",
       "      <td>64.006560</td>\n",
       "      <td>62.934676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>816</td>\n",
       "      <td>1271</td>\n",
       "      <td>2004-01-14</td>\n",
       "      <td>17.782722</td>\n",
       "      <td>17.782722</td>\n",
       "      <td>17.630570</td>\n",
       "      <td>17.744684</td>\n",
       "      <td>830200</td>\n",
       "      <td>62.991169</td>\n",
       "      <td>58.111141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>817</td>\n",
       "      <td>1272</td>\n",
       "      <td>2004-01-15</td>\n",
       "      <td>17.782727</td>\n",
       "      <td>17.827105</td>\n",
       "      <td>17.440386</td>\n",
       "      <td>17.446726</td>\n",
       "      <td>1024100</td>\n",
       "      <td>47.058603</td>\n",
       "      <td>63.999377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>818</td>\n",
       "      <td>1273</td>\n",
       "      <td>2004-01-16</td>\n",
       "      <td>17.560844</td>\n",
       "      <td>17.655938</td>\n",
       "      <td>17.421370</td>\n",
       "      <td>17.611561</td>\n",
       "      <td>355600</td>\n",
       "      <td>37.736816</td>\n",
       "      <td>31.788480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>819</td>\n",
       "      <td>1274</td>\n",
       "      <td>2004-01-20</td>\n",
       "      <td>17.782731</td>\n",
       "      <td>18.074355</td>\n",
       "      <td>17.719335</td>\n",
       "      <td>18.055336</td>\n",
       "      <td>956200</td>\n",
       "      <td>63.076888</td>\n",
       "      <td>57.807452</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>820</td>\n",
       "      <td>1275</td>\n",
       "      <td>2004-01-21</td>\n",
       "      <td>18.055340</td>\n",
       "      <td>18.232851</td>\n",
       "      <td>18.010963</td>\n",
       "      <td>18.163115</td>\n",
       "      <td>748000</td>\n",
       "      <td>73.776387</td>\n",
       "      <td>72.281057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>821</td>\n",
       "      <td>1276</td>\n",
       "      <td>2004-01-22</td>\n",
       "      <td>18.258202</td>\n",
       "      <td>18.289901</td>\n",
       "      <td>17.960238</td>\n",
       "      <td>18.074352</td>\n",
       "      <td>425100</td>\n",
       "      <td>73.591356</td>\n",
       "      <td>77.537359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>822</td>\n",
       "      <td>1277</td>\n",
       "      <td>2004-01-23</td>\n",
       "      <td>18.099702</td>\n",
       "      <td>18.321590</td>\n",
       "      <td>18.099702</td>\n",
       "      <td>18.232836</td>\n",
       "      <td>1697300</td>\n",
       "      <td>59.458915</td>\n",
       "      <td>61.604253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>823</td>\n",
       "      <td>1278</td>\n",
       "      <td>2004-01-26</td>\n",
       "      <td>18.131406</td>\n",
       "      <td>18.378652</td>\n",
       "      <td>18.099707</td>\n",
       "      <td>18.340614</td>\n",
       "      <td>683900</td>\n",
       "      <td>65.909133</td>\n",
       "      <td>62.331539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>824</td>\n",
       "      <td>1279</td>\n",
       "      <td>2004-01-27</td>\n",
       "      <td>18.384993</td>\n",
       "      <td>18.454729</td>\n",
       "      <td>18.258199</td>\n",
       "      <td>18.302578</td>\n",
       "      <td>754500</td>\n",
       "      <td>73.094061</td>\n",
       "      <td>72.932315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>825</td>\n",
       "      <td>1280</td>\n",
       "      <td>2004-01-28</td>\n",
       "      <td>18.289901</td>\n",
       "      <td>18.334279</td>\n",
       "      <td>18.023636</td>\n",
       "      <td>18.042654</td>\n",
       "      <td>940500</td>\n",
       "      <td>78.282622</td>\n",
       "      <td>66.666751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>826</td>\n",
       "      <td>1281</td>\n",
       "      <td>2004-01-29</td>\n",
       "      <td>18.163114</td>\n",
       "      <td>18.207491</td>\n",
       "      <td>17.941226</td>\n",
       "      <td>18.029982</td>\n",
       "      <td>3247100</td>\n",
       "      <td>65.217596</td>\n",
       "      <td>70.992508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>827</td>\n",
       "      <td>1282</td>\n",
       "      <td>2004-01-30</td>\n",
       "      <td>18.004611</td>\n",
       "      <td>18.055328</td>\n",
       "      <td>17.820761</td>\n",
       "      <td>17.858799</td>\n",
       "      <td>613500</td>\n",
       "      <td>46.107128</td>\n",
       "      <td>57.370060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>828</td>\n",
       "      <td>1283</td>\n",
       "      <td>2004-02-02</td>\n",
       "      <td>17.903188</td>\n",
       "      <td>18.049000</td>\n",
       "      <td>17.763716</td>\n",
       "      <td>17.960245</td>\n",
       "      <td>938900</td>\n",
       "      <td>30.161415</td>\n",
       "      <td>42.619959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>829</td>\n",
       "      <td>1284</td>\n",
       "      <td>2004-02-03</td>\n",
       "      <td>17.846124</td>\n",
       "      <td>17.966577</td>\n",
       "      <td>17.795406</td>\n",
       "      <td>17.890501</td>\n",
       "      <td>434500</td>\n",
       "      <td>34.143106</td>\n",
       "      <td>28.754202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>830</td>\n",
       "      <td>1285</td>\n",
       "      <td>2004-02-04</td>\n",
       "      <td>17.846121</td>\n",
       "      <td>17.877820</td>\n",
       "      <td>17.662272</td>\n",
       "      <td>17.763706</td>\n",
       "      <td>536000</td>\n",
       "      <td>32.000012</td>\n",
       "      <td>34.615795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>831</td>\n",
       "      <td>1286</td>\n",
       "      <td>2004-02-05</td>\n",
       "      <td>17.744697</td>\n",
       "      <td>17.782734</td>\n",
       "      <td>17.446733</td>\n",
       "      <td>17.605225</td>\n",
       "      <td>1040200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.842618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>832</td>\n",
       "      <td>1287</td>\n",
       "      <td>2004-02-06</td>\n",
       "      <td>17.630579</td>\n",
       "      <td>17.725674</td>\n",
       "      <td>17.529145</td>\n",
       "      <td>17.725674</td>\n",
       "      <td>515900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>833</td>\n",
       "      <td>1288</td>\n",
       "      <td>2004-02-09</td>\n",
       "      <td>17.751032</td>\n",
       "      <td>18.004618</td>\n",
       "      <td>17.751032</td>\n",
       "      <td>17.941221</td>\n",
       "      <td>739300</td>\n",
       "      <td>21.348135</td>\n",
       "      <td>17.569339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>834</td>\n",
       "      <td>1289</td>\n",
       "      <td>2004-02-10</td>\n",
       "      <td>17.972915</td>\n",
       "      <td>18.220161</td>\n",
       "      <td>17.947556</td>\n",
       "      <td>18.182123</td>\n",
       "      <td>941800</td>\n",
       "      <td>50.833431</td>\n",
       "      <td>41.598479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>835</td>\n",
       "      <td>1290</td>\n",
       "      <td>2004-02-11</td>\n",
       "      <td>18.131411</td>\n",
       "      <td>18.448394</td>\n",
       "      <td>18.087034</td>\n",
       "      <td>18.429375</td>\n",
       "      <td>2014100</td>\n",
       "      <td>66.141440</td>\n",
       "      <td>58.499957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>836</td>\n",
       "      <td>1291</td>\n",
       "      <td>2004-02-12</td>\n",
       "      <td>18.416691</td>\n",
       "      <td>18.556164</td>\n",
       "      <td>18.334276</td>\n",
       "      <td>18.486427</td>\n",
       "      <td>782900</td>\n",
       "      <td>79.640872</td>\n",
       "      <td>75.357759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>837</td>\n",
       "      <td>1292</td>\n",
       "      <td>2004-02-13</td>\n",
       "      <td>18.467415</td>\n",
       "      <td>18.518132</td>\n",
       "      <td>18.315263</td>\n",
       "      <td>18.442057</td>\n",
       "      <td>860300</td>\n",
       "      <td>79.714116</td>\n",
       "      <td>79.681597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>838</td>\n",
       "      <td>1293</td>\n",
       "      <td>2004-02-17</td>\n",
       "      <td>18.575179</td>\n",
       "      <td>18.632236</td>\n",
       "      <td>18.524462</td>\n",
       "      <td>18.600538</td>\n",
       "      <td>532700</td>\n",
       "      <td>89.436209</td>\n",
       "      <td>81.704308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>839</td>\n",
       "      <td>1294</td>\n",
       "      <td>2004-02-18</td>\n",
       "      <td>18.619559</td>\n",
       "      <td>18.632237</td>\n",
       "      <td>18.353294</td>\n",
       "      <td>18.404011</td>\n",
       "      <td>754000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>89.723718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 1323 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  unnamed: 0   timestamp       open       high        low  \\\n",
       "0          810        1265  2004-01-06  17.814430  17.833448  17.662278   \n",
       "1          811        1266  2004-01-07  17.681286  17.681286  17.484757   \n",
       "2          812        1267  2004-01-08  17.611557  17.674953  17.459406   \n",
       "3          813        1268  2004-01-09  17.592536  17.877821  17.510121   \n",
       "4          814        1269  2004-01-12  17.789070  17.839787  17.719335   \n",
       "5          815        1270  2004-01-13  17.846125  17.960239  17.751031   \n",
       "6          816        1271  2004-01-14  17.782722  17.782722  17.630570   \n",
       "7          817        1272  2004-01-15  17.782727  17.827105  17.440386   \n",
       "8          818        1273  2004-01-16  17.560844  17.655938  17.421370   \n",
       "9          819        1274  2004-01-20  17.782731  18.074355  17.719335   \n",
       "10         820        1275  2004-01-21  18.055340  18.232851  18.010963   \n",
       "11         821        1276  2004-01-22  18.258202  18.289901  17.960238   \n",
       "12         822        1277  2004-01-23  18.099702  18.321590  18.099702   \n",
       "13         823        1278  2004-01-26  18.131406  18.378652  18.099707   \n",
       "14         824        1279  2004-01-27  18.384993  18.454729  18.258199   \n",
       "15         825        1280  2004-01-28  18.289901  18.334279  18.023636   \n",
       "16         826        1281  2004-01-29  18.163114  18.207491  17.941226   \n",
       "17         827        1282  2004-01-30  18.004611  18.055328  17.820761   \n",
       "18         828        1283  2004-02-02  17.903188  18.049000  17.763716   \n",
       "19         829        1284  2004-02-03  17.846124  17.966577  17.795406   \n",
       "20         830        1285  2004-02-04  17.846121  17.877820  17.662272   \n",
       "21         831        1286  2004-02-05  17.744697  17.782734  17.446733   \n",
       "22         832        1287  2004-02-06  17.630579  17.725674  17.529145   \n",
       "23         833        1288  2004-02-09  17.751032  18.004618  17.751032   \n",
       "24         834        1289  2004-02-10  17.972915  18.220161  17.947556   \n",
       "25         835        1290  2004-02-11  18.131411  18.448394  18.087034   \n",
       "26         836        1291  2004-02-12  18.416691  18.556164  18.334276   \n",
       "27         837        1292  2004-02-13  18.467415  18.518132  18.315263   \n",
       "28         838        1293  2004-02-17  18.575179  18.632236  18.524462   \n",
       "29         839        1294  2004-02-18  18.619559  18.632237  18.353294   \n",
       "\n",
       "        close   volume       rsi_6      rsi_7  ...  eom_18_EURUSD=X  \\\n",
       "0   17.776392   691100   91.334101  81.891705  ...              0.0   \n",
       "1   17.592531  1676800   72.556789  77.477096  ...              0.0   \n",
       "2   17.643255  1334900   63.748235  68.320334  ...              0.0   \n",
       "3   17.732008  2369800   51.689542  63.117682  ...              0.0   \n",
       "4   17.782730   820000   61.661799  64.749230  ...              0.0   \n",
       "5   17.789068  1003000   64.006560  62.934676  ...              0.0   \n",
       "6   17.744684   830200   62.991169  58.111141  ...              0.0   \n",
       "7   17.446726  1024100   47.058603  63.999377  ...              0.0   \n",
       "8   17.611561   355600   37.736816  31.788480  ...              0.0   \n",
       "9   18.055336   956200   63.076888  57.807452  ...              0.0   \n",
       "10  18.163115   748000   73.776387  72.281057  ...              0.0   \n",
       "11  18.074352   425100   73.591356  77.537359  ...              0.0   \n",
       "12  18.232836  1697300   59.458915  61.604253  ...              0.0   \n",
       "13  18.340614   683900   65.909133  62.331539  ...              0.0   \n",
       "14  18.302578   754500   73.094061  72.932315  ...              0.0   \n",
       "15  18.042654   940500   78.282622  66.666751  ...              0.0   \n",
       "16  18.029982  3247100   65.217596  70.992508  ...              0.0   \n",
       "17  17.858799   613500   46.107128  57.370060  ...              0.0   \n",
       "18  17.960245   938900   30.161415  42.619959  ...              0.0   \n",
       "19  17.890501   434500   34.143106  28.754202  ...              0.0   \n",
       "20  17.763706   536000   32.000012  34.615795  ...              0.0   \n",
       "21  17.605225  1040200    0.000000  27.842618  ...              0.0   \n",
       "22  17.725674   515900    0.000000   0.000000  ...              0.0   \n",
       "23  17.941221   739300   21.348135  17.569339  ...              0.0   \n",
       "24  18.182123   941800   50.833431  41.598479  ...              0.0   \n",
       "25  18.429375  2014100   66.141440  58.499957  ...              0.0   \n",
       "26  18.486427   782900   79.640872  75.357759  ...              0.0   \n",
       "27  18.442057   860300   79.714116  79.681597  ...              0.0   \n",
       "28  18.600538   532700   89.436209  81.704308  ...              0.0   \n",
       "29  18.404011   754000  100.000000  89.723718  ...              0.0   \n",
       "\n",
       "    eom_19_EURUSD=X  eom_20_EURUSD=X  eom_21_EURUSD=X  eom_22_EURUSD=X  \\\n",
       "0               0.0              0.0              0.0              0.0   \n",
       "1               0.0              0.0              0.0              0.0   \n",
       "2               0.0              0.0              0.0              0.0   \n",
       "3               0.0              0.0              0.0              0.0   \n",
       "4               0.0              0.0              0.0              0.0   \n",
       "5               0.0              0.0              0.0              0.0   \n",
       "6               0.0              0.0              0.0              0.0   \n",
       "7               0.0              0.0              0.0              0.0   \n",
       "8               0.0              0.0              0.0              0.0   \n",
       "9               0.0              0.0              0.0              0.0   \n",
       "10              0.0              0.0              0.0              0.0   \n",
       "11              0.0              0.0              0.0              0.0   \n",
       "12              0.0              0.0              0.0              0.0   \n",
       "13              0.0              0.0              0.0              0.0   \n",
       "14              0.0              0.0              0.0              0.0   \n",
       "15              0.0              0.0              0.0              0.0   \n",
       "16              0.0              0.0              0.0              0.0   \n",
       "17              0.0              0.0              0.0              0.0   \n",
       "18              0.0              0.0              0.0              0.0   \n",
       "19              0.0              0.0              0.0              0.0   \n",
       "20              0.0              0.0              0.0              0.0   \n",
       "21              0.0              0.0              0.0              0.0   \n",
       "22              0.0              0.0              0.0              0.0   \n",
       "23              0.0              0.0              0.0              0.0   \n",
       "24              0.0              0.0              0.0              0.0   \n",
       "25              0.0              0.0              0.0              0.0   \n",
       "26              0.0              0.0              0.0              0.0   \n",
       "27              0.0              0.0              0.0              0.0   \n",
       "28              0.0              0.0              0.0              0.0   \n",
       "29              0.0              0.0              0.0              0.0   \n",
       "\n",
       "    eom_23_EURUSD=X  eom_24_EURUSD=X  eom_25_EURUSD=X  eom_26_EURUSD=X  labels  \n",
       "0               0.0              0.0              0.0              0.0       0  \n",
       "1               0.0              0.0              0.0              0.0       0  \n",
       "2               0.0              0.0              0.0              0.0       0  \n",
       "3               0.0              0.0              0.0              0.0       0  \n",
       "4               0.0              0.0              0.0              0.0       0  \n",
       "5               0.0              0.0              0.0              0.0       0  \n",
       "6               0.0              0.0              0.0              0.0      -1  \n",
       "7               0.0              0.0              0.0              0.0       0  \n",
       "8               0.0              0.0              0.0              0.0       1  \n",
       "9               0.0              0.0              0.0              0.0       0  \n",
       "10              0.0              0.0              0.0              0.0       0  \n",
       "11              0.0              0.0              0.0              0.0       0  \n",
       "12              0.0              0.0              0.0              0.0       0  \n",
       "13              0.0              0.0              0.0              0.0       0  \n",
       "14              0.0              0.0              0.0              0.0       0  \n",
       "15              0.0              0.0              0.0              0.0       0  \n",
       "16              0.0              0.0              0.0              0.0       0  \n",
       "17              0.0              0.0              0.0              0.0       0  \n",
       "18              0.0              0.0              0.0              0.0       0  \n",
       "19              0.0              0.0              0.0              0.0       0  \n",
       "20              0.0              0.0              0.0              0.0       0  \n",
       "21              0.0              0.0              0.0              0.0       0  \n",
       "22              0.0              0.0              0.0              0.0       0  \n",
       "23              0.0              0.0              0.0              0.0       0  \n",
       "24              0.0              0.0              0.0              0.0       0  \n",
       "25              0.0              0.0              0.0              0.0       0  \n",
       "26              0.0              0.0              0.0              0.0       0  \n",
       "27              0.0              0.0              0.0              0.0       0  \n",
       "28              0.0              0.0              0.0              0.0       0  \n",
       "29              0.0              0.0              0.0              0.0       0  \n",
       "\n",
       "[30 rows x 1323 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# from IPython.core.interactiveshell import InteractiveShell # TODO do we need this?\n",
    "\n",
    "np.random.seed(42)\n",
    "symbol = 'XLE'\n",
    "\n",
    "# after generating dataset in cell above, import dataset\n",
    "df = pd.read_csv(\"data/df_\"+symbol+\"_aux.csv\")\n",
    "df = df[df['timestamp'] < '2016-01-01']  # For backtesting the last 5 years will be excluded from training.\n",
    "df['labels'] = df['labels'].astype(np.int8)\n",
    "# if 'dividend_amount' in df.columns:\n",
    "#   df.drop(columns=['dividend_amount', 'split_coefficient'], inplace=True)\n",
    "display(df.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['kdjk_15', 'rsv_16', 'kdjk_16', 'rsv_17', 'kdjk_17', 'rsv_18',\n",
       "       'kdjk_18', 'rsv_19', 'kdjk_19', 'rsv_20',\n",
       "       ...\n",
       "       'eom_18_EURUSD=X', 'eom_19_EURUSD=X', 'eom_20_EURUSD=X',\n",
       "       'eom_21_EURUSD=X', 'eom_22_EURUSD=X', 'eom_23_EURUSD=X',\n",
       "       'eom_24_EURUSD=X', 'eom_25_EURUSD=X', 'eom_26_EURUSD=X', 'labels'],\n",
       "      dtype='object', length=897)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[426:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features 1319\n",
      "Shape of x, y train/cv/test (1899, 1319) (1899,) (475, 1319) (475,) (594, 1319) (594,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "#[:, 'open':'eom_26']\n",
    "list_features = list(df.loc[:, 'open':'eom_26_EURUSD=X'].columns)\n",
    "print('Total number of features', len(list_features))\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.loc[:, 'open':'eom_26_EURUSD=X'].values, df['labels'].values, train_size=0.8, \n",
    "                                                    test_size=0.2, random_state=2, shuffle=True, stratify=df['labels'].values)\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, train_size=0.8, test_size=0.2, \n",
    "                                                random_state=2, shuffle=True, stratify=y_train)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) # or StandardScaler?\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_cv = scaler.transform(x_cv)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "x_main = x_train.copy()\n",
    "print(\"Shape of x, y train/cv/test {} {} {} {} {} {}\".format(x_train.shape, y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 225  # should be a perfect square\n",
    "selection_method = 'all'\n",
    "topk = 320 if selection_method == 'all' else num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 898  941  942  943  944  945  946  947  948  949  950  951  952  953\n",
      "  954  955  956  957  958  959  960  961  983  984  985  986  987  988\n",
      "  989  990  991  992  993  994  995  996  997  998  999 1000 1001 1002\n",
      " 1003 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247\n",
      " 1248 1249 1250 1251 1252 1253 1254 1255 1298 1299 1300 1301 1302 1303\n",
      " 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317\n",
      " 1318] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('rsi_6', 'rsi_7', 'rsi_8', 'rsi_9', 'rsi_10', 'rsi_11', 'rsi_12', 'rsi_14', 'wr_6', 'wr_7', 'wr_8', 'wr_9', 'wr_10', 'wr_11', 'wr_12', 'wr_13', 'wr_14', 'wr_15', 'wr_16', 'wr_17', 'wr_18', 'wr_19', 'wr_20', 'wr_21', 'wr_22', 'wr_23', 'wr_24', 'wr_25', 'wr_26', 'mfi_6', 'mfi_7', 'mfi_8', 'mfi_9', 'mfi_10', 'mfi_11', 'mfi_12', 'roc_6', 'roc_7', 'roc_8', 'roc_9', 'roc_10', 'roc_11', 'roc_12', 'roc_13', 'roc_14', 'roc_15', 'roc_16', 'roc_17', 'roc_18', 'roc_19', 'roc_20', 'roc_21', 'roc_22', 'roc_23', 'roc_24', 'roc_25', 'roc_26', 'cmf_6', 'cmf_7', 'cmf_9', 'cmf_10', 'cmo_6', 'cmo_7', 'cmo_8', 'cmo_9', 'cmo_10', 'cmo_11', 'cmo_12', 'cmo_13', 'trix_6', 'trix_7', 'trix_8', 'trix_9', 'trix_17', 'trix_18', 'trix_19', 'trix_20', 'trix_21', 'trix_22', 'trix_23', 'trix_24', 'trix_25', 'trix_26', 'cci_6', 'cci_7', 'cci_8', 'cci_9', 'cci_10', 'cci_11', 'cci_12', 'cci_13', 'cci_14', 'cci_15', 'cci_16', 'cci_17', 'cci_18', 'cci_19', 'cci_20', 'cci_21', 'cci_22', 'cci_23', 'cci_24', 'cci_25', 'cci_26', 'dpo_6', 'dpo_7', 'dpo_8', 'dpo_9', 'dpo_10', 'dpo_11', 'dpo_12', 'dpo_13', 'dpo_14', 'dpo_15', 'dpo_16', 'dpo_17', 'dpo_18', 'dpo_19', 'dpo_22', 'dpo_23', 'dmi_6', 'dmi_7', 'dmi_8', 'dmi_9', 'dmi_10', 'dmi_11', 'dmi_12', 'dmi_13', 'dmi_14', 'dmi_15', 'dmi_16', 'dmi_17', 'dmi_18', 'dmi_19', 'dmi_20', 'dmi_21', 'dmi_22', 'dmi_23', 'dmi_24', 'dmi_25', 'dmi_26', 'fi_6', 'fi_7', 'fi_8', 'fi_9', 'fi_10', 'fi_11', 'fi_12', 'fi_13', 'fi_14', 'fi_15', 'fi_16', 'fi_17', 'fi_18', 'fi_19', 'fi_20', 'fi_21', 'fi_22', 'fi_23', 'fi_24', 'fi_25', 'fi_26', 'rsv_6', 'kdjk_6', 'rsv_7', 'kdjk_7', 'rsv_8', 'kdjk_8', 'rsv_9', 'kdjk_9', 'rsv_10', 'kdjk_10', 'rsv_11', 'kdjk_11', 'rsv_12', 'kdjk_12', 'rsv_13', 'kdjk_13', 'rsv_14', 'kdjk_14', 'rsv_15', 'kdjk_15', 'rsv_16', 'kdjk_16', 'rsv_17', 'kdjk_17', 'rsv_18', 'kdjk_18', 'rsv_19', 'kdjk_19', 'rsv_20', 'kdjk_20', 'rsv_21', 'kdjk_21', 'rsv_22', 'kdjk_22', 'rsv_23', 'kdjk_23', 'rsv_24', 'kdjk_24', 'rsv_25', 'kdjk_25', 'rsv_26', 'kdjk_26', 'eom_6', 'eom_7', 'eom_8', 'eom_9', 'eom_10', 'eom_11', 'eom_12', 'eom_13', 'eom_14', 'eom_15', 'eom_16', 'eom_17', 'eom_18', 'eom_19', 'eom_20', 'eom_21', 'eom_22', 'eom_23', 'eom_24', 'eom_25', 'eom_26', 'rsi_6_CL=F', 'rsi_7_CL=F', 'rsi_8_CL=F', 'rsi_9_CL=F', 'rsi_10_CL=F', 'rsi_11_CL=F', 'rsi_13_CL=F', 'rsi_14_CL=F', 'rsi_15_CL=F', 'rsi_16_CL=F', 'rsi_17_CL=F', 'rsi_18_CL=F', 'rsi_19_CL=F', 'rsi_20_CL=F', 'rsi_21_CL=F', 'mfi_6_CL=F', 'mfi_7_CL=F', 'mfi_8_CL=F', 'mfi_9_CL=F', 'mfi_13_CL=F', 'mfi_14_CL=F', 'mfi_15_CL=F', 'mfi_16_CL=F', 'mfi_17_CL=F', 'mfi_18_CL=F', 'mfi_19_CL=F', 'mfi_20_CL=F', 'mfi_21_CL=F', 'mfi_22_CL=F', 'mfi_23_CL=F', 'cmf_16_CL=F', 'cmf_17_CL=F', 'cmf_18_CL=F', 'cmf_19_CL=F', 'cmf_20_CL=F', 'cmo_6_CL=F', 'cmo_7_CL=F', 'cmo_8_CL=F', 'cmo_9_CL=F', 'cmo_10_CL=F', 'cmo_11_CL=F', 'cmo_14_CL=F', 'cmo_15_CL=F', 'cmo_16_CL=F', 'cmo_17_CL=F', 'cmo_18_CL=F', 'cmo_19_CL=F', 'cmo_20_CL=F', 'cmo_21_CL=F', 'cmo_22_CL=F', 'dpo_6_CL=F', 'dpo_8_CL=F', 'dpo_10_CL=F', 'dpo_11_CL=F', 'dpo_14_CL=F', 'dpo_15_CL=F', 'dpo_16_CL=F', 'dpo_17_CL=F', 'dmi_6_CL=F', 'kdjk_7_CL=F', 'kdjk_8_CL=F', 'kdjk_9_CL=F', 'rsi_16_EURUSD=X', 'rsi_17_EURUSD=X', 'rsi_18_EURUSD=X', 'rsi_20_EURUSD=X', 'rsi_21_EURUSD=X', 'rsi_23_EURUSD=X', 'rsi_24_EURUSD=X', 'wr_11_EURUSD=X', 'roc_7_EURUSD=X', 'roc_8_EURUSD=X', 'roc_17_EURUSD=X', 'roc_18_EURUSD=X', 'roc_19_EURUSD=X', 'roc_21_EURUSD=X', 'roc_24_EURUSD=X', 'roc_25_EURUSD=X', 'roc_26_EURUSD=X', 'cmo_17_EURUSD=X', 'cmo_18_EURUSD=X', 'cmo_19_EURUSD=X', 'cmo_21_EURUSD=X', 'cmo_22_EURUSD=X', 'cmo_24_EURUSD=X', 'cmo_25_EURUSD=X', 'dpo_7_EURUSD=X', 'dpo_14_EURUSD=X', 'dpo_15_EURUSD=X', 'kdjk_7_EURUSD=X', 'kdjk_8_EURUSD=X', 'kdjk_9_EURUSD=X', 'kdjk_10_EURUSD=X', 'rsv_11_EURUSD=X', 'kdjk_11_EURUSD=X')\n",
      "[   5    6    7    8    9   10   11   13   26   27   28   29   30   31\n",
      "   32   33   34   35   36   37   38   39   40   41   42   43   44   45\n",
      "   46   47   48   49   50   51   52   53   68   69   70   71   72   73\n",
      "   74   75   76   77   78   79   80   81   82   83   84   85   86   87\n",
      "   88   89   90   92   93  110  111  112  113  114  115  116  117  257\n",
      "  258  259  260  268  269  270  271  272  273  274  275  276  277  278\n",
      "  279  280  281  282  283  284  285  286  287  288  289  290  291  292\n",
      "  293  294  295  296  297  298  299  300  301  302  303  304  305  306\n",
      "  307  308  309  310  311  312  315  316  341  342  343  344  345  346\n",
      "  347  348  349  350  351  352  353  354  355  356  357  358  359  360\n",
      "  361  383  384  385  386  387  388  389  390  391  392  393  394  395\n",
      "  396  397  398  399  400  401  402  403  404  405  406  407  408  409\n",
      "  410  411  412  413  414  415  416  417  418  419  420  421  422  423\n",
      "  424  425  426  427  428  429  430  431  432  433  434  435  436  437\n",
      "  438  439  440  441  442  443  444  445  446  447  448  449  450  451\n",
      "  452  453  454  455  456  457  458  459  460  461  462  463  464  465\n",
      "  466  473  474  475  476  477  478  480  481  482  483  484  485  486\n",
      "  487  488  515  516  517  518  522  523  524  525  526  527  528  529\n",
      "  530  531  532  567  568  569  570  571  578  579  580  581  582  583\n",
      "  586  587  588  589  590  591  592  593  594  725  727  729  730  733\n",
      "  734  735  736  767  833  835  837  909  910  911  913  914  916  917\n",
      "  925  963  964  973  974  975  977  980  981  982 1015 1016 1017 1019\n",
      " 1020 1022 1023 1152 1159 1160 1259 1261 1263 1265 1266 1267]\n",
      "****************************************\n",
      "320 ('rsi_9', 'rsi_10', 'rsi_14', 'rsi_16', 'rsi_20', 'rsi_21', 'rsi_22', 'rsi_24', 'rsi_25', 'wr_8', 'wr_9', 'wr_10', 'wr_11', 'wr_12', 'wr_13', 'wr_14', 'wr_15', 'wr_16', 'wr_21', 'wr_25', 'mfi_6', 'mfi_8', 'mfi_9', 'mfi_13', 'mfi_15', 'mfi_16', 'mfi_23', 'roc_8', 'roc_9', 'roc_11', 'roc_18', 'roc_21', 'cmf_7', 'cmf_15', 'cmf_16', 'cmf_24', 'cmo_6', 'cmo_8', 'cmo_9', 'cmo_11', 'cmo_15', 'cmo_17', 'cmo_18', 'cmo_19', 'open_18_sma', 'open_sma_18', 'open_19_sma', 'open_sma_19', 'open_23_sma', 'open_sma_23', 'open_13_ema', 'ema_13', 'open_20_ema', 'ema_20', 'open_21_ema', 'ema_21', 'open_22_ema', 'ema_22', 'open_23_ema', 'ema_23', 'open_24_ema', 'ema_24', 'open_25_ema', 'ema_25', 'open_26_ema', 'ema_26', 'wma_26', 'hma_14', 'hma_19', 'trix_9', 'trix_12', 'trix_16', 'trix_19', 'trix_25', 'cci_12', 'cci_13', 'cci_15', 'cci_21', 'dpo_8', 'dpo_19', 'dpo_24', 'kst_23', 'kst_24', 'dmi_6', 'dmi_8', 'dmi_10', 'dmi_12', 'dmi_13', 'dmi_16', 'dmi_17', 'dmi_18', 'dmi_19', 'dmi_25', 'dmi_26', 'bb_20', 'bb_21', 'bb_22', 'bb_23', 'rsv_8', 'kdjk_8', 'rsv_9', 'rsv_10', 'rsv_11', 'rsv_12', 'rsv_13', 'kdjk_13', 'rsv_14', 'rsv_15', 'rsv_16', 'kdjk_16', 'kdjk_17', 'kdjk_18', 'kdjk_19', 'kdjk_20', 'rsv_21', 'kdjk_22', 'kdjk_24', 'rsv_25', 'kdjk_25', 'kdjk_26', 'rsi_7_CL=F', 'rsi_10_CL=F', 'rsi_12_CL=F', 'rsi_15_CL=F', 'rsi_16_CL=F', 'rsi_18_CL=F', 'rsi_21_CL=F', 'wr_11_CL=F', 'wr_12_CL=F', 'wr_13_CL=F', 'wr_18_CL=F', 'wr_22_CL=F', 'mfi_7_CL=F', 'mfi_8_CL=F', 'mfi_9_CL=F', 'mfi_12_CL=F', 'mfi_14_CL=F', 'mfi_17_CL=F', 'mfi_20_CL=F', 'mfi_23_CL=F', 'mfi_24_CL=F', 'mfi_25_CL=F', 'cmf_8_CL=F', 'cmf_14_CL=F', 'cmf_15_CL=F', 'cmf_19_CL=F', 'cmo_8_CL=F', 'cmo_9_CL=F', 'cmo_12_CL=F', 'cmo_15_CL=F', 'cmo_19_CL=F', 'cmo_22_CL=F', 'open_sma_9_CL=F', 'open_sma_13_CL=F', 'open_sma_16_CL=F', 'open_sma_24_CL=F', 'ema_22_CL=F', 'ema_23_CL=F', 'ema_24_CL=F', 'wma_6_CL=F', 'wma_7_CL=F', 'wma_9_CL=F', 'wma_19_CL=F', 'hma_0_CL=F', 'hma_1_CL=F', 'hma_2_CL=F', 'hma_3_CL=F', 'hma_4_CL=F', 'hma_5_CL=F', 'hma_6_CL=F', 'hma_7_CL=F', 'hma_11_CL=F', 'hma_13_CL=F', 'hma_14_CL=F', 'hma_15_CL=F', 'hma_19_CL=F', 'hma_20_CL=F', 'trix_22_CL=F', 'trix_23_CL=F', 'trix_24_CL=F', 'trix_25_CL=F', 'cci_8_CL=F', 'cci_12_CL=F', 'cci_20_CL=F', 'dpo_9_CL=F', 'dpo_14_CL=F', 'dpo_19_CL=F', 'kst_6_CL=F', 'kst_7_CL=F', 'kst_13_CL=F', 'dmi_12_CL=F', 'dmi_14_CL=F', 'dmi_15_CL=F', 'bb_6_CL=F', 'bb_7_CL=F', 'bb_10_CL=F', 'bb_11_CL=F', 'bb_25_CL=F', 'bb_26_CL=F', 'kdjk_7_CL=F', 'kdjk_9_CL=F', 'rsv_11_CL=F', 'rsv_12_CL=F', 'kdjk_12_CL=F', 'rsv_13_CL=F', 'rsv_18_CL=F', 'kdjk_18_CL=F', 'kdjk_20_CL=F', 'rsv_22_CL=F', 'kdjk_23_CL=F', 'kdjk_25_CL=F', 'unnamed: 0_EURUSD=X', 'open_EURUSD=X', 'high_EURUSD=X', 'rsi_12_EURUSD=X', 'rsi_16_EURUSD=X', 'rsi_17_EURUSD=X', 'wr_6_EURUSD=X', 'wr_7_EURUSD=X', 'wr_9_EURUSD=X', 'wr_10_EURUSD=X', 'wr_11_EURUSD=X', 'wr_12_EURUSD=X', 'wr_13_EURUSD=X', 'wr_15_EURUSD=X', 'wr_16_EURUSD=X', 'wr_17_EURUSD=X', 'wr_19_EURUSD=X', 'wr_20_EURUSD=X', 'wr_21_EURUSD=X', 'wr_25_EURUSD=X', 'mfi_6_EURUSD=X', 'mfi_10_EURUSD=X', 'mfi_13_EURUSD=X', 'mfi_20_EURUSD=X', 'mfi_24_EURUSD=X', 'roc_18_EURUSD=X', 'roc_26_EURUSD=X', 'cmf_8_EURUSD=X', 'cmf_11_EURUSD=X', 'cmf_21_EURUSD=X', 'cmf_24_EURUSD=X', 'cmo_6_EURUSD=X', 'open_sma_6_EURUSD=X', 'open_sma_7_EURUSD=X', 'open_sma_9_EURUSD=X', 'ema_6_EURUSD=X', 'ema_7_EURUSD=X', 'ema_8_EURUSD=X', 'ema_10_EURUSD=X', 'ema_11_EURUSD=X', 'ema_16_EURUSD=X', 'ema_18_EURUSD=X', 'wma_7_EURUSD=X', 'wma_8_EURUSD=X', 'wma_9_EURUSD=X', 'wma_10_EURUSD=X', 'wma_11_EURUSD=X', 'wma_14_EURUSD=X', 'wma_15_EURUSD=X', 'hma_16_EURUSD=X', 'hma_18_EURUSD=X', 'trix_7_EURUSD=X', 'trix_10_EURUSD=X', 'trix_14_EURUSD=X', 'trix_19_EURUSD=X', 'trix_20_EURUSD=X', 'trix_21_EURUSD=X', 'trix_22_EURUSD=X', 'trix_23_EURUSD=X', 'trix_24_EURUSD=X', 'trix_25_EURUSD=X', 'cci_10_EURUSD=X', 'cci_12_EURUSD=X', 'cci_18_EURUSD=X', 'cci_20_EURUSD=X', 'dpo_6_EURUSD=X', 'dpo_9_EURUSD=X', 'dpo_15_EURUSD=X', 'dpo_19_EURUSD=X', 'dpo_24_EURUSD=X', 'kst_12_EURUSD=X', 'kst_13_EURUSD=X', 'dmi_9_EURUSD=X', 'dmi_10_EURUSD=X', 'dmi_12_EURUSD=X', 'dmi_18_EURUSD=X', 'dmi_21_EURUSD=X', 'bb_6_EURUSD=X', 'bb_7_EURUSD=X', 'fi_8_EURUSD=X', 'fi_11_EURUSD=X', 'fi_19_EURUSD=X', 'rsv_6_EURUSD=X', 'rsv_7_EURUSD=X', 'kdjk_7_EURUSD=X', 'rsv_9_EURUSD=X', 'rsv_10_EURUSD=X', 'kdjk_10_EURUSD=X', 'rsv_11_EURUSD=X', 'rsv_12_EURUSD=X', 'rsv_13_EURUSD=X', 'rsv_15_EURUSD=X', 'rsv_16_EURUSD=X', 'rsv_17_EURUSD=X', 'kdjk_17_EURUSD=X', 'kdjk_18_EURUSD=X', 'rsv_19_EURUSD=X', 'kdjk_19_EURUSD=X', 'rsv_20_EURUSD=X', 'kdjk_20_EURUSD=X', 'rsv_21_EURUSD=X', 'kdjk_21_EURUSD=X', 'kdjk_22_EURUSD=X', 'kdjk_23_EURUSD=X', 'rsv_25_EURUSD=X', 'kdjk_25_EURUSD=X', 'eom_7_EURUSD=X', 'eom_9_EURUSD=X', 'eom_17_EURUSD=X')\n",
      "[   8    9   13   15   19   20   21   23   24   28   29   30   31   32\n",
      "   33   34   35   36   41   45   47   49   50   54   56   57   64   70\n",
      "   71   73   80   83   90   98   99  107  110  112  113  115  119  121\n",
      "  122  123  155  156  157  158  165  166  187  188  201  202  203  204\n",
      "  205  206  207  208  209  210  211  212  213  214  235  250  255  260\n",
      "  263  267  270  276  284  285  287  293  301  312  317  337  338  341\n",
      "  343  345  347  348  351  352  353  354  360  361  376  377  378  379\n",
      "  408  409  410  412  414  416  418  419  420  422  424  425  427  429\n",
      "  431  433  434  437  441  442  443  445  474  477  479  482  483  485\n",
      "  488  499  500  501  506  510  516  517  518  521  523  526  529  532\n",
      "  533  534  559  565  566  570  580  581  584  587  591  594  602  606\n",
      "  609  617  636  637  638  641  642  644  654  662  663  664  665  666\n",
      "  667  668  669  673  675  676  677  681  682  699  700  701  702  706\n",
      "  710  718  728  733  738  746  747  753  773  775  776  788  789  792\n",
      "  793  807  808  833  837  840  842  843  844  854  855  859  862  865\n",
      "  869  893  894  895  905  909  910  920  921  923  924  925  926  927\n",
      "  929  930  931  933  934  935  939  941  945  948  955  959  974  982\n",
      "  985  988  998 1001 1004 1025 1026 1028 1046 1047 1048 1050 1051 1056\n",
      " 1058 1068 1069 1070 1071 1072 1075 1076 1104 1106 1110 1113 1117 1122\n",
      " 1123 1124 1125 1126 1127 1128 1134 1136 1142 1144 1151 1154 1160 1164\n",
      " 1169 1178 1179 1196 1197 1199 1205 1208 1214 1215 1237 1240 1248 1256\n",
      " 1258 1259 1262 1264 1265 1266 1268 1270 1274 1276 1278 1279 1281 1282\n",
      " 1283 1284 1285 1286 1287 1289 1291 1294 1295 1299 1301 1309]\n",
      "CPU times: user 15 s, sys: 249 ms, total: 15.2 s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from operator import itemgetter\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "if selection_method == 'anova' or selection_method == 'all':\n",
    "    select_k_best = SelectKBest(f_classif, k=topk)\n",
    "    if selection_method != 'all':\n",
    "        x_train = select_k_best.fit_transform(x_main, y_train)\n",
    "        x_cv = select_k_best.transform(x_cv)\n",
    "        x_test = select_k_best.transform(x_test)\n",
    "    else:\n",
    "        select_k_best.fit(x_main, y_train)\n",
    "    \n",
    "    selected_features_anova = itemgetter(*select_k_best.get_support(indices=True))(list_features)\n",
    "    print(selected_features_anova)\n",
    "    print(select_k_best.get_support(indices=True))\n",
    "    print(\"****************************************\")\n",
    "    \n",
    "if selection_method == 'mutual_info' or selection_method == 'all':\n",
    "    select_k_best = SelectKBest(mutual_info_classif, k=topk)\n",
    "    if selection_method != 'all':\n",
    "        x_train = select_k_best.fit_transform(x_main, y_train)\n",
    "        x_cv = select_k_best.transform(x_cv)\n",
    "        x_test = select_k_best.transform(x_test)\n",
    "    else:\n",
    "        select_k_best.fit(x_main, y_train)\n",
    "\n",
    "    selected_features_mic = itemgetter(*select_k_best.get_support(indices=True))(list_features)\n",
    "    print(len(selected_features_mic), selected_features_mic)\n",
    "    print(select_k_best.get_support(indices=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common selected featues 100 ['kdjk_26', 'kdjk_9_CL=F', 'kdjk_19', 'wr_21', 'trix_25', 'rsi_10_CL=F', 'rsi_16_CL=F', 'cmo_6', 'rsv_10', 'mfi_9_CL=F', 'rsv_16', 'dmi_12', 'wr_15', 'cmo_9_CL=F', 'roc_18_EURUSD=X', 'rsi_9', 'dmi_18', 'cmo_19_CL=F', 'cmo_15_CL=F', 'dmi_6', 'rsv_8', 'dmi_17', 'dmi_26', 'rsi_18_CL=F', 'kdjk_7_EURUSD=X', 'dmi_10', 'rsv_12', 'rsv_14', 'wr_12', 'kdjk_8', 'rsi_21_CL=F', 'cmo_11', 'roc_26_EURUSD=X', 'trix_9', 'mfi_20_CL=F', 'roc_18', 'cci_21', 'trix_19', 'mfi_7_CL=F', 'wr_25', 'cci_13', 'rsi_15_CL=F', 'mfi_17_CL=F', 'kdjk_24', 'rsi_16_EURUSD=X', 'cci_15', 'roc_9', 'cmf_19_CL=F', 'mfi_14_CL=F', 'wr_11', 'rsi_17_EURUSD=X', 'kdjk_22', 'mfi_8', 'rsv_9', 'mfi_9', 'dpo_19', 'mfi_8_CL=F', 'wr_11_EURUSD=X', 'mfi_23_CL=F', 'rsi_14', 'cci_12', 'dmi_13', 'rsv_13', 'roc_21', 'wr_8', 'wr_9', 'mfi_6', 'kdjk_7_CL=F', 'roc_8', 'wr_13', 'rsv_11_EURUSD=X', 'dmi_25', 'rsv_11', 'kdjk_13', 'cmo_9', 'kdjk_18', 'kdjk_20', 'cmo_8', 'dpo_8', 'rsv_15', 'cmo_8_CL=F', 'dpo_14_CL=F', 'wr_16', 'roc_11', 'cmf_7', 'dmi_16', 'dmi_19', 'kdjk_17', 'kdjk_25', 'wr_14', 'dpo_15_EURUSD=X', 'rsv_25', 'rsv_21', 'rsi_10', 'kdjk_10_EURUSD=X', 'wr_10', 'kdjk_16', 'rsi_7_CL=F', 'cmo_22_CL=F', 'dmi_8']\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "number of common features found 100 < 225 required features. Increase \"topk variable\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jr/qy0bs70j6qngjxk4pyvhj9_w0000gn/T/ipykernel_50599/574824041.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"common selected featues\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'number of common features found {} < {} required features. Increase \"topk variable\"'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mfeat_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: number of common features found 100 < 225 required features. Increase \"topk variable\""
     ]
    }
   ],
   "source": [
    "if selection_method == 'all':\n",
    "    common = list(set(selected_features_anova).intersection(selected_features_mic))\n",
    "    print(\"common selected featues\", len(common), common)\n",
    "    if len(common) < num_features:\n",
    "        raise Exception('number of common features found {} < {} required features. Increase \"topk variable\"'.format(len(common), num_features))\n",
    "    feat_idx = []\n",
    "    for c in common:\n",
    "        feat_idx.append(list_features.index(c))\n",
    "    feat_idx = sorted(feat_idx[0:225])\n",
    "    print(feat_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x, y train/cv/test (1899, 225) (1899,) (475, 225) (475,) (594, 225) (594,)\n"
     ]
    }
   ],
   "source": [
    "if selection_method == 'all':\n",
    "    x_train = x_train[:, feat_idx]\n",
    "    x_cv = x_cv[:, feat_idx]\n",
    "    x_test = x_test[:, feat_idx]\n",
    "\n",
    "print(\"Shape of x, y train/cv/test {} {} {} {} {} {}\".format(x_train.shape, \n",
    "                                                             y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of class 0 = 7.003686150605581, class 1 = 88.62559241706161\n"
     ]
    }
   ],
   "source": [
    "_labels, _counts = np.unique(y_train, return_counts=True)\n",
    "print(\"percentage of class 0 = {}, class 1 = {}\".format(_counts[0]/len(y_train) * 100, _counts[1]/len(y_train) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from utils import reshape_array_as_image\n",
    "from metrics import f1_weighted, f1_metric\n",
    "\n",
    "def get_sample_weights(y): # TODO add source\n",
    "    \"\"\"\n",
    "    calculate the sample weights based on class weights. Used for models with\n",
    "    imbalanced data and one hot encoding prediction.\n",
    "\n",
    "    params:\n",
    "        y: class labels as integers\n",
    "    \"\"\"\n",
    "\n",
    "    y = y.astype(int)  # compute_class_weight needs int labels\n",
    "    class_weights = compute_class_weight(class_weight = \"balanced\", classes = np.unique(y),y = y)\n",
    "    print(\"real class weights are {}\".format(class_weights), np.unique(y))\n",
    "    print(\"value_counts\", np.unique(y, return_counts=True))\n",
    "    sample_weights = y.copy().astype(float)\n",
    "    for i in set(y): #np.unique\n",
    "        sample_weights[sample_weights == i] = class_weights[i]  # if i == 2 else 0.8 * class_weights[i]\n",
    "        # sample_weights = np.where(sample_weights == i, class_weights[int(i)], y_)\n",
    "\n",
    "    return sample_weights\n",
    "\n",
    "get_custom_objects().update({\"f1_metric\": f1_metric, \"f1_weighted\": f1_weighted}) # why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real class weights are [4.7593985  0.37611408 7.62650602] [-1  0  1]\n",
      "value_counts (array([-1,  0,  1]), array([ 133, 1683,   83]))\n",
      "Test sample_weights\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0  0  0  0  0\n",
      " -1  0  0  0  0  0]\n",
      "[4.7593985  4.7593985  4.7593985  4.7593985  4.7593985  4.7593985\n",
      " 4.7593985  4.7593985  4.7593985  4.7593985  4.7593985  4.7593985\n",
      " 7.62650602 4.7593985  4.7593985  4.7593985  4.7593985  7.62650602\n",
      " 4.7593985  4.7593985  4.7593985  4.7593985  4.7593985  4.7593985\n",
      " 7.62650602 4.7593985  4.7593985  4.7593985  4.7593985  4.7593985 ]\n"
     ]
    }
   ],
   "source": [
    "sample_weights = get_sample_weights(y_train)\n",
    "print(\"Test sample_weights\")\n",
    "rand_idx = np.random.randint(0, 1000, 30)\n",
    "print(y_train[rand_idx])\n",
    "print(sample_weights[rand_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train (1899, 3)\n"
     ]
    }
   ],
   "source": [
    "one_hot_enc = OneHotEncoder(sparse=False, categories='auto')  # , categories='auto'\n",
    "y_train = one_hot_enc.fit_transform(y_train.reshape(-1, 1))\n",
    "print(\"y_train\",y_train.shape)\n",
    "y_cv = one_hot_enc.transform(y_cv.reshape(-1, 1))\n",
    "y_test = one_hot_enc.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final shape of x, y train/test (1899, 15, 15, 3) (1899, 3) (594, 15, 15, 3) (594, 3)\n"
     ]
    }
   ],
   "source": [
    "dim = int(np.sqrt(num_features))\n",
    "x_train = reshape_array_as_image(x_train, dim, dim)\n",
    "x_cv = reshape_array_as_image(x_cv, dim, dim)\n",
    "x_test = reshape_array_as_image(x_test, dim, dim)\n",
    "# adding a 1-dim for channels (3)\n",
    "x_train = np.stack((x_train,) * 3, axis=-1)\n",
    "x_test = np.stack((x_test,) * 3, axis=-1)\n",
    "x_cv = np.stack((x_cv,) * 3, axis=-1)\n",
    "print(\"final shape of x, y train/test {} {} {} {}\".format(x_train.shape, y_train.shape, x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "columns = rows = 3\n",
    "for i in range(1, columns*rows +1):\n",
    "    index = np.random.randint(len(x_train))\n",
    "    img = x_train[index]\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title('image_'+str(index)+'_class_'+str(np.argmax(y_train[index])), fontsize=10)\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import print_with_timestamp, show_images\n",
    "show_images(3,3, arr=x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15ce87880>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = [i for i in range(225)]\n",
    "test = np.array(test).reshape(-1, 1)\n",
    "#test = scaler.fit_transform(test)\n",
    "dim = int(np.sqrt(num_features))\n",
    "test = np.reshape(test, (dim, dim))\n",
    "plt.imshow(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "        208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "        221, 222, 223, 224]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, LeakyReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger, Callback\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from tensorflow.keras.initializers import RandomUniform, RandomNormal\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "params = {'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.2, 'conv2d_filters_1': 32, 'conv2d_kernel_size_1': 3, 'conv2d_mp_1': 0, \n",
    "                                               'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.0, 'conv2d_do_2': 0.3, \n",
    "                                               'conv2d_filters_2': 64, 'conv2d_kernel_size_2': 3, 'conv2d_mp_2': 2, 'conv2d_strides_2': 1, \n",
    "                                               'kernel_regularizer_2': 0.0, 'layers': 'two'}, \n",
    "           'dense_layers': {'dense_do_1': 0.3, 'dense_nodes_1': 128, 'kernel_regularizer_1': 0.0, 'layers': 'one'},\n",
    "           'epochs': 50, 'lr': 0.001, 'optimizer': 'adam'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import *\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "def f1_custom(y_true, y_pred):\n",
    "    y_t = np.argmax(y_true, axis=1)\n",
    "    y_p = np.argmax(y_pred, axis=1)\n",
    "    f1_score(y_t, y_p, labels=None, average='weighted', sample_weight=None, zero_division='warn')\n",
    "\n",
    "def create_model_cnn(params):\n",
    "    model = Sequential()\n",
    "\n",
    "    print(\"Training with params {}\".format(params))\n",
    "    \n",
    "    conv2d_layer1 = Conv2D(params[\"conv2d_layers\"][\"conv2d_filters_1\"],\n",
    "                           params[\"conv2d_layers\"][\"conv2d_kernel_size_1\"],\n",
    "                           strides=params[\"conv2d_layers\"][\"conv2d_strides_1\"],\n",
    "                           kernel_regularizer=regularizers.l2(params[\"conv2d_layers\"][\"kernel_regularizer_1\"]), \n",
    "                           padding='same',activation=\"relu\", use_bias=True,\n",
    "                           kernel_initializer='glorot_uniform',\n",
    "                           input_shape=(x_train[0].shape[0],\n",
    "                                        x_train[0].shape[1], x_train[0].shape[2]))\n",
    "    model.add(conv2d_layer1)\n",
    "    if params[\"conv2d_layers\"]['conv2d_mp_1'] > 1:\n",
    "        model.add(MaxPool2D(pool_size=params[\"conv2d_layers\"]['conv2d_mp_1']))\n",
    "        \n",
    "    model.add(Dropout(params['conv2d_layers']['conv2d_do_1']))\n",
    "    if params[\"conv2d_layers\"]['layers'] == 'two':\n",
    "        conv2d_layer2 = Conv2D(params[\"conv2d_layers\"][\"conv2d_filters_2\"],\n",
    "                               params[\"conv2d_layers\"][\"conv2d_kernel_size_2\"],\n",
    "                               strides=params[\"conv2d_layers\"][\"conv2d_strides_2\"],\n",
    "                               kernel_regularizer=regularizers.l2(params[\"conv2d_layers\"][\"kernel_regularizer_2\"]),\n",
    "                               padding='same',activation=\"relu\", use_bias=True,\n",
    "                               kernel_initializer='glorot_uniform')\n",
    "        model.add(conv2d_layer2)\n",
    "        \n",
    "        if params[\"conv2d_layers\"]['conv2d_mp_2'] > 1:\n",
    "            model.add(MaxPool2D(pool_size=params[\"conv2d_layers\"]['conv2d_mp_2']))\n",
    "        \n",
    "        model.add(Dropout(params['conv2d_layers']['conv2d_do_2']))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(params['dense_layers'][\"dense_nodes_1\"], activation='relu'))\n",
    "    model.add(Dropout(params['dense_layers']['dense_do_1']))\n",
    "\n",
    "    if params['dense_layers'][\"layers\"] == 'two':\n",
    "        model.add(Dense(params['dense_layers'][\"dense_nodes_2\"], activation='relu', \n",
    "                        kernel_regularizer=params['dense_layers'][\"kernel_regularizer_1\"]))\n",
    "        model.add(Dropout(params['dense_layers']['dense_do_2']))\n",
    "\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    if params[\"optimizer\"] == 'rmsprop':\n",
    "        optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "    elif params[\"optimizer\"] == 'sgd':\n",
    "        optimizer = optimizers.SGD(lr=params[\"lr\"], decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    elif params[\"optimizer\"] == 'adam':\n",
    "        optimizer = optimizers.Adam(learning_rate=params[\"lr\"], beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', f1_metric])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def check_baseline(pred, y_test):\n",
    "    print(\"size of test set\", len(y_test))\n",
    "    e = np.equal(pred, y_test)\n",
    "    print(\"TP class counts\", np.unique(y_test[e], return_counts=True))\n",
    "    print(\"True class counts\", np.unique(y_test, return_counts=True))\n",
    "    print(\"Pred class counts\", np.unique(pred, return_counts=True))\n",
    "    holds = np.unique(y_test, return_counts=True)[1][2]  # number 'hold' predictions\n",
    "    print(\"baseline acc:\", (holds/len(y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params {'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.2, 'conv2d_filters_1': 32, 'conv2d_kernel_size_1': 3, 'conv2d_mp_1': 0, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.0, 'conv2d_do_2': 0.3, 'conv2d_filters_2': 64, 'conv2d_kernel_size_2': 3, 'conv2d_mp_2': 2, 'conv2d_strides_2': 1, 'kernel_regularizer_2': 0.0, 'layers': 'two'}, 'dense_layers': {'dense_do_1': 0.3, 'dense_nodes_1': 128, 'kernel_regularizer_1': 0.0, 'layers': 'one'}, 'epochs': 200, 'lr': 0.001, 'optimizer': 'adam'}\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 12:15:07.649310: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "model = create_model_cnn(params)\n",
    "\n",
    "best_model_path = os.path.join('.', 'best_model_keras')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                   patience=100, min_delta=0.0001)\n",
    "# csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'log_training_batch.log'), append=True)\n",
    "rlp = ReduceLROnPlateau(monitor='val_loss', factor=0.02, patience=20, verbose=1, mode='min',\n",
    "                        min_delta=0.001, cooldown=1, min_lr=0.0001)\n",
    "mcp = ModelCheckpoint(best_model_path, monitor='val_f1_metric', verbose=1,\n",
    "                      save_best_only=True, save_weights_only=False, mode='max', period=1)  # val_f1_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 2.1614 - accuracy: 0.8720 - f1_metric: 0.8573\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.88383, saving model to ./best_model_keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 12:15:12.740836: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "30/30 [==============================] - 6s 147ms/step - loss: 2.1614 - accuracy: 0.8720 - f1_metric: 0.8573 - val_loss: 0.5112 - val_accuracy: 0.8863 - val_f1_metric: 0.8838 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.8626 - accuracy: 0.8879 - f1_metric: 0.8879\n",
      "Epoch 00002: val_f1_metric did not improve from 0.88383\n",
      "30/30 [==============================] - 2s 63ms/step - loss: 1.8827 - accuracy: 0.8863 - f1_metric: 0.8855 - val_loss: 0.4984 - val_accuracy: 0.8863 - val_f1_metric: 0.8838 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.8484 - accuracy: 0.8863 - f1_metric: 0.8860\n",
      "Epoch 00003: val_f1_metric did not improve from 0.88383\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 1.8484 - accuracy: 0.8863 - f1_metric: 0.8860 - val_loss: 0.5110 - val_accuracy: 0.8863 - val_f1_metric: 0.8838 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.8673 - accuracy: 0.8858 - f1_metric: 0.8858\n",
      "Epoch 00004: val_f1_metric did not improve from 0.88383\n",
      "30/30 [==============================] - 2s 62ms/step - loss: 1.8594 - accuracy: 0.8863 - f1_metric: 0.8865 - val_loss: 0.5101 - val_accuracy: 0.8863 - val_f1_metric: 0.8838 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.8585 - accuracy: 0.8842 - f1_metric: 0.8842\n",
      "Epoch 00005: val_f1_metric did not improve from 0.88383\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.8440 - accuracy: 0.8863 - f1_metric: 0.8872 - val_loss: 0.5142 - val_accuracy: 0.8863 - val_f1_metric: 0.8838 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.8094 - accuracy: 0.8858 - f1_metric: 0.8858\n",
      "Epoch 00006: val_f1_metric did not improve from 0.88383\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 1.8086 - accuracy: 0.8863 - f1_metric: 0.8865 - val_loss: 0.4888 - val_accuracy: 0.8863 - val_f1_metric: 0.8838 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.8007 - accuracy: 0.8863 - f1_metric: 0.8862\n",
      "Epoch 00007: val_f1_metric did not improve from 0.88383\n",
      "30/30 [==============================] - 2s 79ms/step - loss: 1.8007 - accuracy: 0.8863 - f1_metric: 0.8862 - val_loss: 0.4838 - val_accuracy: 0.8863 - val_f1_metric: 0.8838 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.7742 - accuracy: 0.8858 - f1_metric: 0.8858\n",
      "Epoch 00008: val_f1_metric did not improve from 0.88383\n",
      "30/30 [==============================] - 2s 77ms/step - loss: 1.7696 - accuracy: 0.8863 - f1_metric: 0.8865 - val_loss: 0.4839 - val_accuracy: 0.8863 - val_f1_metric: 0.8838 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.7762 - accuracy: 0.8863 - f1_metric: 0.8857\n",
      "Epoch 00009: val_f1_metric did not improve from 0.88383\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.7762 - accuracy: 0.8863 - f1_metric: 0.8857 - val_loss: 0.4975 - val_accuracy: 0.8863 - val_f1_metric: 0.8838 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.7899 - accuracy: 0.8863 - f1_metric: 0.8863\n",
      "Epoch 00010: val_f1_metric did not improve from 0.88383\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.7899 - accuracy: 0.8863 - f1_metric: 0.8862 - val_loss: 0.4930 - val_accuracy: 0.8863 - val_f1_metric: 0.8838 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.7890 - accuracy: 0.8863 - f1_metric: 0.8875\n",
      "Epoch 00011: val_f1_metric did not improve from 0.88383\n",
      "30/30 [==============================] - 2s 76ms/step - loss: 1.7890 - accuracy: 0.8863 - f1_metric: 0.8875 - val_loss: 0.4714 - val_accuracy: 0.8863 - val_f1_metric: 0.8838 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.7280 - accuracy: 0.8874 - f1_metric: 0.8874\n",
      "Epoch 00012: val_f1_metric did not improve from 0.88383\n",
      "30/30 [==============================] - 2s 82ms/step - loss: 1.7448 - accuracy: 0.8863 - f1_metric: 0.8857 - val_loss: 0.4971 - val_accuracy: 0.8863 - val_f1_metric: 0.8838 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.7720 - accuracy: 0.8863 - f1_metric: 0.8865\n",
      "Epoch 00013: val_f1_metric did not improve from 0.88383\n",
      "30/30 [==============================] - 2s 65ms/step - loss: 1.7720 - accuracy: 0.8863 - f1_metric: 0.8865 - val_loss: 0.4800 - val_accuracy: 0.8863 - val_f1_metric: 0.8838 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.7266 - accuracy: 0.8863 - f1_metric: 0.8857\n",
      "Epoch 00014: val_f1_metric did not improve from 0.88383\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.7343 - accuracy: 0.8863 - f1_metric: 0.8856 - val_loss: 0.5094 - val_accuracy: 0.8863 - val_f1_metric: 0.8838 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.7478 - accuracy: 0.8863 - f1_metric: 0.8862\n",
      "Epoch 00015: val_f1_metric did not improve from 0.88383\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 1.7478 - accuracy: 0.8863 - f1_metric: 0.8862 - val_loss: 0.4715 - val_accuracy: 0.8863 - val_f1_metric: 0.8838 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.7294 - accuracy: 0.8847 - f1_metric: 0.8844\n",
      "Epoch 00016: val_f1_metric did not improve from 0.88383\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 1.7229 - accuracy: 0.8863 - f1_metric: 0.8867 - val_loss: 0.4859 - val_accuracy: 0.8863 - val_f1_metric: 0.8838 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.7010 - accuracy: 0.8852 - f1_metric: 0.8852\n",
      "Epoch 00017: val_f1_metric improved from 0.88383 to 0.88466, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "30/30 [==============================] - 4s 123ms/step - loss: 1.6915 - accuracy: 0.8863 - f1_metric: 0.8867 - val_loss: 0.4776 - val_accuracy: 0.8863 - val_f1_metric: 0.8847 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.7059 - accuracy: 0.8863 - f1_metric: 0.8858\n",
      "Epoch 00018: val_f1_metric did not improve from 0.88466\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 1.7059 - accuracy: 0.8863 - f1_metric: 0.8858 - val_loss: 0.5004 - val_accuracy: 0.8863 - val_f1_metric: 0.8838 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.7162 - accuracy: 0.8863 - f1_metric: 0.8863\n",
      "Epoch 00019: val_f1_metric improved from 0.88466 to 0.88546, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "30/30 [==============================] - 5s 154ms/step - loss: 1.7143 - accuracy: 0.8868 - f1_metric: 0.8870 - val_loss: 0.4800 - val_accuracy: 0.8884 - val_f1_metric: 0.8855 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.7018 - accuracy: 0.8857 - f1_metric: 0.8854\n",
      "Epoch 00020: val_f1_metric did not improve from 0.88546\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.7018 - accuracy: 0.8857 - f1_metric: 0.8854 - val_loss: 0.4893 - val_accuracy: 0.8863 - val_f1_metric: 0.8838 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.7019 - accuracy: 0.8868 - f1_metric: 0.8875\n",
      "Epoch 00021: val_f1_metric improved from 0.88546 to 0.88578, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "30/30 [==============================] - 5s 179ms/step - loss: 1.7019 - accuracy: 0.8868 - f1_metric: 0.8875 - val_loss: 0.4879 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.6772 - accuracy: 0.8869 - f1_metric: 0.8869\n",
      "Epoch 00022: val_f1_metric improved from 0.88578 to 0.88658, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "30/30 [==============================] - 4s 150ms/step - loss: 1.6757 - accuracy: 0.8857 - f1_metric: 0.8852 - val_loss: 0.4737 - val_accuracy: 0.8884 - val_f1_metric: 0.8866 - lr: 0.0010\n",
      "Epoch 23/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/30 [============================>.] - ETA: 0s - loss: 1.6840 - accuracy: 0.8831 - f1_metric: 0.8831\n",
      "Epoch 00023: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.6680 - accuracy: 0.8847 - f1_metric: 0.8854 - val_loss: 0.4775 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.6506 - accuracy: 0.8874 - f1_metric: 0.8876\n",
      "Epoch 00024: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 63ms/step - loss: 1.6782 - accuracy: 0.8868 - f1_metric: 0.8867 - val_loss: 0.5051 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.6680 - accuracy: 0.8863 - f1_metric: 0.8868\n",
      "Epoch 00025: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.6646 - accuracy: 0.8863 - f1_metric: 0.8867 - val_loss: 0.4751 - val_accuracy: 0.8884 - val_f1_metric: 0.8866 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.6471 - accuracy: 0.8863 - f1_metric: 0.8863\n",
      "Epoch 00026: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.6512 - accuracy: 0.8852 - f1_metric: 0.8847 - val_loss: 0.4771 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.6391 - accuracy: 0.8895 - f1_metric: 0.8895\n",
      "Epoch 00027: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 63ms/step - loss: 1.6692 - accuracy: 0.8863 - f1_metric: 0.8850 - val_loss: 0.4871 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.6559 - accuracy: 0.8890 - f1_metric: 0.8887\n",
      "Epoch 00028: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 63ms/step - loss: 1.6803 - accuracy: 0.8863 - f1_metric: 0.8847 - val_loss: 0.4757 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.6320 - accuracy: 0.8885 - f1_metric: 0.8883\n",
      "Epoch 00029: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.6410 - accuracy: 0.8868 - f1_metric: 0.8858 - val_loss: 0.4948 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.6370 - accuracy: 0.8842 - f1_metric: 0.8849\n",
      "Epoch 00030: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 63ms/step - loss: 1.6237 - accuracy: 0.8852 - f1_metric: 0.8864 - val_loss: 0.4944 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.6174 - accuracy: 0.8885 - f1_metric: 0.8892\n",
      "Epoch 00031: val_f1_metric did not improve from 0.88658\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "30/30 [==============================] - 2s 63ms/step - loss: 1.6183 - accuracy: 0.8878 - f1_metric: 0.8882 - val_loss: 0.4736 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.5913 - accuracy: 0.8863 - f1_metric: 0.8862\n",
      "Epoch 00032: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 1.6099 - accuracy: 0.8857 - f1_metric: 0.8853 - val_loss: 0.4783 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.5483 - accuracy: 0.8869 - f1_metric: 0.8865\n",
      "Epoch 00033: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.5602 - accuracy: 0.8857 - f1_metric: 0.8848 - val_loss: 0.4774 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.5487 - accuracy: 0.8874 - f1_metric: 0.8874 ETA: 1s - loss: 1.6992 \n",
      "Epoch 00034: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.5592 - accuracy: 0.8863 - f1_metric: 0.8857 - val_loss: 0.4789 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.5475 - accuracy: 0.8836 - f1_metric: 0.8837\n",
      "Epoch 00035: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.5262 - accuracy: 0.8857 - f1_metric: 0.8868 - val_loss: 0.4763 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.5402 - accuracy: 0.8869 - f1_metric: 0.8862\n",
      "Epoch 00036: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 62ms/step - loss: 1.5469 - accuracy: 0.8863 - f1_metric: 0.8854 - val_loss: 0.4787 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 37/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.5115 - accuracy: 0.8852 - f1_metric: 0.8857\n",
      "Epoch 00037: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.4984 - accuracy: 0.8863 - f1_metric: 0.8872 - val_loss: 0.4799 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 38/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.4899 - accuracy: 0.8868 - f1_metric: 0.8872\n",
      "Epoch 00038: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 63ms/step - loss: 1.4899 - accuracy: 0.8868 - f1_metric: 0.8872 - val_loss: 0.4806 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 39/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.5513 - accuracy: 0.8885 - f1_metric: 0.8882\n",
      "Epoch 00039: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.5411 - accuracy: 0.8878 - f1_metric: 0.8873 - val_loss: 0.4804 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 40/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.5024 - accuracy: 0.8874 - f1_metric: 0.8877\n",
      "Epoch 00040: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 1.4982 - accuracy: 0.8878 - f1_metric: 0.8883 - val_loss: 0.4816 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 41/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.5006 - accuracy: 0.8869 - f1_metric: 0.8868\n",
      "Epoch 00041: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.5012 - accuracy: 0.8868 - f1_metric: 0.8867 - val_loss: 0.4811 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 42/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.5368 - accuracy: 0.8863 - f1_metric: 0.8862\n",
      "Epoch 00042: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.5368 - accuracy: 0.8863 - f1_metric: 0.8862 - val_loss: 0.4820 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 43/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4976 - accuracy: 0.8869 - f1_metric: 0.8867\n",
      "Epoch 00043: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.5102 - accuracy: 0.8863 - f1_metric: 0.8858 - val_loss: 0.4825 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 44/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.5360 - accuracy: 0.8831 - f1_metric: 0.8831\n",
      "Epoch 00044: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.5153 - accuracy: 0.8841 - f1_metric: 0.8847 - val_loss: 0.4823 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 45/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.5217 - accuracy: 0.8858 - f1_metric: 0.8857\n",
      "Epoch 00045: val_f1_metric did not improve from 0.88658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 2s 74ms/step - loss: 1.5069 - accuracy: 0.8863 - f1_metric: 0.8864 - val_loss: 0.4806 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 46/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4820 - accuracy: 0.8874 - f1_metric: 0.8878\n",
      "Epoch 00046: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.4713 - accuracy: 0.8878 - f1_metric: 0.8885 - val_loss: 0.4826 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 47/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4494 - accuracy: 0.8847 - f1_metric: 0.8849\n",
      "Epoch 00047: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.4592 - accuracy: 0.8852 - f1_metric: 0.8857 - val_loss: 0.4811 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 48/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.4600 - accuracy: 0.8857 - f1_metric: 0.8862\n",
      "Epoch 00048: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 1.4600 - accuracy: 0.8857 - f1_metric: 0.8862 - val_loss: 0.4843 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 49/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4853 - accuracy: 0.8858 - f1_metric: 0.8857\n",
      "Epoch 00049: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 1.4872 - accuracy: 0.8863 - f1_metric: 0.8864 - val_loss: 0.4852 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 50/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4681 - accuracy: 0.8852 - f1_metric: 0.8855\n",
      "Epoch 00050: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.4510 - accuracy: 0.8868 - f1_metric: 0.8877 - val_loss: 0.4838 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 51/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4629 - accuracy: 0.8863 - f1_metric: 0.8857\n",
      "Epoch 00051: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 65ms/step - loss: 1.4582 - accuracy: 0.8868 - f1_metric: 0.8864 - val_loss: 0.4843 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 52/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4571 - accuracy: 0.8879 - f1_metric: 0.8876\n",
      "Epoch 00052: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.4482 - accuracy: 0.8884 - f1_metric: 0.8883 - val_loss: 0.4835 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 53/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.4647 - accuracy: 0.8878 - f1_metric: 0.8877\n",
      "Epoch 00053: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.4647 - accuracy: 0.8878 - f1_metric: 0.8877 - val_loss: 0.4815 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 54/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4760 - accuracy: 0.8874 - f1_metric: 0.8879\n",
      "Epoch 00054: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 77ms/step - loss: 1.4666 - accuracy: 0.8889 - f1_metric: 0.8901 - val_loss: 0.4772 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 55/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.4892 - accuracy: 0.8863 - f1_metric: 0.8869\n",
      "Epoch 00055: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 1.4892 - accuracy: 0.8863 - f1_metric: 0.8869 - val_loss: 0.4796 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 56/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.5084 - accuracy: 0.8858 - f1_metric: 0.8865\n",
      "Epoch 00056: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.4985 - accuracy: 0.8873 - f1_metric: 0.8887 - val_loss: 0.4818 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 57/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4921 - accuracy: 0.8890 - f1_metric: 0.8892\n",
      "Epoch 00057: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 62ms/step - loss: 1.4984 - accuracy: 0.8884 - f1_metric: 0.8882 - val_loss: 0.4828 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 58/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.4610 - accuracy: 0.8868 - f1_metric: 0.8869\n",
      "Epoch 00058: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 1.4610 - accuracy: 0.8868 - f1_metric: 0.8869 - val_loss: 0.4844 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 59/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.4527 - accuracy: 0.8878 - f1_metric: 0.8880\n",
      "Epoch 00059: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.4527 - accuracy: 0.8878 - f1_metric: 0.8880 - val_loss: 0.4872 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 60/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.4380 - accuracy: 0.8873 - f1_metric: 0.8870\n",
      "Epoch 00060: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.4380 - accuracy: 0.8873 - f1_metric: 0.8870 - val_loss: 0.4836 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 61/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4308 - accuracy: 0.8890 - f1_metric: 0.8890\n",
      "Epoch 00061: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.4445 - accuracy: 0.8873 - f1_metric: 0.8865 - val_loss: 0.4861 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 62/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4725 - accuracy: 0.8885 - f1_metric: 0.8882\n",
      "Epoch 00062: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.4693 - accuracy: 0.8873 - f1_metric: 0.8865 - val_loss: 0.4819 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 63/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.4789 - accuracy: 0.8868 - f1_metric: 0.8878\n",
      "Epoch 00063: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.4789 - accuracy: 0.8868 - f1_metric: 0.8878 - val_loss: 0.4848 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 64/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4675 - accuracy: 0.8869 - f1_metric: 0.8870\n",
      "Epoch 00064: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.4606 - accuracy: 0.8868 - f1_metric: 0.8869 - val_loss: 0.4855 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 65/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4451 - accuracy: 0.8890 - f1_metric: 0.8887\n",
      "Epoch 00065: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 1.4625 - accuracy: 0.8878 - f1_metric: 0.8870 - val_loss: 0.4850 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 66/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.4421 - accuracy: 0.8899 - f1_metric: 0.8904\n",
      "Epoch 00066: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 1.4421 - accuracy: 0.8899 - f1_metric: 0.8904 - val_loss: 0.4838 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 67/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4223 - accuracy: 0.8874 - f1_metric: 0.8876\n",
      "Epoch 00067: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.4251 - accuracy: 0.8873 - f1_metric: 0.8875 - val_loss: 0.4813 - val_accuracy: 0.8884 - val_f1_metric: 0.8858 - lr: 1.0000e-04\n",
      "Epoch 68/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/30 [============================>.] - ETA: 0s - loss: 1.4029 - accuracy: 0.8858 - f1_metric: 0.8858\n",
      "Epoch 00068: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.4042 - accuracy: 0.8873 - f1_metric: 0.8880 - val_loss: 0.4831 - val_accuracy: 0.8863 - val_f1_metric: 0.8836 - lr: 1.0000e-04\n",
      "Epoch 69/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4258 - accuracy: 0.8858 - f1_metric: 0.8857\n",
      "Epoch 00069: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 1.4230 - accuracy: 0.8868 - f1_metric: 0.8872 - val_loss: 0.4840 - val_accuracy: 0.8863 - val_f1_metric: 0.8838 - lr: 1.0000e-04\n",
      "Epoch 70/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4433 - accuracy: 0.8869 - f1_metric: 0.8874 ETA: 1s - loss: 1.2766 \n",
      "Epoch 00070: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.4344 - accuracy: 0.8873 - f1_metric: 0.8880 - val_loss: 0.4829 - val_accuracy: 0.8821 - val_f1_metric: 0.8799 - lr: 1.0000e-04\n",
      "Epoch 71/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.4202 - accuracy: 0.8884 - f1_metric: 0.8888\n",
      "Epoch 00071: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 1.4202 - accuracy: 0.8884 - f1_metric: 0.8888 - val_loss: 0.4810 - val_accuracy: 0.8842 - val_f1_metric: 0.8817 - lr: 1.0000e-04\n",
      "Epoch 72/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.4326 - accuracy: 0.8878 - f1_metric: 0.8874\n",
      "Epoch 00072: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 1.4326 - accuracy: 0.8878 - f1_metric: 0.8874 - val_loss: 0.4832 - val_accuracy: 0.8863 - val_f1_metric: 0.8828 - lr: 1.0000e-04\n",
      "Epoch 73/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4185 - accuracy: 0.8879 - f1_metric: 0.8879\n",
      "Epoch 00073: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.4276 - accuracy: 0.8873 - f1_metric: 0.8869 - val_loss: 0.4796 - val_accuracy: 0.8863 - val_f1_metric: 0.8836 - lr: 1.0000e-04\n",
      "Epoch 74/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3960 - accuracy: 0.8874 - f1_metric: 0.8874\n",
      "Epoch 00074: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.4028 - accuracy: 0.8873 - f1_metric: 0.8873 - val_loss: 0.4812 - val_accuracy: 0.8842 - val_f1_metric: 0.8828 - lr: 1.0000e-04\n",
      "Epoch 75/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4599 - accuracy: 0.8874 - f1_metric: 0.8881\n",
      "Epoch 00075: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 1.4386 - accuracy: 0.8889 - f1_metric: 0.8903 - val_loss: 0.4854 - val_accuracy: 0.8863 - val_f1_metric: 0.8836 - lr: 1.0000e-04\n",
      "Epoch 76/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4062 - accuracy: 0.8874 - f1_metric: 0.8880\n",
      "Epoch 00076: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.4105 - accuracy: 0.8878 - f1_metric: 0.8887 - val_loss: 0.4853 - val_accuracy: 0.8884 - val_f1_metric: 0.8847 - lr: 1.0000e-04\n",
      "Epoch 77/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.4201 - accuracy: 0.8884 - f1_metric: 0.8874\n",
      "Epoch 00077: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.4201 - accuracy: 0.8884 - f1_metric: 0.8874 - val_loss: 0.4851 - val_accuracy: 0.8884 - val_f1_metric: 0.8847 - lr: 1.0000e-04\n",
      "Epoch 78/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4045 - accuracy: 0.8890 - f1_metric: 0.8884\n",
      "Epoch 00078: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 1.3923 - accuracy: 0.8894 - f1_metric: 0.8890 - val_loss: 0.4880 - val_accuracy: 0.8821 - val_f1_metric: 0.8808 - lr: 1.0000e-04\n",
      "Epoch 79/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4508 - accuracy: 0.8890 - f1_metric: 0.8897\n",
      "Epoch 00079: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 65ms/step - loss: 1.4447 - accuracy: 0.8889 - f1_metric: 0.8895 - val_loss: 0.4871 - val_accuracy: 0.8821 - val_f1_metric: 0.8808 - lr: 1.0000e-04\n",
      "Epoch 80/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4225 - accuracy: 0.8869 - f1_metric: 0.8870\n",
      "Epoch 00080: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.4226 - accuracy: 0.8868 - f1_metric: 0.8869 - val_loss: 0.4833 - val_accuracy: 0.8842 - val_f1_metric: 0.8808 - lr: 1.0000e-04\n",
      "Epoch 81/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.4276 - accuracy: 0.8894 - f1_metric: 0.8905\n",
      "Epoch 00081: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.4276 - accuracy: 0.8894 - f1_metric: 0.8905 - val_loss: 0.4833 - val_accuracy: 0.8842 - val_f1_metric: 0.8836 - lr: 1.0000e-04\n",
      "Epoch 82/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3498 - accuracy: 0.8895 - f1_metric: 0.8897\n",
      "Epoch 00082: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.3758 - accuracy: 0.8884 - f1_metric: 0.8880 - val_loss: 0.4844 - val_accuracy: 0.8821 - val_f1_metric: 0.8799 - lr: 1.0000e-04\n",
      "Epoch 83/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4199 - accuracy: 0.8879 - f1_metric: 0.8887\n",
      "Epoch 00083: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 1.4181 - accuracy: 0.8884 - f1_metric: 0.8893 - val_loss: 0.4867 - val_accuracy: 0.8821 - val_f1_metric: 0.8799 - lr: 1.0000e-04\n",
      "Epoch 84/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3998 - accuracy: 0.8901 - f1_metric: 0.8910\n",
      "Epoch 00084: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.3936 - accuracy: 0.8894 - f1_metric: 0.8900 - val_loss: 0.4888 - val_accuracy: 0.8842 - val_f1_metric: 0.8808 - lr: 1.0000e-04\n",
      "Epoch 85/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3979 - accuracy: 0.8874 - f1_metric: 0.8879\n",
      "Epoch 00085: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.3913 - accuracy: 0.8884 - f1_metric: 0.8893 - val_loss: 0.4862 - val_accuracy: 0.8821 - val_f1_metric: 0.8772 - lr: 1.0000e-04\n",
      "Epoch 86/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.3775 - accuracy: 0.8905 - f1_metric: 0.8908\n",
      "Epoch 00086: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 1.3775 - accuracy: 0.8905 - f1_metric: 0.8908 - val_loss: 0.4880 - val_accuracy: 0.8842 - val_f1_metric: 0.8808 - lr: 1.0000e-04\n",
      "Epoch 87/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3609 - accuracy: 0.8869 - f1_metric: 0.8871\n",
      "Epoch 00087: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.3646 - accuracy: 0.8873 - f1_metric: 0.8878 - val_loss: 0.4867 - val_accuracy: 0.8821 - val_f1_metric: 0.8799 - lr: 1.0000e-04\n",
      "Epoch 88/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4125 - accuracy: 0.8890 - f1_metric: 0.8900\n",
      "Epoch 00088: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.4030 - accuracy: 0.8894 - f1_metric: 0.8905 - val_loss: 0.4854 - val_accuracy: 0.8821 - val_f1_metric: 0.8808 - lr: 1.0000e-04\n",
      "Epoch 89/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3842 - accuracy: 0.8895 - f1_metric: 0.8895\n",
      "Epoch 00089: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 1.3895 - accuracy: 0.8889 - f1_metric: 0.8886 - val_loss: 0.4861 - val_accuracy: 0.8821 - val_f1_metric: 0.8808 - lr: 1.0000e-04\n",
      "Epoch 90/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3685 - accuracy: 0.8879 - f1_metric: 0.8881\n",
      "Epoch 00090: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.3627 - accuracy: 0.8894 - f1_metric: 0.8903 - val_loss: 0.4887 - val_accuracy: 0.8821 - val_f1_metric: 0.8799 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3697 - accuracy: 0.8922 - f1_metric: 0.8927\n",
      "Epoch 00091: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 62ms/step - loss: 1.3906 - accuracy: 0.8905 - f1_metric: 0.8900 - val_loss: 0.4886 - val_accuracy: 0.8821 - val_f1_metric: 0.8799 - lr: 1.0000e-04\n",
      "Epoch 92/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.4055 - accuracy: 0.8869 - f1_metric: 0.8873\n",
      "Epoch 00092: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 1.3962 - accuracy: 0.8868 - f1_metric: 0.8872 - val_loss: 0.4895 - val_accuracy: 0.8821 - val_f1_metric: 0.8799 - lr: 1.0000e-04\n",
      "Epoch 93/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3874 - accuracy: 0.8885 - f1_metric: 0.8892\n",
      "Epoch 00093: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.3781 - accuracy: 0.8899 - f1_metric: 0.8913 - val_loss: 0.4853 - val_accuracy: 0.8821 - val_f1_metric: 0.8808 - lr: 1.0000e-04\n",
      "Epoch 94/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3720 - accuracy: 0.8885 - f1_metric: 0.8889\n",
      "Epoch 00094: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.3695 - accuracy: 0.8884 - f1_metric: 0.8887 - val_loss: 0.4892 - val_accuracy: 0.8821 - val_f1_metric: 0.8799 - lr: 1.0000e-04\n",
      "Epoch 95/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3744 - accuracy: 0.8863 - f1_metric: 0.8859\n",
      "Epoch 00095: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 62ms/step - loss: 1.3670 - accuracy: 0.8868 - f1_metric: 0.8866 - val_loss: 0.4878 - val_accuracy: 0.8821 - val_f1_metric: 0.8799 - lr: 1.0000e-04\n",
      "Epoch 96/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3225 - accuracy: 0.8917 - f1_metric: 0.8915\n",
      "Epoch 00096: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.3344 - accuracy: 0.8910 - f1_metric: 0.8905 - val_loss: 0.4882 - val_accuracy: 0.8821 - val_f1_metric: 0.8799 - lr: 1.0000e-04\n",
      "Epoch 97/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3775 - accuracy: 0.8906 - f1_metric: 0.8904\n",
      "Epoch 00097: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 65ms/step - loss: 1.3692 - accuracy: 0.8905 - f1_metric: 0.8902 - val_loss: 0.4886 - val_accuracy: 0.8821 - val_f1_metric: 0.8799 - lr: 1.0000e-04\n",
      "Epoch 98/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3679 - accuracy: 0.8933 - f1_metric: 0.8932\n",
      "Epoch 00098: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.3779 - accuracy: 0.8926 - f1_metric: 0.8921 - val_loss: 0.4904 - val_accuracy: 0.8821 - val_f1_metric: 0.8799 - lr: 1.0000e-04\n",
      "Epoch 99/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.3343 - accuracy: 0.8926 - f1_metric: 0.8936\n",
      "Epoch 00099: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.3343 - accuracy: 0.8926 - f1_metric: 0.8936 - val_loss: 0.4964 - val_accuracy: 0.8821 - val_f1_metric: 0.8799 - lr: 1.0000e-04\n",
      "Epoch 100/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3377 - accuracy: 0.8874 - f1_metric: 0.8876\n",
      "Epoch 00100: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.3307 - accuracy: 0.8884 - f1_metric: 0.8890 - val_loss: 0.4881 - val_accuracy: 0.8821 - val_f1_metric: 0.8769 - lr: 1.0000e-04\n",
      "Epoch 101/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.3724 - accuracy: 0.8899 - f1_metric: 0.8906\n",
      "Epoch 00101: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 1.3724 - accuracy: 0.8899 - f1_metric: 0.8906 - val_loss: 0.4884 - val_accuracy: 0.8821 - val_f1_metric: 0.8796 - lr: 1.0000e-04\n",
      "Epoch 102/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3279 - accuracy: 0.8863 - f1_metric: 0.8871\n",
      "Epoch 00102: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 63ms/step - loss: 1.3303 - accuracy: 0.8868 - f1_metric: 0.8877 - val_loss: 0.4897 - val_accuracy: 0.8821 - val_f1_metric: 0.8799 - lr: 1.0000e-04\n",
      "Epoch 103/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.3581 - accuracy: 0.8910 - f1_metric: 0.8896\n",
      "Epoch 00103: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.3581 - accuracy: 0.8910 - f1_metric: 0.8896 - val_loss: 0.4899 - val_accuracy: 0.8821 - val_f1_metric: 0.8799 - lr: 1.0000e-04\n",
      "Epoch 104/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3663 - accuracy: 0.8852 - f1_metric: 0.8859\n",
      "Epoch 00104: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 1.3606 - accuracy: 0.8857 - f1_metric: 0.8866 - val_loss: 0.4927 - val_accuracy: 0.8821 - val_f1_metric: 0.8799 - lr: 1.0000e-04\n",
      "Epoch 105/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.3500 - accuracy: 0.8942 - f1_metric: 0.8937\n",
      "Epoch 00105: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 1.3500 - accuracy: 0.8942 - f1_metric: 0.8937 - val_loss: 0.4894 - val_accuracy: 0.8800 - val_f1_metric: 0.8769 - lr: 1.0000e-04\n",
      "Epoch 106/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3580 - accuracy: 0.8885 - f1_metric: 0.8890\n",
      "Epoch 00106: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.3440 - accuracy: 0.8884 - f1_metric: 0.8891 - val_loss: 0.4912 - val_accuracy: 0.8821 - val_f1_metric: 0.8808 - lr: 1.0000e-04\n",
      "Epoch 107/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3455 - accuracy: 0.8869 - f1_metric: 0.8859\n",
      "Epoch 00107: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 1.3405 - accuracy: 0.8878 - f1_metric: 0.8874 - val_loss: 0.4892 - val_accuracy: 0.8821 - val_f1_metric: 0.8788 - lr: 1.0000e-04\n",
      "Epoch 108/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3111 - accuracy: 0.8917 - f1_metric: 0.8920\n",
      "Epoch 00108: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.3265 - accuracy: 0.8910 - f1_metric: 0.8909 - val_loss: 0.4909 - val_accuracy: 0.8821 - val_f1_metric: 0.8788 - lr: 1.0000e-04\n",
      "Epoch 109/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3603 - accuracy: 0.8885 - f1_metric: 0.8889\n",
      "Epoch 00109: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.3515 - accuracy: 0.8894 - f1_metric: 0.8903 - val_loss: 0.4916 - val_accuracy: 0.8821 - val_f1_metric: 0.8788 - lr: 1.0000e-04\n",
      "Epoch 110/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3316 - accuracy: 0.8939 - f1_metric: 0.8940\n",
      "Epoch 00110: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 1.3228 - accuracy: 0.8926 - f1_metric: 0.8922 - val_loss: 0.4927 - val_accuracy: 0.8821 - val_f1_metric: 0.8799 - lr: 1.0000e-04\n",
      "Epoch 111/200\n",
      "29/30 [============================>.] - ETA: 0s - loss: 1.3053 - accuracy: 0.8885 - f1_metric: 0.8887\n",
      "Epoch 00111: val_f1_metric did not improve from 0.88658\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.3274 - accuracy: 0.8878 - f1_metric: 0.8878 - val_loss: 0.4913 - val_accuracy: 0.8821 - val_f1_metric: 0.8799 - lr: 1.0000e-04\n",
      "Epoch 00111: early stopping\n",
      "CPU times: user 6min 11s, sys: 55.4 s, total: 7min 7s\n",
      "Wall time: 3min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(x_train, y_train, epochs=params['epochs'], verbose=1,\n",
    "                            batch_size=64, shuffle=True,\n",
    "                            # validation_split=0.3,\n",
    "                            validation_data=(x_cv, y_cv),\n",
    "                            callbacks=[mcp, rlp, es]\n",
    "                            , sample_weight=sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd5gV5fnw8e89p5/tBXZh6R0BKa4oYjcqEiNJjBJsWCJq7In+omkao6nGGBNLTGJ4jZVgjMbYDYJdikjvdWm7bG+nzvP+MYdlgbMLC3tYyv25rnPtnnmm3DNnZu55pjwjxhiUUkqp3VkdHYBSSqlDkyYIpZRSSWmCUEoplZQmCKWUUklpglBKKZWUJgillFJJaYJQ6gCISC8RMSLi3od+rxSRDw90PEodLJog1FFDRNaJSERE8nfr/kVi59yrYyJT6tCkCUIdbdYCk3Z8EZFhQLDjwlHq0KUJQh1t/gFc0ez7ZODp5j2ISJaIPC0iZSKyXkR+LCJWoswlIg+KyHYRWQN8NcmwfxORLSKySUTuFxFXW4MUka4i8qqIVIjIKhG5tlnZaBGZIyI1IrJNRB5KdPeLyDMiUi4iVSIyW0QK2jptpXbQBKGONp8CmSIyOLHj/jbwzG79/BHIAvoAp+EklKsSZdcC5wMjgWLgW7sNOxWIAf0S/ZwDfGc/4nwBKAG6JqbxCxE5M1H2B+APxphMoC8wLdF9ciLu7kAecD3QuB/TVgrQBKGOTjtqEWcDS4FNOwqaJY27jTG1xph1wO+AyxO9XAw8bIzZaIypAH7ZbNgCYDxwmzGm3hhTCvw+Mb59JiLdgbHAD4wxIWPMfOCv7Kz5RIF+IpJvjKkzxnzarHse0M8YEzfGzDXG1LRl2ko1pwlCHY3+AVwCXMlup5eAfMADrG/WbT1QlPi/K7Bxt7IdeiaG3ZI4xVMF/Bno3Mb4ugIVxpjaFmK4BhgALEucRjq/2Xy9BbwgIptF5Dci4mnjtJVqoglCHXWMMetxLlaPB/61W/F2nCPxns269WBnLWMLzimc5mU7bATCQL4xJjvxyTTGDGljiJuBXBHJSBaDMWalMWYSTuL5NTBdRNKMMVFjzM+MMccAJ+GcCrsCpfaTJgh1tLoGONMYU9+8ozEmjnNO/wERyRCRnsD32HmdYhpwi4h0E5Ec4K5mw24B3gZ+JyKZImKJSF8ROa0tgRljNgIfA79MXHg+NhHvMwAicpmIdDLG2EBVYjBbRM4QkWGJ02Q1OInObsu0lWpOE4Q6KhljVhtj5rRQfDNQD6wBPgSeA55KlP0F5zTOl8A89qyBXAF4gSVAJTAd6LIfIU4CeuHUJl4G7jHGvJsoGwcsFpE6nAvW3zbGNAKFienV4FxbmYlz2kmp/SL6wiCllFLJaA1CKaVUUpoglFJKJaUJQimlVFKaIJRSSiV1RDUtnJ+fb3r16tXRYSil1GFj7ty5240xnZKVHVEJolevXsyZ09Kdi0oppXYnIutbKtNTTEoppZLSBKGUUiopTRBKKaWSOqKuQSQTjUYpKSkhFAp1dCiHNb/fT7du3fB4tHFQpY4WR3yCKCkpISMjg169eiEiHR3OYckYQ3l5OSUlJfTu3bujw1FKHSRH/CmmUChEXl6eJocDICLk5eVpLUypo8wRnyAATQ7tQJehUkefoyJBtMYYw7aaELWhaEeHopRSh5SjPkGICNvrwtSGYh0dilJKHVKO+gQB4LYsYvHUvBejqqqKxx57rM3DjR8/nqqqqr33uJsrr7yS6dOnt3k4pZTanSYIwGUJMTs1b2ZsKUHEYq3XWF5//XWys7NTEpNSSu2LI/421+Z+9p/FLNlcs0f3UDSOAQIeV5vHeUzXTO75WsvvpL/rrrtYvXo1I0aMwOPx4Pf7ycnJYdmyZaxYsYKvf/3rbNy4kVAoxK233sqUKVOAne1K1dXVcd5553HyySfz8ccfU1RUxCuvvEIgENhrbO+99x533HEHsViM448/nscffxyfz8ddd93Fq6++itvt5pxzzuHBBx/kn//8Jz/72c9wuVxkZWUxa9asNi8LpdSR5ahKEC0REWw7NaeYfvWrX7Fo0SLmz5/P+++/z1e/+lUWLVrU9DzBU089RW5uLo2NjRx//PFceOGF5OXl7TKOlStX8vzzz/OXv/yFiy++mJdeeonLLrus1emGQiGuvPJK3nvvPQYMGMAVV1zB448/zuWXX87LL7/MsmXLEJGm01j33Xcfb731FkVFRft1akspdeQ5qhJES0f6W6tDlNWGGVqUmfLbOUePHr3Lw2aPPPIIL7/8MgAbN25k5cqVeySI3r17M2LECACOO+441q1bt9fpLF++nN69ezNgwAAAJk+ezKOPPspNN92E3+/nmmuu4fzzz+f8888HYOzYsVx55ZVcfPHFfPOb32yPWVVKHeb0GgTgdgkGQzxFtYjm0tLSmv5///33effdd/nkk0/48ssvGTlyZNKH0Xw+X9P/Lpdrr9cvWuN2u/n888/51re+xWuvvca4ceMAeOKJJ7j//vvZuHEjxx13HOXl5fs9DaXUkeGoqkG0xG05tYaYbXC3/TJEqzIyMqitrU1aVl1dTU5ODsFgkGXLlvHpp5+223QHDhzIunXrWLVqFf369eMf//gHp512GnV1dTQ0NDB+/HjGjh1Lnz59AFi9ejUnnHACJ5xwAm+88QYbN27coyajlDq6aIJg1wTR3vLy8hg7dixDhw4lEAhQUFDQVDZu3DieeOIJBg8ezMCBAznxxBPbbbp+v5+///3vXHTRRU0Xqa+//noqKiqYMGECoVAIYwwPPfQQAHfeeScrV67EGMNZZ53F8OHD2y0WpdThSYxJ/WmVg6W4uNjs/ka5pUuXMnjw4FaHa4zGWbmtlp65QbKC3lSGeFjbl2WplDq8iMhcY0xxsjK9BkFqaxBKKXW40lNMHJ4J4sYbb+Sjjz7apdutt97KVVdd1UERKaWONJogcJ6DcFtCLJ6ap6lT4dFHH+3oEJRSRzg9xZTgsqzDqgahlFKppgkiwe0STRBKKdWMJogE5xSTJgillNohZQlCRLqLyAwRWSIii0Xk1iT9iIg8IiKrRGSBiIxqVjZZRFYmPpNTFecObssinqIWXZVS6nCUyhpEDPi+MeYY4ETgRhE5Zrd+zgP6Jz5TgMcBRCQXuAc4ARgN3CMiOSmMtekUU0c/F5Kent5i2bp16xg6dOhBjEYpdTRLWYIwxmwxxsxL/F8LLAWKduttAvC0cXwKZItIF+Bc4B1jTIUxphJ4BxiXqljh8LzVVSmlUumg3OYqIr2AkcBnuxUVARubfS9JdGupe7JxT8GpfdCjR4/WA3njLti6MGlRlm3ji9pYXhe0pUXXwmFw3q9aLL7rrrvo3r07N954IwD33nsvbrebGTNmUFlZSTQa5f7772fChAn7Pk2c5rxvuOEG5syZg9vt5qGHHuKMM85g8eLFXHXVVUQiEWzb5qWXXqJr165cfPHFlJSUEI/H+clPfsLEiRPbND2l1NEn5QlCRNKBl4DbjDF7vq3nABljngSeBKepjf0dT1Mz38a0LUHsxcSJE7ntttuaEsS0adN46623uOWWW8jMzGT79u2ceOKJXHDBBW1qavzRRx9FRFi4cCHLli3jnHPOYcWKFTzxxBPceuutXHrppUQiEeLxOK+//jpdu3blv//9L+A0EqiUUnuT0gQhIh6c5PCsMeZfSXrZBHRv9r1botsm4PTdur9/wAG1cqQfjcZZs62WHrlBstuxPaaRI0dSWlrK5s2bKSsrIycnh8LCQm6//XZmzZqFZVls2rSJbdu2UVhYuM/j/fDDD7n55psBGDRoED179mTFihWMGTOGBx54gJKSEr75zW/Sv39/hg0bxve//31+8IMfcP7553PKKae02/wppY5cqbyLSYC/AUuNMQ+10NurwBWJu5lOBKqNMVuAt4BzRCQncXH6nES3lGm6BpGCW10vuugipk+fzosvvsjEiRN59tlnKSsrY+7cucyfP5+CgoKk74HYH5dccgmvvvoqgUCA8ePH87///Y8BAwYwb948hg0bxo9//GPuu+++dpmWUurIlsoaxFjgcmChiMxPdPsh0APAGPME8DowHlgFNABXJcoqROTnwOzEcPcZYypSGCsuSxBS87DcxIkTufbaa9m+fTszZ85k2rRpdO7cGY/Hw4wZM1i/fn2bx3nKKafw7LPPcuaZZ7JixQo2bNjAwIEDWbNmDX369OGWW25hw4YNLFiwgEGDBpGbm8tll11GdnY2f/3rX9t9HpVSR56UJQhjzIdAqyfVjXNP6Y0tlD0FPJWC0JISEVwuIZaCZyGGDBlCbW0tRUVFdOnShUsvvZSvfe1rDBs2jOLiYgYNGtTmcX73u9/lhhtuYNiwYbjdbqZOnYrP52PatGn84x//wOPxUFhYyA9/+ENmz57NnXfeiWVZeDweHn/88XafR6XUkUffB9HMim21eF0WvfLT9t7zUUjfB6HUkUffB7GP3Ja2x6SUUjtoc9/NuC2Lxmiso8Ng4cKFXH755bt08/l8fPbZ7o+RKKVU6miCaMbtEmKhjq9BDBs2jPnz5++9R6WUSiE9xdSM2xLixmDraSallNIE0ZzLpe0xKaXUDpogmnFbzuLQZr+VUkoTxC52PE0djrVvgnjkkUcYPHgwF154IWPGjMHn8/Hggw+26zSUUqq96UXqZgIeFz63xdbqEBl+Ny6rffLnY489xrvvvovX62X9+vX8+9//bpfxKqVUKmkNohnLErrlBInGbTZX7WwbKW4b4vt5XeL6669nzZo1nHfeeTz77LMcf/zxeDye9gpZKaVS5qiqQfz681+zrGLZXvuLxGyicRuf28I2EI07p5y8bguPa9ecOih3ED8Y/YMWx/XEE0/w5ptvMmPGDPLz8w9sBpRS6iA6qhLEvvK6LeK2aboW4Upcm4jEbGK2wee2sNrxnRFKKXUoOqoSRGtH+rsLR+NUNkTICXrxeVwYY6hsiLKluhGPy6Jf53RNEkqpI5peg2iBz+OiMCuAz+MCnNZec9O8dM8NEorGKa1pn/c3KKXUoeqoqkG0h0y/h5ygl7LaMJkBD0Hvvi/CrVu3UlxcTE1NDZZl8fDDD7NkyRIyMzNTGLFSSu0fTRD7oUu2n7pwjJKKRvp2Tm+6RtGSdevWNf1fUlKS4uiUUqp96Cmm/eC2LLrlBAjF4izdUsP68npqGqMcSe/WUEopTRD7KcPvoW+ndHKCHurDcdaV11PZEGmxf9sYbGM0iSilDht6iukApPncpPncdM02rN1ez+aqEAGPm4DX1dSPbRvK6sKU1YaxjUEQPC6hV34afo+rlbErpVTH0hpEOxARuucGcVnChop64rbzoF1FfZgV22rZVuM03VGY6adTho+4bdharXdBKaUObSmrQYjIU8D5QKkxZmiS8juBS5vFMRjoZIypEJF1QC0QB2ItvS/1UOJxWfTIDbKmrJ7lW+uIJVqE9Xtc9MlJJ92/c1GLwLaaEPXhGGk+N7YxbKxowBKhW04A0ecrlFKHgFTWIKYC41oqNMb81hgzwhgzArgbmGmMqWjWyxmJ8kM+OeyQ5nPTNceP32NRmOWnf+d0+nfeNTkA5Kf7cFtOo4DGGDZVNlLdGKWyIUJNY7SDoldKqV2lrAZhjJklIr32sfdJwPOpiuVgykvzkZfma7UflyUUZPrYVNXI+vIGakJROmf4qQ1F2VwdIr1ZS7Jx2263VmWVUqotOnzPIyJBnJrGS806G+BtEZkrIlP2MvwUEZkjInPKyspSGWq7yknz4nVb1ISi5AS9FGT6KMoJcFy/rmytDhOJxVlfXs/izTWU1e68XnHnnXcyZMgQ7rzzTmbNmsWoUaNwu91Mnz69A+dGKXUkOhTuYvoa8NFup5dONsZsEpHOwDsisswYMyvZwMaYJ4EnAYqLiw+be0gtEbrnBKlujFKY5UdECHrdiEB5fZiKhggCBL1utlSHsETIS/fx5JNPUlFRgcvlYt26dUydOlVfPqSUSolDIUF8m91OLxljNiX+lorIy8BoIGmCaIutv/gF4aV7b+67LXyDB1H4wx+2WH7XXXfRvXt3brzxRgDuvfde3G43M2bMoLKykmg0yv3338+ECRMAmpKC121RmOnH7RI2lDewqaqRyyZeSF1dHccddxx33303EydOBMDSU1BKqRTo0D2LiGQBpwGvNOuWJiIZO/4HzgEWdUyEB27ixIlMmzat6fu0adOYPHkyL7/8MvPmzWPGjBl8//vf3+UBun6d0+mRG8SbaFa8R26QdJ+b3/z5WQKBAPPnz29KDjsYY4jE4vognlKq3aTyNtfngdOBfBEpAe4BPADGmCcSvX0DeNsYU99s0ALg5cStnm7gOWPMm+0RU2tH+qkycuRISktL2bx5M2VlZeTk5FBYWMjtt9/OrFmzsCyLTZs2sW3bNgoLC5OOw7KEnnlBVpbWYRuIxW3ciRcXhaNxGiNxNlc3smxrLX6Pi8JMPxl+t94uq5Q6IKm8i2nSPvQzFed22Obd1gDDUxNVx7jooouYPn06W7duZeLEiTz77LOUlZUxd+5cPB4PvXr1IhRq/cE5l2XRMzcIwMbKRnrkBthWE6a8LkIoGsdtObfWVtZHWVdej9/tampEMOB10SVxnUMppfbVoXAN4og3ceJErr32WrZv387MmTOZNm0anTt3xuPxMGPGDNavX79P4wl43VgCtaEoS7fEsI0hN81LVtBDQaafzhl+OqX7qGyIUNXgPE9hDGyvc5r5KMrWh/CUUvtOr24eBEOGDKG2tpaioiK6dOnCpZdeypw5cxg2bBhPP/00gwYNatP4ctO8+D0uqtcv5cRhA3hp+nSuu+46hgwZknixkY8+ndLp0ymdvp3T6Zzhp6I+wraa8AHNhzGwpqzugMahlDp8aA3iIFm4cGHT//n5+XzyySdJ+6ura30H3Ly8X+cx+/R+iYJMH7G4TWltiLhtyA56CHpdNETilNeFqY/EyU3z0indh5Xk3Ra2MVQ1RCmtDXHl0zP53UXDufC4bnudrlLq8KYJ4iggIhTlBACoaIhQXh/GZQlx2+CyBL/HxbaaEJX1EQqz/GQFPE2nohrCMTZUNhCJ2QgwsCCDX76xlK8cU0BWwNOBc6WUSjVNEIeghQsXcvnll+/Szefz8dlnn+33OEWEbrlBCm2b2lCMulCMoNdFdtCLyxLqwjG2VDWyoaKBoNdN1yw/DZE4W2pCeCyhV14anlo/v7u4Dxf86UN+/84K7r1gyIHOqlLqEHZUJAhjzGF1cXbYsGHMnz8/JeN2WxY5QS85Qe8u3dN9bvp1TqeyIcrW6hCrEtcaMv0euuUEmu6IGlqUxWUn9uTpT9ZxUXE3hnTNSkmcSqmOd8RfpPb7/ZSXl+sDZPvAucDtZWBhBgWZfrpmB+iZ57znory8HL/fD8D3zx5ITtDLD19eRE1IW59V6kglR9KOs7i42MyZM2eXbtFolJKSkr0+Z6Ba5/f76datGx6Pc93htQWbufWF+RRlB/jjpJEM757dwREqpfaHiMxt6bUKR/wpJo/HQ+/evTs6jCPO+cd2pTDTz60vzOfCxz/m8jE9OalvPqN6ZJOX3npz50qpw8MRX4NQqVXdEOXHryzizUVbiMaddWl4tywmjCjia8O70ilDk4VSh7LWahCaIFS7CEXjLNxUzedrK/jvgi0s2VKDxyU8ffUJjOmb19HhKaVaoAlCHXQrt9VyxVOf0z03yLTrxnR0OEqpFrSWII74u5hUx+hfkMG1p/Th87UVzF5XsfcBlFKHHE0QKmW+Pbo7uWleHpuxqqNDUUrtB00QKmWCXjdXj+3FjOVlLN5c3dHhKKXaSBOESqnLx/Qi3efm4XdXsm57PaU1IWJxu6PDUkrtgyP+OQjVsbICHi4f05PH31/NO0u2ATCoMIOXbjiJNJ+ufkodynQLVSl361n9Gdk9m7pwjK01IX771nLu+88Sfv2tYwGobozyl1lrmHRCD4qyAx0crVJqB00QKuX8HhfnDNn5vu26UIzH3l/NaQM7Mbx7Nlf9/XNWbKtjXXk9f7pkVNJx3PTcPDpn+Pnp145JWl5aE2LhpmrOGlyQknlQ6mikCUIddLefPYCPVm3nrpcW4Pe4aIzE+crgAv67cAu3ldbRr3P6Lv3P21DJawu2IAIXHle0RwuysbjNtU/P4cuSan52wRAmn9TrIM6NUkeulF2kFpGnRKRURBa1UH66iFSLyPzE56fNysaJyHIRWSUid6UqRtUxPC6LP3x7ZNMLi6bfcBK/vnAYfrcr6S2xT85cQ6bfTXbAwy9fX7ZHy7x/+WAtX5ZUM7Agg3v/s5g3F205WLOi1BEtlTWIqcCfgKdb6ecDY8z5zTuIiAt4FDgbKAFmi8irxpglqQpUHXy98tN487ZTyfR7yAo6LcRedmIPnvpoHbec1Z9e+WmA8w7st5Zs5cbT+5Gb5uW+15Ywc0UZpw/sDMCq0lp+/84Kxg0p5PcTR3DpXz/llhfmc+0p1azYVseSzTX0zk9jwoiujBtaSIZf34Kn1L5KWQ3CGDML2J9HaEcDq4wxa4wxEeAFYEK7BqcOCd1zg03JAeDaU/vgsoTH3t9Zi/jLB2vwuCwmn9SLy07sSc+8IL98fRm1oSgrttVyxz8XEPS5+PnXhxLwuvjb5OPpkRvk0RmrWVVax4ge2WyoaODO6QsY/cB7fLamvCNmVanDUkdfgxgjIl8Cm4E7jDGLgSJgY7N+SoATOiI4dXB1zvAz6fjuPPPZBoyBswYX8NLcTVxU3K2pVdj/O3cQNz43j2H3vt003B++PaKpPCfNy1u3nUpdKNaUfIwxfLGxittfnM8d07/krdtOJehN/apfXhfmpXklTJtTwobyBhDwuix+ev4xXHx895RPX6kD1ZEJYh7Q0xhTJyLjgX8D/ds6EhGZAkwB6NGjR/tGqA66288eQEMkzusLt/DPuSWIwLWn9GkqHz+skB+OH0Q0buieG2RAQTqDCjN3GYfLkl1qJiLCqB45/PZbw5n45Cf85s3lrb5PuzYU5bH3V3PpCT3olhNs8zwYY3js/dU8/O4KonHDcT1zuOrkXgB8uqaCe15dzJi+eXTPbfu4lTqYUtqaq4j0Al4zxgzdh37XAcU4SeJeY8y5ie53Axhjfrm3cWhrrkeOxkict5dsRUS4YHjXdhvvva8uZurH63hhyomc2GfPZsijcZurp87mg5XbOb5XDi9OGYNl7fv7zI0x/OqNZfx51hq+OqwLt36lPwMKMprKN1U1cvZDMxndO5e/X3l8m96VXh+O8eqXm1lTVsd1p/UlX1/MpNrBIdmaq4gUSmLrEJHRiVjKgdlAfxHpLSJe4NvAqx0Vp+oYAa+LCSOK2jU5APzfuIH0yA3y/WlfMnNF2S53RBlj+Mm/F/HByu2cN7SQ2esqeeaz9XsdZ9w2bK8Ls2JbLT95ZRF/nrWGK8b05I+TRu6SHACKsgPccc5A3l9exmsL9u1uq4ZIjHteWcQJv3iPu/+1kL9+uJbzH/mQueu1lVyVWimrQYjI88DpQD6wDbgH8AAYY54QkZuAG4AY0Ah8zxjzcWLY8cDDgAt4yhjzwL5MU2sQal/M31jFjc/OY1NVI8f3ymHCiCLitmHZ1hqe/3wjN53Rj++fM4DJf5/NnHUVvHXbqXTPDbKtJkQoGqdnnnOHlTGGl7/YxM/+s4TqxmjT+K8/rS8/GDewxdpB3DZ847GP2FzVyG8vGs4p/fJxu5Ifq4Wica59eg4frdrO10cWcekJPfB7XHz32Xlsqmzk6yOLiMRsKhsifGVwAVeM6dmmWolS+sIgpXYTjsWZNnsjf/zfKkprw03dLy7uxq8vPBYRoaSygXN/P4seeWl4XMKCEqdF2hP75HLJCT15c9EWXl+4leKeOXxteFdy07x0zw0yvFvWXnfSS7fUcOlfP6OiPkJempevjyzi5jP7kR30NvUTjdvc8Mw83l26jQcvGs63juvWVFbdGOWH/1rIByvLyEnz4rKENWX1TBrdg/smDMHTQsJRaneaIJRqQSRmU1YXxu+28HtcezQg+NxnG/jRvxcyons2XxlcgAg8++kGNlU14nEJ3zt7IFMSt+e2VTgWZ+byMl6Zv5k3F28lK+DhR+MHc9bgzny8upwXZm9k1ooyfj5hCJeP6dXquGzb8ODby3ns/dWc0j+fP10yiqxAxz/zEYrGeeidFUwY0XWPJ+DVoUEThFIHIBSN4/e4mr7HbcPHq7fTJSuwR7Mg+2vplhp+9PJC5m2oauqW4XNz+9kDuPrk3vs8nhdnb+BHLy+iINPPI5NGcFzP3Bb7rQvHeGfJVoZ3y6ZPp53zUR+OsXZ7Pf06p+8y3zss2VzDzc/P40dfHcyZg1pv++qZT9fz438vIsPn5m9XHs/o3i3HozqGJgilDgO27VzTKKlsZGy/PIZ3z96vU0XzNlRy6wtfsKmykVvPGsDNZ/bb5U6s6oYoUz9ex1MfraW6MYrHJXznlD5MOaUPL80r4bH3V1NRH8FtCYO6ZHDRcd2b2reKxm0m/OkjlmypIeh1Mf36kzima2bSOGzbcNZDM/G5LSJxm81VjU2n777cWEWnDB/Xn9Z3v5aVaj+aIJQ6ytSGovz0lcW8/MUmxg8r5KGLR+D3uJi7voIbnplHaW2YrwwuYPJJPXll/mamJ545MQZO6Z/PN0cVsaq0jo9WlTN/YxU/HD+IKaf25ZH3VvLQOyv4+YQhPDpjNZbAv28ay8aKRl74fANpPjc/Pf8YLEt4e/FWpvxjLn+cNJIxffO44m+fs2RLDUDTtN667VQGFjp3etm24ZnP1nNq/05NTa2o1NMEodRRyBjD3z5cy/3/XcroXrmcO7SQX72xlK7ZAf40aRTDuu28JjBnXQX/nr+J8cO6cFLf/Kbucdtw6wtf8NqCLUw5tQ9//2gt44Z24Y+TRrJoUzUXPfEJItAQieNzW4RjNjec3pcfjBvExU98wqaqRmbeeTpul0VtKMqsFdvplR+kc4afU38zg/HDuvC7i4cD8PIXJdz+4pcUZPqYdt2YprvFVGppglDqKPafLzfz/WlfEonbnD6wE3+YOHKXJ833xnNYS6gAACAASURBVLmbai7vLi0lL83LO987jdw0526r95Zu428fruX8Y7tywYiu/PL1pTz72QauPKkXUz9ex0/OP4ZrWriGcs8ri3ju8w188H9nkh30cOaD7xP0uSmvCxP0upl2/Zh9eoFUSWUDP31lMRcM78oFw7u26cFGpQlCqaPe3PWVLNlczSUn9NyvO65C0Ti/fnMZZw8u4KR++S321/xJ9Ay/m0/uPov0Fl4tu7GigdN+O4NrT+lDZsDDb99azgtTTiTd52bSXz4l0+/h1AH5dMrwM6gwg3FDCvfY+RtjuHrqbGYsLwNgRPdsfvq1YxjVI2eP+X978VZmr6tgVWkdV57Ui1u/MmC/lsWRRhOEUuqgqQlFmfL0HL4yuIDvNGtHK5mbnpvHzOVl2MZwUr98/nKFs5/6YkMl9/5nCZsqGymvD2MMHNsti3u+NoTjeu7c+b+5aAvXPzOPH44fRE7Qy2/fWk55fYS/Ti7mjEST8DNXlHH11Nm4RDi2WxYZfjczlpdxSv98Hp44gryjvMmSA04QIpIGNBpjbBEZAAwC3jDGRPcy6EGlCUKpw8vCkmq+9qcPcVvC27efusvttjvE4jb/XbiFX7y+lG01YSaM6Mod5wwkJ83LV343k5w0L/+5aSxul0VNKMqkJz9l7fZ6pl03Bq/b4sLHPqYoJ8C068eQ6fdgjGHanI385JXFZPo9XFzcjW+OKqJf54wkEe4UjdvMWlHGv+ZtIm4bHr10VIs1kO114cOmraz2SBBzgVOAHOAjnPaSIsaYS9sz0AOlCUKpw8/d/1pAt5wgN57Rr9X+6sMxHp2xir99uBZjYHCXDBZsqualG07a5ZRSaU2Ibzz2MZG4jc9tEYravHLT2D2uZyzaVM1v31rOByvLsA30yA3SNdtPl6wAk0b32OWZjfkbq/jO/5vD9row6T43deEYv/3WsVxUvGez7a/M38RtL87nsUtGcd6wLge4dFKvPRLEPGPMKBG5GQgYY34jIvONMSPaO9gDoQlCqSPf5qpGfv/OCqbPK+GS0T144BvD9uhn5bZaLnz8Y8IxmxevG8OI7tktjq+0NsR/vtzCvPWVbK0JsaasjphteP2WU+ieG6QhEmP8Hz4gGjfce8EQThvQiYue+JjS2jAz7jh9l4cJGyIxznxwJltrQnTPDfDu907D53bKZ60o4/WFW9haE2J7XZjh3bK5+uTe9E1SazqY2iNBfAF8F/g9cI0xZrGILDTG7PnLdCBNEEodPUprQ+Sl+Vo8zbO6rI7GSJyhRW1r4mNDeQPjH/mAgYUZvDjlRH7+2hL+3yfref7aExnT12ki/uPV27nkL59x93mDuK7Zw34Pv7uCh99dyW1f6c/D765sen7kk9XlXPHUZwS9brrnBsgOePl8XQWRmM1Zgzpz7pDCvb4jZFtNiJ+/toSrT+69x0X4A9FagtjXFwbdBtwNvJxIDn2AGe0VoFJKtVXnDH+r5ft7ZN4jL8gD3xjKrS/M5/pEY4lXje3VlBwATuqbz+kDO/HojFVMPL472UEvW6tD/Hmm8x6Q274ygAUl1fzxvVUc1zOH65+ZS4/cIP/67timNrLKasM88+l6nvt8A+8tKwWgMNNPv87p9MoPcmKfPMYP7YJlCRX1ES7762esLK3jk9XlvHLT2P16mVVbtfkuJhGxgHRjTE1qQtp/WoNQSrWXO/75JdPnltAnP43/3nIKAe+u7VIt3VLD+Ec+YHi3bE7oncuSLTV8traC9753Gt1zg6wqreXchz/AGEN20Mu/vzuWHnl77tSNMU07/i82VLK2vIE1ZXXUhmKM6pHNnecO4oHXl7ByWx0/u2AID7y+lG45QV66YUzTq3OjcXu/W/Btj1NMzwHXA3GcC9SZwB+MMb/dr4hSRBOEUqq91Idj/OL1pVxyQo8WW6L96wdrmDZnI+u2NxCJ29x0Rj/uOHdgU/nP/rOYZz/bwHPfOYHiXvveUKFtG6bPK+E3by5je53TLtZfrijmjEGdeX95KVdPnc0JvfPIz/CxeHM1tm14/84z9ms+2yNBzDfGjBCRS4FRwF3AXGPMsfsVUYpoglBKdYS4bSitDVGQ4d/lYT7bNlQ3RslJ87YydMuqG6P87cO1jOyezRmDOjd1n/rRWh54fSmdM/wc0zWToV2zuOWsfvv1sqj2SBCLgRHAc8CfjDEzReRLY8zwNkeTQpoglFJHi1jcbvFNhG3RHu+k/jOwDkgDZolIT+CQuwahlFJHi/ZIDnudxr70ZIx5BHikWaf1IrJ/J7yUUkodFvYpBYlIlog8JCJzEp/f4dQmlFJKHaH2tY7yFFALXJz41AB/T1VQSimlOt6+PijX1xhzYbPvPxOR+a0NICJPAecDpcaYoUnKLwV+AAhO8rnBGPNlomxdolsciLV0AUUppVTq7GsNolFETt7xRUTGAo17GWYqMK6V8rXAaYnmOn4OPLlb+RnGmBGaHJRSqmPsaw3ieuBpEdnxtEglMLm1AYwxs0SkVyvlHzf7+inQbR9jUUopdRDsUw3CGLPjmYdjgWONMSOBM9sxjmuAN5pPEnhbROaKyJTWBhSRKTsunpeVlbVjSEopdXRr0420xpiaZm0wfa89AkjcLnsNzvWIHU42xowCzgNuFJFTW4npSWNMsTGmuFOnTu0RklJKKdqYIHZzwC9zFZFjgb8CE4wx5Tu6G2M2Jf6WAi8Dow90WkoppdrmQBLEAb3MWkR6AP8CLjfGrGjWPU1EMnb8D5wDLDqQaSmllGq7Vi9Si0gtyROBAIEk3ZsP+zxwOpAvIiXAPYAHwBjzBPBTIA94LNHA1I7bWQuAlxPd3MBzxpg3932WlFJKtYdWE4QxpvW3eLc+7KS9lH8H+E6S7muAQ6oRQKWUOhqlvrUnpZRShyVNEEoppZLSBKGUUiopTRBKKaWS0gShlFIqKU0QSimlktIEoZRSKilNEEoppZLSBKGUUiopTRBKKaWS0gShlFIqKU0QSimlktIEoZRSKilNEEoppZLSBKGUUiopTRBKKaWS0gShlFIqKU0QSimlktIEoZRSKilNEEoppZJKaYIQkadEpFREFrVQLiLyiIisEpEFIjKqWdlkEVmZ+ExOZZxKKaX2lOoaxFRgXCvl5wH9E58pwOMAIpIL3AOcAIwG7hGRnJRGqpRSahcpTRDGmFlARSu9TACeNo5PgWwR6QKcC7xjjKkwxlQC79B6olFKKdXOOvoaRBGwsdn3kkS3lrrvQUSmiMgcEZlTVlaWskCVUupo09EJ4oAZY540xhQbY4o7derU0eEopdQRo6MTxCage7Pv3RLdWuqulFLqIOnoBPEqcEXibqYTgWpjzBbgLeAcEclJXJw+J9FNKaXUQeJO5chF5HngdCBfREpw7kzyABhjngBeB8YDq4AG4KpEWYWI/ByYnRjVfcaY1i52K6WUamcpTRDGmEl7KTfAjS2UPQU8lYq4lFJK7V1Hn2JSSil1iNIEoZRSKilNEEoppZLSBKGUUiopTRBKKaWSSuldTEopdaSxjU1dtI7aSC21kVoyvZl0SeuCiLR5XDE7xprqNRSmFZLpzdzn4YwxxOwYHpenzdNsC00QQLyqCnAW+oqqFVQ0VhKzY8RMlKgdI2bHiNoRIvEokXgYwcLr8uJ1eXFZLiyxsMTCNjZxO46I4HN58Vo+Z/wmRtSO0RhroD7SQNgOk+5JJ8uXhd/lJ2oihGORpuF8lh/b2E3Tj9pRYnYUOxbHikSxIjEssfAG0vEFM4g1NhCpLCdWV4MnmEEgtxOeYDr127dSX7YZ6hrIIkAmATwuD8brIe51EbGjNMZDRGJhrEgMiURxG4tgWhZp6Tl4vH6MMdjGIAKWOBXOSDxCOBYmbEeI2zFiJk44FqI+Vk99tAG/y0deII9sX7azjMSFYBG1I0TjUcJ2hHA8TCQewWVZBNwB/JafkB2iLlJPY6wRg9n1RzImMZ0wUTuCx/KS5gkScAdwWW5cYuGyXLjEhSUuPJYbn8uPz+0jGo9QF6mjPlZPbcTZsKN2hGxfNjn+XALuALaJE7fjxLExxsY2NuF4mMZYiGg8is/tI+DyA0J9rI66aD22beNO/P710XqqI9U0xhrJ9mWTH8gn6E6jIbFMYnbUic1y4bN8BNx+fC4fUTtKKB4mbsfwWF58bh8ey4NbXLgsN1Zip2MwhGJhQrFGInYUr+XB4/LiFhcGMMamNlJLaUMpFeFKgu4guf4cMv3ZhDJ81Gd4iAY9+C0fPstLzO8mLHFC8RCReIRQLETMxAi6g6R70wm6g7jFhTfmTD/mEeLGpqyxjM3VG6mq3kZG3EuuCZJFgIA7QNATJB70sS09TnW0xhmnHSNu4uT4cygMFpLuTWdj7UbWV62jIVJHfkYBhcFCXOKiLlpHfbSeUCxEOB4makebtjOgaYfstbwUZRTRJa0LjbFGttRvYXvjdjI8GeT4cwi6g9REaqgKVxGOh7HEwiUusn3ZdE3vSl4gj011m1hZuZLShlLyA/kUBAvoFOxEfiCfXH8u1eFq1lSvYW31WmK2s70ZDNXhaqrD1cRNfJfVM9ObyaDcQYgINeEaGmIN5AfyKUovItObSU2khspQJS7LRVF6EQXBApZVLOPDTR9SE6kBoCi9iP7Z/cnx55DtyyZqR1lfs54NtRsIxUJN21JdtI6qUBVxE6d7RncG5g5kUO4grh127X4lqdaI8yjCkaG4uNjMmTOnzcMtGzkK09iYgoiUOjTFLNiSK2zt5MKFi8xGSGu08YTjeCM23ij4o7sOE3GDZYPbbn3cIa9Q2tlLOOhBRBCEqB0hHI/gihk611nkVttYBiryPGzKFeqCgttyO4k9buGNgcsWqnK9lHXyEPJZFFYaOm2PYjVGqLMiVEsIr3GRaXwEbTfxeNw5mCIOfh/4vFhuDzYGgyEcD9MQbSASC5OJn2wJEsRHoxWjzopSb0KE7AjG2IgIaZ40MjwZWJbFjuMVj8uDz+XDa3nxuDx4LA+NPmFDWohV/mr8YUOnWiGzNk4k3EBjpN45wLCcRGcbm4ZYg3PQmREgu/cAuvUcRnhzCeFVqzDllVT74pT7ooR9Ftn+bLJ82XgtD3ETxzY2gZiL9JgLdyRObaSWqnA1Ib9w/d8+2691QUTmGmOKk5VpDQLofOcdrC5fwfQV0+mX3Y8xXcc01Qpc4mr6uC03LssFOFXDmB1zVj7jrIKCIGI5R7t2nJhxtjDnqNbCmzhCdInLWVljjc7RsHj2GC8CbnHvEoPlcoHPh/h92MYm2lhPpLEOly+AP7cTvsxsQnVVNFSWEa2vIy2vkKyC7rizs6k2DVTYtUTsKK5wDCsSw+fyEnAF8Ln9uPx+xO8jQozqmu1U15YRi4YRxDl6MmATx2DwufyJI2A/bsuNW9z4XT7SvRm4LTcxE6OisZzyxnKidrRpxfa4PHgtnzNddwC/y0/MxGmIObUGv8tPpjeToCfYVFsBEJyjIpfl1ER2iNoR6qMNxI2zzOJ2PFHzihO1IzTGGgnHwnhdXjI8GaR708n0ZuF3+wGIxMNsb9xOKBZyah+WGxeJ391y4Xf58bsDuC0XUTtKQ7QBY0zTfLbGNnEi8UhiWnse1cVNjEjcqQntHJchakcJxyOJI+8YOw/gBL/bWe4ucSfGHyVmYog4y8jv8mOJa9cJxaLEKiqJlW/Hrq1DXBZxbOJl5WSvXkPftWsRjxtXfjZWdhautDQkGAS/H3xejM/rrNvhCITDuN0+JODH8vmx0oJYwSDi9YEIBkO8vILImjXkrl6F3dCwaywGjNvCN7QrnqKu4HKRtXYdRWtWE99St/P39nmx/AHnN56zced4LAtPURFWRidMQxgTCoHbjeX3I14veBPrRjyOaQxjV4QwsVizALyJj0F8PiyfH3G7MdEIdiiMifoSv53dlNjATnyaFijQ7GDSGNKrq8kPhRjVrC8rMxPxehErHazdL/UGsO049pJK+Gg+MN8ZJisLT0FX4hU1xCsqMJEIUJr47CQej7Pc/X4QAdJw5WSTCpoggPLzjufmNx6hxznHcM+4qQQ9wY4Oqd1lAz33sd/ue+9lrzLaML39FQD2/axt8uGzUjittDb2v2M6qRz/4cYYQ6y0DLuuFk+3blg+X0eHtAdjDPGKCqKbt2Clp+EpLMQK7P2XNLZNrLSUWFkZni5dcOXlNZ0iMsZAPJ50OHEfvN32UZ8gqsPV3Py/mwm6gzxy5iNHZHJQ6nAlIngKOkNB544OpUUigjsvD3deXtuGsyw8hYV4CguTjpODmAha0vERdLAMbwYX9L2A07qdRmHanj+UUkodrY76BGGJxXdHfLejw1BKqUOOPiinlFIqKU0QSimlktIEoZRSKilNEEoppZLSBKGUUiopTRBKKaWS0gShlFIqqZQmCBEZJyLLRWSViNyVpPz3IjI/8VkhIlXNyuLNyl5NZZxKKaX2lLIH5UTEBTwKnA2UALNF5FVjzJId/Rhjbm/W/83AyGajaDTGjEhVfEoppVqXyhrEaGCVMWaNMSYCvABMaKX/ScDzKYxHKaVUG6QyQRQBG5t9L0l024OI9AR6A/9r1tkvInNE5FMR+XpLExGRKYn+5pSVlbVH3EoppTh0LlJ/G5huzC6vaeqZeInFJcDDItI32YDGmCeNMcXGmOJOnTodjFiVUuqokMoEsYldXy3QLdEtmW+z2+klY8ymxN81wPvsen1CKaVUiqUyQcwG+otIbxHx4iSBPe5GEpFBQA7wSbNuOSLiS/yfD4wFluw+rFJKqdRJ2V1MxpiYiNwEvAW4gKeMMYtF5D5gjjFmR7L4NvCC2fXl2IOBP4uIjZPEftX87iellFKpJ7vulw9vxcXFZs6cOR0dhlJKHTZEZG7ieu8eDpWL1EoppQ4xmiCUUkolpQlCKaVUUpoglFJKJaUJQimlVFKaIJRSSiWlCUIppVRSmiCUUkolpQlCKaVUUpoglFJKJaUJQimlVFKaIJRSSiWlCUIppVRSmiCUUkolpQlCKaVUUpoglFJKJaUJQimlVFKaIJRSSiWlCUIppVRSmiCUUkolldIEISLjRGS5iKwSkbuSlF8pImUiMj/x+U6zsskisjLxmZzKOJVSSu3JnaoRi4gLeBQ4GygBZovIq8aYJbv1+qIx5qbdhs0F7gGKAQPMTQxbmZJgP3gIep8K3Yp3ditfDcvfAJcXPAEI18C2JbB9BRwzAU66qeXxHajK9ZDRBdze1E1DKaX2ImUJAhgNrDLGrAEQkReACcDuCSKZc4F3jDEViWHfAcYBz7d7lI1V8Onj8N7PnB3/iTfCghdg7v8DE9+137TOEMyDt38ElgtOvKHdw2HdR/D0BTD4Arjo73vvv2YLBLKdJLY/tiyAjx6Gc+6HzK77Nw6l1BEplQmiCNjY7HsJcEKS/i4UkVOBFcDtxpiNLQxblJIoA9lwyzz4+E/w8R9hyStguaH4ajj5NnAHINoAniCk5UE8BtOvhDfvAn82jJiUfLxbFsD/fg4N5VA4DLoMh6HfAn/mzn6+eAaWvArjfgl5faG6BKZd4Ux/8b9gxCXQ/+yWY1/9P3j+EsjpCZf+E7J7tG3eq0vg2YugbitUbYAr/wtu374Nu3k+LJgGIk6y7HESDBzXtukrdTiy41C+CtbOgvUfgy/d2bZ7nexsC60JVTvbXfUm8GVAbm9ILwA7Bo2VEG10DkK9ac62Fd/RvR5iEYiFnP1RpM757kt3xuPLdPYh7UyMMe0+UgAR+RYwzhjzncT3y4ETmp9OEpE8oM4YExaR64CJxpgzReQOwG+MuT/R30+ARmPMg0mmMwWYAtCjR4/j1q9fv/9B126Dpa9C3zNbX9ixsLNjXfch9D8Hio6DgiHg8oAxsOw1mPc0BHKc7lsXOCtG52Pgsn9BZhdY9BJMv8YZnzcNzv0FzHnKObV19Zsw/WqINcJ3PwNvELYuhLUfOAkjvz8sf9NJJjm9nB28yweTXnBWnnn/z1l5PUEnIRUMhTN+BFnNcmy4Fp4a5ySGsbfA/+6H4mvg/Id2ndeSubBoujOOYy925vHLF+DVW5xyl8dZHnYUzvwxnHKHs2KnUjy2MzGBs8FGG5w44lFnY8OAWCAuZyPypjvD2DaEqpwNbMeGFY84p/Uq1yU2xsS4PAHwZzm/jx1z+jPGWa4ev1OW1gmC+c731uIN10Ck3hl3qMaJoaECakqgaqNTltvH+W09AajZDLVbIJDrrEP5/Z3YqjY63WNhZ2dhbOc0qMvjDLdjntI7Q0ZXCOY6615DhfM31ujshEScZeJNc9bTHfMQroO6bc685vTatWZqjLO+lC5xdpDxqDMesZwP4izX6o3OTtCOg9vvrL/ZPaHTQMjp7cQKzjKN1DvDNJQ721/dNqgrhfpSZzmld3Zqtr5MaKxw+jMmMZ/NPp7EbxQLOeOr2QI1m5z1XMSJLR5x5n3HvBUMdbbF8tVQutT5TXYsP8vtjM+OOfNhxyAe3rkcSew3M4uc75E6Z0ef0cX5TeyYM+1QjbNzN7YTN0n2ty6vE1Nz7oDTPVy9b9tEMB/+b/W+9bsbEZlrjClOWpbCBDEGuNcYc27i+90AxphfttC/C6gwxmSJyCTgdGPMdYmyPwPvG2NaPcVUXFxs5syZ056z0bJwLbzzUydJbF+xa5nlhtHXwWn/59RQjIHV78G0yc4Gf/Jt8MYPoNvxcMEf4T+3wPqPnGEnvQADz3NONU0dD6OnODu5z//srGQAhcc6K3TBELj8ZagvcxJWVSI5+rJgwLmAcVbetbOccZz2f9D7FChbAfOfdY5+Lv0n9DsL3v4JfPwInPFjZ4Os2wZL/wOb5znDmjhkdXdiXvwv6HUKXDQV0vKdI5lXb4IFL8KoyXDsRNgy39mRiOXsUC13YoOpcmLMLHI2pngEarc6SS5U7eyg9tjZ44xnx8bftDElksSOflpjuZ2NP1Sz26lDIelG21ZZ3Z0dTm4fZ0deuyWxs9vm7NRaE8xzdnDVG3eNRaydv/nBsMeOSpz58qU789RQ4ewk9ya9ELK6OeOLNTpJoHLdnjvBPYizPqUXQnonZ0ddV+ok0XCds5yCuc76GK51dp7hOif57lgHXD4nIWV0ddbjHTV2YztlnoCzzpSvhm2LnG0nu6dz8JaWnxhvjZMULLfTr+Vx/ro8zvablu+sv73GOgkvFoIVbzrbS7jWic9y7Uw23rREEhWnW1Z3Z/mEaqBijfO7e9OdeXP7nURYX+as/4FcJ4H70p3l6fY525M33YknUu9M09gw+Pz9+NE7LkG4cU4bnQVsAmYDlxhjFjfrp4sxZkvi/28APzDGnJi4SD0XGJXodR5w3I5rEi05qAmiuVA1bF/l/EgizoqZ7Hz+5i/gmW9Bw3bntNOV/3WOQu04fPZnZ2UeednO/l+50TkNhcDx1zjJYtW7sPCfzoo38R/O8AB1ZTDz186F9mMm7HrkV7ke3rwblv93ZzeXD776Oxh1ufM9HoNnvuEkkx3yB8Loa50d/sbPYNaDsPFTJ/md+8DOI0FwkuD/fg4f/G5nt7TOzoYRbXRqGL5MJ14Td47wovVOf/5syChMHK0njmrdPidGK3Gj3Y4N3Jfu7EzBGacdc462PAFn43K5nQ1axFmuJu7sRBornQ1/x1G/N83pHqp2Nuac3s5RZTB35/QjDYkj/wZnvC6vM5+xkDNPoWpnQ67bBmXLYdtiZ0cYzHPmJ6PQOQJOL3Dm0Rt0Nm5fprPRB3KcI1hvYn6iIWeHEQ87O7i0Ts7OYtti54g9mOucRszo6hztu3zO8rWjzs4k2rBzh1m71UlSDRXOQcr/b+/eYuWsyjCO/x+627ALCT3aQAsWQpUUhUKIqceQ6gUgoSYeKsFIKsakMVrPIjdGY2MkRmuVkHCo1oSApqI2XhBJIdV4KBaLnIoBa4WSlrbUVgVjDzxerFU7br5d2J3dPXtmnl8ymfnWzJ6+a97peudb32ny9NL3iYPl8/JLpdgeeKEOSHvK3w1OK3GfNFAG0eefLH0dnFreZ9o58JrzYebryudt1yJW7wdObp6mPHyo/IDZ97ejRU91EJ04ufRt8ozyOY+UXfp/0sDR78urdfjg/3+P+1BHCkT9h68AVgITgNW2V0j6KrDJ9jpJXweuAg4Be4Fltp+of/sR4Ib6Vitsv+IW244ViJF4/i/wwK3w9s+UweNYXtwLG26EC95fprHate03ZTCYeV4ZDIf+xzh8sPyqGpx6dBBtZZcB8Vhxb91Qfv2fseDYr7PLgHRkL7GI6IiOFYix1hUFIiJiHDlWgciR1BER0SgFIiIiGqVAREREoxSIiIholAIRERGNUiAiIqJRCkRERDRKgYiIiEY9daCcpN3A8Z6tbwawZxTDGU/St+7Vy/1L38aH19qe2fRETxWIdkjaNNzRhN0ufetevdy/9G38yxRTREQ0SoGIiIhGKRBH3dLpAE6g9K179XL/0rdxLtsgIiKiUdYgIiKiUQpEREQ06vsCIekySX+W9JSk6zsdT7sknSnpfkmPS3pM0vLaPk3SvZKerPdTOx3r8ZI0QdJmSb+oy2dL2lhz+CNJkzod4/GQNEXSWklPSNoi6c09lrdP1+/ko5LulHRyt+ZO0mpJuyQ92tLWmCsVq2ofH5Z08fDvPL70dYGQNAG4CbgcmA9cLWl+Z6Nq2yHgs7bnAwuBj9c+XQ+stz0PWF+Xu9VyYEvL8jeAb9s+F/g7cF1Homrfd4B7bJ8HXEjpY0/kTdJs4JPAJbbfQLkM8Qfp3tz9ALhsSNtwubocmFdvHwNuHqMY29bXBQJ4E/CU7a22DwB3AYs7HFNbbO+w/cf6+J+UQWY2pV9r6svWAO/pTITtkTQHeDdwW10WsAhYW1/SlX2TdBrwDuB2ANsHbO+jR/JWDQCDkgaAycAOujR3tn8F7B3SPFyuFgM/dPF7YIqk08cm0vb0e4GYDTzTsry9tvUE3HNJ4AAAA4NJREFUSXOBi4CNwCzbO+pTO4FZHQqrXSuBLwAv1eXpwD7bh+pyt+bwbGA38P06fXabpFPokbzZfhb4JvA0pTDsBx6kN3J3xHC56tpxpt8LRM+SdCrwE+BTtv/R+pzLvs1dt3+zpCuBXbYf7HQsJ8AAcDFws+2LgBcYMp3UrXkDqPPxiymF8AzgFF4+RdMzujlXrfq9QDwLnNmyPKe2dTVJEynF4Q7bd9fm546s1tb7XZ2Krw1vBa6StI0yHbiIMm8/pU5bQPfmcDuw3fbGuryWUjB6IW8A7wL+anu37YPA3ZR89kLujhguV107zvR7gfgDMK/uSTGJstFsXYdjakudk78d2GL7Wy1PrQOurY+vBX4+1rG1y/aXbM+xPZeSq/tsXwPcD7yvvqxb+7YTeEbS62vTO4HH6YG8VU8DCyVNrt/RI/3r+ty1GC5X64AP172ZFgL7W6aixrW+P5Ja0hWUee0JwGrbKzocUlskvQ34NfAIR+fpb6Bsh/gxcBbllOgfsD10I1vXkHQp8DnbV0o6h7JGMQ3YDHzI9n86Gd/xkLSAsvF9ErAVWEr5EdcTeZP0FWAJZU+7zcBHKXPxXZc7SXcCl1JO6/0c8GXgZzTkqhbE71Gm1F4Eltre1Im4R6rvC0RERDTr9ymmiIgYRgpEREQ0SoGIiIhGKRAREdEoBSIiIhqlQESMgKTDkh5quY3ayfMkzW09O2hEpw288ksiosW/bS/odBARYyFrEBGjQNI2STdKekTSA5LOre1zJd1XrwOwXtJZtX2WpJ9K+lO9vaW+1QRJt9brJvxS0mDHOhV9LwUiYmQGh0wxLWl5br/tN1KOml1Z274LrLF9AXAHsKq2rwI22L6Qcs6lx2r7POAm2+cD+4D3nuD+RAwrR1JHjICkf9k+taF9G7DI9tZ6ssSdtqdL2gOcbvtgbd9he4ak3cCc1tNK1NOz31svOIOkLwITbX/txPcs4uWyBhExejzM45FoPQ/RYbKdMDooBSJi9Cxpuf9dffxbyplnAa6hnEgRyiUpl8H/rrF92lgFGfFq5ddJxMgMSnqoZfke20d2dZ0q6WHKWsDVte0TlKvEfZ5yxbiltX05cIuk6yhrCssoV1qLGDeyDSJiFNRtEJfY3tPpWCJGS6aYIiKiUdYgIiKiUdYgIiKiUQpEREQ0SoGIiIhGKRAREdEoBSIiIhr9F8vNuTZMUCBOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['f1_metric'])\n",
    "plt.plot(history.history['val_f1_metric'])\n",
    "\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train_loss', 'val_loss', 'f1', 'val_f1'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras evaluate= [0.5032440423965454, 0.8855218887329102, 0.8881579041481018]\n",
      "size of test set 594\n",
      "TP class counts (array([1]), array([526]))\n",
      "True class counts (array([0, 1, 2]), array([ 42, 526,  26]))\n",
      "Pred class counts (array([1]), array([594]))\n",
      "baseline acc: 4.377104377104377\n",
      "[[  0  42   0]\n",
      " [  0 526   0]\n",
      " [  0  26   0]]\n",
      "F1 score (weighted) 0.8317580567580567\n",
      "F1 score (macro) 0.3130952380952381\n",
      "F1 score (micro) 0.8855218855218855\n",
      "cohen's Kappa 0.0\n",
      "Recall of class 0 = 0.0\n",
      "Recall of class 1 = 1.0\n",
      "Recall of class 2 = 0.0\n",
      "Recall avg 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "\n",
    "model = load_model(best_model_path)\n",
    "test_res = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"keras evaluate=\", test_res)\n",
    "pred = model.predict(x_test)\n",
    "pred_classes = np.argmax(pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "check_baseline(pred_classes, y_test_classes)\n",
    "conf_mat = confusion_matrix(y_test_classes, pred_classes)\n",
    "print(conf_mat)\n",
    "labels = [0,1,2]\n",
    "\n",
    "f1_weighted = f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='weighted', sample_weight=None)\n",
    "print(\"F1 score (weighted)\", f1_weighted)\n",
    "print(\"F1 score (macro)\", f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='macro', sample_weight=None))\n",
    "print(\"F1 score (micro)\", f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='micro', sample_weight=None))  # weighted and micro preferred in case of imbalance\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-s-kappa --> supports multiclass; ref: https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english\n",
    "print(\"cohen's Kappa\", cohen_kappa_score(y_test_classes, pred_classes))\n",
    "\n",
    "recall = []\n",
    "for i, row in enumerate(conf_mat):\n",
    "    recall.append(np.round(row[i]/np.sum(row), 2))\n",
    "    print(\"Recall of class {} = {}\".format(i, recall[i]))\n",
    "print(\"Recall avg\", sum(recall)/len(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
