{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below line of code if you want to calculate features and save dataframe\n",
    "# this script prints the path at which dataframe with calculated features is saved.\n",
    "# train.py calls the DataGenerator class to \n",
    "\n",
    "# %run ./main.py WMT original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "",
    "_uuid": "",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unnamed: 0</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi_6</th>\n",
       "      <th>rsi_7</th>\n",
       "      <th>rsi_8</th>\n",
       "      <th>...</th>\n",
       "      <th>eom_19</th>\n",
       "      <th>eom_20</th>\n",
       "      <th>eom_21</th>\n",
       "      <th>eom_22</th>\n",
       "      <th>eom_23</th>\n",
       "      <th>eom_24</th>\n",
       "      <th>eom_25</th>\n",
       "      <th>eom_26</th>\n",
       "      <th>volume_delta</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>1999-02-08</td>\n",
       "      <td>13.327940</td>\n",
       "      <td>13.454873</td>\n",
       "      <td>13.309807</td>\n",
       "      <td>13.454873</td>\n",
       "      <td>31100</td>\n",
       "      <td>76.190708</td>\n",
       "      <td>64.331059</td>\n",
       "      <td>58.823622</td>\n",
       "      <td>...</td>\n",
       "      <td>135.334375</td>\n",
       "      <td>135.334375</td>\n",
       "      <td>135.334375</td>\n",
       "      <td>135.334375</td>\n",
       "      <td>135.334375</td>\n",
       "      <td>135.334375</td>\n",
       "      <td>135.334375</td>\n",
       "      <td>135.334375</td>\n",
       "      <td>-34900.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>1999-02-09</td>\n",
       "      <td>13.473011</td>\n",
       "      <td>13.473011</td>\n",
       "      <td>13.201013</td>\n",
       "      <td>13.201013</td>\n",
       "      <td>64300</td>\n",
       "      <td>90.415574</td>\n",
       "      <td>77.880480</td>\n",
       "      <td>66.723212</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.174450</td>\n",
       "      <td>-19.174450</td>\n",
       "      <td>-19.174450</td>\n",
       "      <td>-19.174450</td>\n",
       "      <td>-19.174450</td>\n",
       "      <td>-19.174450</td>\n",
       "      <td>-19.174450</td>\n",
       "      <td>-19.174450</td>\n",
       "      <td>33200.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>1999-02-10</td>\n",
       "      <td>13.255406</td>\n",
       "      <td>13.382339</td>\n",
       "      <td>13.164740</td>\n",
       "      <td>13.237273</td>\n",
       "      <td>69300</td>\n",
       "      <td>71.428397</td>\n",
       "      <td>73.333248</td>\n",
       "      <td>64.924132</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.930040</td>\n",
       "      <td>-19.930040</td>\n",
       "      <td>-19.930040</td>\n",
       "      <td>-19.930040</td>\n",
       "      <td>-19.930040</td>\n",
       "      <td>-19.930040</td>\n",
       "      <td>-19.930040</td>\n",
       "      <td>-19.930040</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>1999-02-11</td>\n",
       "      <td>13.182880</td>\n",
       "      <td>13.182880</td>\n",
       "      <td>12.929014</td>\n",
       "      <td>13.128480</td>\n",
       "      <td>216800</td>\n",
       "      <td>75.221443</td>\n",
       "      <td>69.231060</td>\n",
       "      <td>70.902750</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.479398</td>\n",
       "      <td>-25.479398</td>\n",
       "      <td>-25.479398</td>\n",
       "      <td>-25.479398</td>\n",
       "      <td>-25.479398</td>\n",
       "      <td>-25.479398</td>\n",
       "      <td>-25.479398</td>\n",
       "      <td>-25.479398</td>\n",
       "      <td>147500.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>1999-02-12</td>\n",
       "      <td>12.983404</td>\n",
       "      <td>13.128470</td>\n",
       "      <td>12.856471</td>\n",
       "      <td>12.856471</td>\n",
       "      <td>242300</td>\n",
       "      <td>61.679370</td>\n",
       "      <td>63.882715</td>\n",
       "      <td>59.599109</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.125683</td>\n",
       "      <td>-7.125683</td>\n",
       "      <td>-7.125683</td>\n",
       "      <td>-7.125683</td>\n",
       "      <td>-7.125683</td>\n",
       "      <td>-7.125683</td>\n",
       "      <td>-7.125683</td>\n",
       "      <td>-7.125683</td>\n",
       "      <td>25500.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>36</td>\n",
       "      <td>1999-02-16</td>\n",
       "      <td>12.956205</td>\n",
       "      <td>13.019672</td>\n",
       "      <td>12.638873</td>\n",
       "      <td>12.765806</td>\n",
       "      <td>36600</td>\n",
       "      <td>54.858716</td>\n",
       "      <td>62.045943</td>\n",
       "      <td>63.978263</td>\n",
       "      <td>...</td>\n",
       "      <td>-169.796291</td>\n",
       "      <td>-169.796291</td>\n",
       "      <td>-169.796291</td>\n",
       "      <td>-169.796291</td>\n",
       "      <td>-169.796291</td>\n",
       "      <td>-169.796291</td>\n",
       "      <td>-169.796291</td>\n",
       "      <td>-169.796291</td>\n",
       "      <td>-205700.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>37</td>\n",
       "      <td>1999-02-17</td>\n",
       "      <td>12.693273</td>\n",
       "      <td>12.820206</td>\n",
       "      <td>12.620740</td>\n",
       "      <td>12.638873</td>\n",
       "      <td>41700</td>\n",
       "      <td>38.718484</td>\n",
       "      <td>43.523222</td>\n",
       "      <td>51.047172</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.042783</td>\n",
       "      <td>-52.042783</td>\n",
       "      <td>-52.042783</td>\n",
       "      <td>-52.042783</td>\n",
       "      <td>-52.042783</td>\n",
       "      <td>-52.042783</td>\n",
       "      <td>-52.042783</td>\n",
       "      <td>-52.042783</td>\n",
       "      <td>5100.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>38</td>\n",
       "      <td>1999-02-18</td>\n",
       "      <td>12.620747</td>\n",
       "      <td>12.747680</td>\n",
       "      <td>12.493814</td>\n",
       "      <td>12.675147</td>\n",
       "      <td>83000</td>\n",
       "      <td>14.337356</td>\n",
       "      <td>37.826525</td>\n",
       "      <td>42.387705</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.502236</td>\n",
       "      <td>-30.502236</td>\n",
       "      <td>-30.502236</td>\n",
       "      <td>-30.502236</td>\n",
       "      <td>-30.502236</td>\n",
       "      <td>-30.502236</td>\n",
       "      <td>-30.502236</td>\n",
       "      <td>-30.502236</td>\n",
       "      <td>41300.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>39</td>\n",
       "      <td>1999-02-19</td>\n",
       "      <td>12.729545</td>\n",
       "      <td>12.747679</td>\n",
       "      <td>12.493813</td>\n",
       "      <td>12.602612</td>\n",
       "      <td>130300</td>\n",
       "      <td>13.283954</td>\n",
       "      <td>24.193748</td>\n",
       "      <td>43.276130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>47300.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40</td>\n",
       "      <td>1999-02-22</td>\n",
       "      <td>12.693278</td>\n",
       "      <td>12.765811</td>\n",
       "      <td>12.584479</td>\n",
       "      <td>12.729545</td>\n",
       "      <td>79500</td>\n",
       "      <td>13.824787</td>\n",
       "      <td>10.843216</td>\n",
       "      <td>22.122103</td>\n",
       "      <td>...</td>\n",
       "      <td>12.407965</td>\n",
       "      <td>12.407965</td>\n",
       "      <td>12.407965</td>\n",
       "      <td>12.407965</td>\n",
       "      <td>12.407965</td>\n",
       "      <td>12.407965</td>\n",
       "      <td>12.407965</td>\n",
       "      <td>12.407965</td>\n",
       "      <td>-50800.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>41</td>\n",
       "      <td>1999-02-23</td>\n",
       "      <td>12.765813</td>\n",
       "      <td>12.820212</td>\n",
       "      <td>12.620747</td>\n",
       "      <td>12.620747</td>\n",
       "      <td>309600</td>\n",
       "      <td>24.657494</td>\n",
       "      <td>22.377750</td>\n",
       "      <td>17.745788</td>\n",
       "      <td>...</td>\n",
       "      <td>2.920773</td>\n",
       "      <td>2.920773</td>\n",
       "      <td>2.920773</td>\n",
       "      <td>2.920773</td>\n",
       "      <td>2.920773</td>\n",
       "      <td>2.920773</td>\n",
       "      <td>2.920773</td>\n",
       "      <td>2.920773</td>\n",
       "      <td>230100.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>42</td>\n",
       "      <td>1999-02-24</td>\n",
       "      <td>12.657005</td>\n",
       "      <td>12.820204</td>\n",
       "      <td>12.566339</td>\n",
       "      <td>12.566339</td>\n",
       "      <td>34500</td>\n",
       "      <td>25.510128</td>\n",
       "      <td>19.999735</td>\n",
       "      <td>18.567534</td>\n",
       "      <td>...</td>\n",
       "      <td>-20.020984</td>\n",
       "      <td>-20.020984</td>\n",
       "      <td>-20.020984</td>\n",
       "      <td>-20.020984</td>\n",
       "      <td>-20.020984</td>\n",
       "      <td>-20.020984</td>\n",
       "      <td>-20.020984</td>\n",
       "      <td>-20.020984</td>\n",
       "      <td>-275100.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>43</td>\n",
       "      <td>1999-02-25</td>\n",
       "      <td>12.557281</td>\n",
       "      <td>12.711414</td>\n",
       "      <td>12.548214</td>\n",
       "      <td>12.629814</td>\n",
       "      <td>44600</td>\n",
       "      <td>23.202131</td>\n",
       "      <td>22.514360</td>\n",
       "      <td>18.087854</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.220081</td>\n",
       "      <td>-23.220081</td>\n",
       "      <td>-23.220081</td>\n",
       "      <td>-23.220081</td>\n",
       "      <td>-23.220081</td>\n",
       "      <td>-23.220081</td>\n",
       "      <td>-23.220081</td>\n",
       "      <td>-23.220081</td>\n",
       "      <td>10100.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>44</td>\n",
       "      <td>1999-02-26</td>\n",
       "      <td>12.675143</td>\n",
       "      <td>12.765809</td>\n",
       "      <td>12.548210</td>\n",
       "      <td>12.548210</td>\n",
       "      <td>24900</td>\n",
       "      <td>50.425129</td>\n",
       "      <td>35.462112</td>\n",
       "      <td>34.221570</td>\n",
       "      <td>...</td>\n",
       "      <td>23.765926</td>\n",
       "      <td>23.765926</td>\n",
       "      <td>23.765926</td>\n",
       "      <td>23.765926</td>\n",
       "      <td>23.765926</td>\n",
       "      <td>23.765926</td>\n",
       "      <td>23.765926</td>\n",
       "      <td>23.765926</td>\n",
       "      <td>-19700.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>45</td>\n",
       "      <td>1999-03-01</td>\n",
       "      <td>12.575414</td>\n",
       "      <td>12.584480</td>\n",
       "      <td>12.385014</td>\n",
       "      <td>12.421281</td>\n",
       "      <td>13300</td>\n",
       "      <td>45.081909</td>\n",
       "      <td>40.825047</td>\n",
       "      <td>30.117380</td>\n",
       "      <td>...</td>\n",
       "      <td>-258.349327</td>\n",
       "      <td>-258.349327</td>\n",
       "      <td>-258.349327</td>\n",
       "      <td>-258.349327</td>\n",
       "      <td>-258.349327</td>\n",
       "      <td>-258.349327</td>\n",
       "      <td>-258.349327</td>\n",
       "      <td>-258.349327</td>\n",
       "      <td>-11600.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>46</td>\n",
       "      <td>1999-03-02</td>\n",
       "      <td>12.439406</td>\n",
       "      <td>12.493806</td>\n",
       "      <td>12.239940</td>\n",
       "      <td>12.294340</td>\n",
       "      <td>55100</td>\n",
       "      <td>27.272201</td>\n",
       "      <td>37.287533</td>\n",
       "      <td>34.323674</td>\n",
       "      <td>...</td>\n",
       "      <td>-54.308840</td>\n",
       "      <td>-54.308840</td>\n",
       "      <td>-54.308840</td>\n",
       "      <td>-54.308840</td>\n",
       "      <td>-54.308840</td>\n",
       "      <td>-54.308840</td>\n",
       "      <td>-54.308840</td>\n",
       "      <td>-54.308840</td>\n",
       "      <td>41800.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>47</td>\n",
       "      <td>1999-03-03</td>\n",
       "      <td>12.403139</td>\n",
       "      <td>12.620738</td>\n",
       "      <td>12.312473</td>\n",
       "      <td>12.566339</td>\n",
       "      <td>103300</td>\n",
       "      <td>28.074402</td>\n",
       "      <td>26.694452</td>\n",
       "      <td>36.434791</td>\n",
       "      <td>...</td>\n",
       "      <td>29.761936</td>\n",
       "      <td>29.761936</td>\n",
       "      <td>29.761936</td>\n",
       "      <td>29.761936</td>\n",
       "      <td>29.761936</td>\n",
       "      <td>29.761936</td>\n",
       "      <td>29.761936</td>\n",
       "      <td>29.761936</td>\n",
       "      <td>48200.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>48</td>\n",
       "      <td>1999-03-04</td>\n",
       "      <td>12.675147</td>\n",
       "      <td>13.092212</td>\n",
       "      <td>12.675147</td>\n",
       "      <td>13.019679</td>\n",
       "      <td>90400</td>\n",
       "      <td>48.039377</td>\n",
       "      <td>51.376323</td>\n",
       "      <td>49.236795</td>\n",
       "      <td>...</td>\n",
       "      <td>192.419153</td>\n",
       "      <td>192.419153</td>\n",
       "      <td>192.419153</td>\n",
       "      <td>192.419153</td>\n",
       "      <td>192.419153</td>\n",
       "      <td>192.419153</td>\n",
       "      <td>192.419153</td>\n",
       "      <td>192.419153</td>\n",
       "      <td>-12900.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>49</td>\n",
       "      <td>1999-03-05</td>\n",
       "      <td>13.346074</td>\n",
       "      <td>13.364207</td>\n",
       "      <td>13.128475</td>\n",
       "      <td>13.264474</td>\n",
       "      <td>92700</td>\n",
       "      <td>76.273364</td>\n",
       "      <td>70.932191</td>\n",
       "      <td>71.893802</td>\n",
       "      <td>...</td>\n",
       "      <td>92.223204</td>\n",
       "      <td>92.223204</td>\n",
       "      <td>92.223204</td>\n",
       "      <td>92.223204</td>\n",
       "      <td>92.223204</td>\n",
       "      <td>92.223204</td>\n",
       "      <td>92.223204</td>\n",
       "      <td>92.223204</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>50</td>\n",
       "      <td>1999-03-08</td>\n",
       "      <td>13.337007</td>\n",
       "      <td>13.491140</td>\n",
       "      <td>13.282607</td>\n",
       "      <td>13.454873</td>\n",
       "      <td>107600</td>\n",
       "      <td>78.947101</td>\n",
       "      <td>73.508095</td>\n",
       "      <td>68.363783</td>\n",
       "      <td>...</td>\n",
       "      <td>27.235713</td>\n",
       "      <td>27.235713</td>\n",
       "      <td>27.235713</td>\n",
       "      <td>27.235713</td>\n",
       "      <td>27.235713</td>\n",
       "      <td>27.235713</td>\n",
       "      <td>27.235713</td>\n",
       "      <td>27.235713</td>\n",
       "      <td>14900.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>51</td>\n",
       "      <td>1999-03-09</td>\n",
       "      <td>13.309810</td>\n",
       "      <td>13.527409</td>\n",
       "      <td>13.255410</td>\n",
       "      <td>13.309810</td>\n",
       "      <td>79000</td>\n",
       "      <td>75.036079</td>\n",
       "      <td>77.227639</td>\n",
       "      <td>72.032042</td>\n",
       "      <td>...</td>\n",
       "      <td>1.561737</td>\n",
       "      <td>1.561737</td>\n",
       "      <td>1.561737</td>\n",
       "      <td>1.561737</td>\n",
       "      <td>1.561737</td>\n",
       "      <td>1.561737</td>\n",
       "      <td>1.561737</td>\n",
       "      <td>1.561737</td>\n",
       "      <td>-28600.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>52</td>\n",
       "      <td>1999-03-10</td>\n",
       "      <td>13.364203</td>\n",
       "      <td>14.107666</td>\n",
       "      <td>13.364203</td>\n",
       "      <td>14.071400</td>\n",
       "      <td>1733800</td>\n",
       "      <td>82.860979</td>\n",
       "      <td>76.551557</td>\n",
       "      <td>78.461322</td>\n",
       "      <td>...</td>\n",
       "      <td>14.773437</td>\n",
       "      <td>14.773437</td>\n",
       "      <td>14.773437</td>\n",
       "      <td>14.773437</td>\n",
       "      <td>14.773437</td>\n",
       "      <td>14.773437</td>\n",
       "      <td>14.773437</td>\n",
       "      <td>14.773437</td>\n",
       "      <td>1654800.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>53</td>\n",
       "      <td>1999-03-11</td>\n",
       "      <td>14.506605</td>\n",
       "      <td>14.651671</td>\n",
       "      <td>14.071407</td>\n",
       "      <td>14.270873</td>\n",
       "      <td>909000</td>\n",
       "      <td>97.028336</td>\n",
       "      <td>91.785533</td>\n",
       "      <td>88.194405</td>\n",
       "      <td>...</td>\n",
       "      <td>39.935758</td>\n",
       "      <td>39.935758</td>\n",
       "      <td>39.935758</td>\n",
       "      <td>39.935758</td>\n",
       "      <td>39.935758</td>\n",
       "      <td>39.935758</td>\n",
       "      <td>39.935758</td>\n",
       "      <td>39.935758</td>\n",
       "      <td>-824800.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>54</td>\n",
       "      <td>1999-03-12</td>\n",
       "      <td>14.434066</td>\n",
       "      <td>14.542865</td>\n",
       "      <td>14.071401</td>\n",
       "      <td>14.216467</td>\n",
       "      <td>604300</td>\n",
       "      <td>94.551081</td>\n",
       "      <td>93.157702</td>\n",
       "      <td>88.012342</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.244679</td>\n",
       "      <td>-4.244679</td>\n",
       "      <td>-4.244679</td>\n",
       "      <td>-4.244679</td>\n",
       "      <td>-4.244679</td>\n",
       "      <td>-4.244679</td>\n",
       "      <td>-4.244679</td>\n",
       "      <td>-4.244679</td>\n",
       "      <td>-304700.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>55</td>\n",
       "      <td>1999-03-15</td>\n",
       "      <td>14.252735</td>\n",
       "      <td>14.289001</td>\n",
       "      <td>13.935403</td>\n",
       "      <td>14.107669</td>\n",
       "      <td>523000</td>\n",
       "      <td>85.123868</td>\n",
       "      <td>86.977845</td>\n",
       "      <td>85.862738</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.179211</td>\n",
       "      <td>-13.179211</td>\n",
       "      <td>-13.179211</td>\n",
       "      <td>-13.179211</td>\n",
       "      <td>-13.179211</td>\n",
       "      <td>-13.179211</td>\n",
       "      <td>-13.179211</td>\n",
       "      <td>-13.179211</td>\n",
       "      <td>-81300.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>56</td>\n",
       "      <td>1999-03-16</td>\n",
       "      <td>13.980746</td>\n",
       "      <td>14.107679</td>\n",
       "      <td>13.926346</td>\n",
       "      <td>14.035146</td>\n",
       "      <td>273300</td>\n",
       "      <td>66.000366</td>\n",
       "      <td>75.458126</td>\n",
       "      <td>78.072137</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.315769</td>\n",
       "      <td>-6.315769</td>\n",
       "      <td>-6.315769</td>\n",
       "      <td>-6.315769</td>\n",
       "      <td>-6.315769</td>\n",
       "      <td>-6.315769</td>\n",
       "      <td>-6.315769</td>\n",
       "      <td>-6.315769</td>\n",
       "      <td>-249700.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>57</td>\n",
       "      <td>1999-03-17</td>\n",
       "      <td>14.107673</td>\n",
       "      <td>14.415938</td>\n",
       "      <td>14.071407</td>\n",
       "      <td>14.397805</td>\n",
       "      <td>362800</td>\n",
       "      <td>70.924883</td>\n",
       "      <td>70.523173</td>\n",
       "      <td>78.169163</td>\n",
       "      <td>...</td>\n",
       "      <td>21.524699</td>\n",
       "      <td>21.524699</td>\n",
       "      <td>21.524699</td>\n",
       "      <td>21.524699</td>\n",
       "      <td>21.524699</td>\n",
       "      <td>21.524699</td>\n",
       "      <td>21.524699</td>\n",
       "      <td>21.524699</td>\n",
       "      <td>89500.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>58</td>\n",
       "      <td>1999-03-18</td>\n",
       "      <td>14.651666</td>\n",
       "      <td>14.687933</td>\n",
       "      <td>14.470334</td>\n",
       "      <td>14.579133</td>\n",
       "      <td>332600</td>\n",
       "      <td>78.985576</td>\n",
       "      <td>77.978485</td>\n",
       "      <td>77.582789</td>\n",
       "      <td>...</td>\n",
       "      <td>21.947044</td>\n",
       "      <td>21.947044</td>\n",
       "      <td>21.947044</td>\n",
       "      <td>21.947044</td>\n",
       "      <td>21.947044</td>\n",
       "      <td>21.947044</td>\n",
       "      <td>21.947044</td>\n",
       "      <td>21.947044</td>\n",
       "      <td>-30200.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>59</td>\n",
       "      <td>1999-03-19</td>\n",
       "      <td>14.704340</td>\n",
       "      <td>14.722538</td>\n",
       "      <td>14.340371</td>\n",
       "      <td>14.431363</td>\n",
       "      <td>260400</td>\n",
       "      <td>78.110990</td>\n",
       "      <td>78.580307</td>\n",
       "      <td>77.708054</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.997375</td>\n",
       "      <td>-6.997375</td>\n",
       "      <td>-6.997375</td>\n",
       "      <td>-6.997375</td>\n",
       "      <td>-6.997375</td>\n",
       "      <td>-6.997375</td>\n",
       "      <td>-6.997375</td>\n",
       "      <td>-6.997375</td>\n",
       "      <td>-72200.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>60</td>\n",
       "      <td>1999-03-22</td>\n",
       "      <td>14.558749</td>\n",
       "      <td>14.631543</td>\n",
       "      <td>14.404062</td>\n",
       "      <td>14.522352</td>\n",
       "      <td>165000</td>\n",
       "      <td>50.808390</td>\n",
       "      <td>72.841781</td>\n",
       "      <td>73.503843</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.882148</td>\n",
       "      <td>-1.882148</td>\n",
       "      <td>-1.882148</td>\n",
       "      <td>-1.882148</td>\n",
       "      <td>-1.882148</td>\n",
       "      <td>-1.882148</td>\n",
       "      <td>-1.882148</td>\n",
       "      <td>-1.882148</td>\n",
       "      <td>-95400.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 429 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unnamed: 0   timestamp       open       high        low      close  \\\n",
       "0           31  1999-02-08  13.327940  13.454873  13.309807  13.454873   \n",
       "1           32  1999-02-09  13.473011  13.473011  13.201013  13.201013   \n",
       "2           33  1999-02-10  13.255406  13.382339  13.164740  13.237273   \n",
       "3           34  1999-02-11  13.182880  13.182880  12.929014  13.128480   \n",
       "4           35  1999-02-12  12.983404  13.128470  12.856471  12.856471   \n",
       "5           36  1999-02-16  12.956205  13.019672  12.638873  12.765806   \n",
       "6           37  1999-02-17  12.693273  12.820206  12.620740  12.638873   \n",
       "7           38  1999-02-18  12.620747  12.747680  12.493814  12.675147   \n",
       "8           39  1999-02-19  12.729545  12.747679  12.493813  12.602612   \n",
       "9           40  1999-02-22  12.693278  12.765811  12.584479  12.729545   \n",
       "10          41  1999-02-23  12.765813  12.820212  12.620747  12.620747   \n",
       "11          42  1999-02-24  12.657005  12.820204  12.566339  12.566339   \n",
       "12          43  1999-02-25  12.557281  12.711414  12.548214  12.629814   \n",
       "13          44  1999-02-26  12.675143  12.765809  12.548210  12.548210   \n",
       "14          45  1999-03-01  12.575414  12.584480  12.385014  12.421281   \n",
       "15          46  1999-03-02  12.439406  12.493806  12.239940  12.294340   \n",
       "16          47  1999-03-03  12.403139  12.620738  12.312473  12.566339   \n",
       "17          48  1999-03-04  12.675147  13.092212  12.675147  13.019679   \n",
       "18          49  1999-03-05  13.346074  13.364207  13.128475  13.264474   \n",
       "19          50  1999-03-08  13.337007  13.491140  13.282607  13.454873   \n",
       "20          51  1999-03-09  13.309810  13.527409  13.255410  13.309810   \n",
       "21          52  1999-03-10  13.364203  14.107666  13.364203  14.071400   \n",
       "22          53  1999-03-11  14.506605  14.651671  14.071407  14.270873   \n",
       "23          54  1999-03-12  14.434066  14.542865  14.071401  14.216467   \n",
       "24          55  1999-03-15  14.252735  14.289001  13.935403  14.107669   \n",
       "25          56  1999-03-16  13.980746  14.107679  13.926346  14.035146   \n",
       "26          57  1999-03-17  14.107673  14.415938  14.071407  14.397805   \n",
       "27          58  1999-03-18  14.651666  14.687933  14.470334  14.579133   \n",
       "28          59  1999-03-19  14.704340  14.722538  14.340371  14.431363   \n",
       "29          60  1999-03-22  14.558749  14.631543  14.404062  14.522352   \n",
       "\n",
       "     volume      rsi_6      rsi_7      rsi_8  ...      eom_19      eom_20  \\\n",
       "0     31100  76.190708  64.331059  58.823622  ...  135.334375  135.334375   \n",
       "1     64300  90.415574  77.880480  66.723212  ...  -19.174450  -19.174450   \n",
       "2     69300  71.428397  73.333248  64.924132  ...  -19.930040  -19.930040   \n",
       "3    216800  75.221443  69.231060  70.902750  ...  -25.479398  -25.479398   \n",
       "4    242300  61.679370  63.882715  59.599109  ...   -7.125683   -7.125683   \n",
       "5     36600  54.858716  62.045943  63.978263  ... -169.796291 -169.796291   \n",
       "6     41700  38.718484  43.523222  51.047172  ...  -52.042783  -52.042783   \n",
       "7     83000  14.337356  37.826525  42.387705  ...  -30.502236  -30.502236   \n",
       "8    130300  13.283954  24.193748  43.276130  ...   -0.000299   -0.000299   \n",
       "9     79500  13.824787  10.843216  22.122103  ...   12.407965   12.407965   \n",
       "10   309600  24.657494  22.377750  17.745788  ...    2.920773    2.920773   \n",
       "11    34500  25.510128  19.999735  18.567534  ...  -20.020984  -20.020984   \n",
       "12    44600  23.202131  22.514360  18.087854  ...  -23.220081  -23.220081   \n",
       "13    24900  50.425129  35.462112  34.221570  ...   23.765926   23.765926   \n",
       "14    13300  45.081909  40.825047  30.117380  ... -258.349327 -258.349327   \n",
       "15    55100  27.272201  37.287533  34.323674  ...  -54.308840  -54.308840   \n",
       "16   103300  28.074402  26.694452  36.434791  ...   29.761936   29.761936   \n",
       "17    90400  48.039377  51.376323  49.236795  ...  192.419153  192.419153   \n",
       "18    92700  76.273364  70.932191  71.893802  ...   92.223204   92.223204   \n",
       "19   107600  78.947101  73.508095  68.363783  ...   27.235713   27.235713   \n",
       "20    79000  75.036079  77.227639  72.032042  ...    1.561737    1.561737   \n",
       "21  1733800  82.860979  76.551557  78.461322  ...   14.773437   14.773437   \n",
       "22   909000  97.028336  91.785533  88.194405  ...   39.935758   39.935758   \n",
       "23   604300  94.551081  93.157702  88.012342  ...   -4.244679   -4.244679   \n",
       "24   523000  85.123868  86.977845  85.862738  ...  -13.179211  -13.179211   \n",
       "25   273300  66.000366  75.458126  78.072137  ...   -6.315769   -6.315769   \n",
       "26   362800  70.924883  70.523173  78.169163  ...   21.524699   21.524699   \n",
       "27   332600  78.985576  77.978485  77.582789  ...   21.947044   21.947044   \n",
       "28   260400  78.110990  78.580307  77.708054  ...   -6.997375   -6.997375   \n",
       "29   165000  50.808390  72.841781  73.503843  ...   -1.882148   -1.882148   \n",
       "\n",
       "        eom_21      eom_22      eom_23      eom_24      eom_25      eom_26  \\\n",
       "0   135.334375  135.334375  135.334375  135.334375  135.334375  135.334375   \n",
       "1   -19.174450  -19.174450  -19.174450  -19.174450  -19.174450  -19.174450   \n",
       "2   -19.930040  -19.930040  -19.930040  -19.930040  -19.930040  -19.930040   \n",
       "3   -25.479398  -25.479398  -25.479398  -25.479398  -25.479398  -25.479398   \n",
       "4    -7.125683   -7.125683   -7.125683   -7.125683   -7.125683   -7.125683   \n",
       "5  -169.796291 -169.796291 -169.796291 -169.796291 -169.796291 -169.796291   \n",
       "6   -52.042783  -52.042783  -52.042783  -52.042783  -52.042783  -52.042783   \n",
       "7   -30.502236  -30.502236  -30.502236  -30.502236  -30.502236  -30.502236   \n",
       "8    -0.000299   -0.000299   -0.000299   -0.000299   -0.000299   -0.000299   \n",
       "9    12.407965   12.407965   12.407965   12.407965   12.407965   12.407965   \n",
       "10    2.920773    2.920773    2.920773    2.920773    2.920773    2.920773   \n",
       "11  -20.020984  -20.020984  -20.020984  -20.020984  -20.020984  -20.020984   \n",
       "12  -23.220081  -23.220081  -23.220081  -23.220081  -23.220081  -23.220081   \n",
       "13   23.765926   23.765926   23.765926   23.765926   23.765926   23.765926   \n",
       "14 -258.349327 -258.349327 -258.349327 -258.349327 -258.349327 -258.349327   \n",
       "15  -54.308840  -54.308840  -54.308840  -54.308840  -54.308840  -54.308840   \n",
       "16   29.761936   29.761936   29.761936   29.761936   29.761936   29.761936   \n",
       "17  192.419153  192.419153  192.419153  192.419153  192.419153  192.419153   \n",
       "18   92.223204   92.223204   92.223204   92.223204   92.223204   92.223204   \n",
       "19   27.235713   27.235713   27.235713   27.235713   27.235713   27.235713   \n",
       "20    1.561737    1.561737    1.561737    1.561737    1.561737    1.561737   \n",
       "21   14.773437   14.773437   14.773437   14.773437   14.773437   14.773437   \n",
       "22   39.935758   39.935758   39.935758   39.935758   39.935758   39.935758   \n",
       "23   -4.244679   -4.244679   -4.244679   -4.244679   -4.244679   -4.244679   \n",
       "24  -13.179211  -13.179211  -13.179211  -13.179211  -13.179211  -13.179211   \n",
       "25   -6.315769   -6.315769   -6.315769   -6.315769   -6.315769   -6.315769   \n",
       "26   21.524699   21.524699   21.524699   21.524699   21.524699   21.524699   \n",
       "27   21.947044   21.947044   21.947044   21.947044   21.947044   21.947044   \n",
       "28   -6.997375   -6.997375   -6.997375   -6.997375   -6.997375   -6.997375   \n",
       "29   -1.882148   -1.882148   -1.882148   -1.882148   -1.882148   -1.882148   \n",
       "\n",
       "    volume_delta  labels  \n",
       "0       -34900.0       0  \n",
       "1        33200.0       2  \n",
       "2         5000.0       2  \n",
       "3       147500.0       2  \n",
       "4        25500.0       2  \n",
       "5      -205700.0       2  \n",
       "6         5100.0       2  \n",
       "7        41300.0       2  \n",
       "8        47300.0       2  \n",
       "9       -50800.0       2  \n",
       "10      230100.0       2  \n",
       "11     -275100.0       2  \n",
       "12       10100.0       2  \n",
       "13      -19700.0       2  \n",
       "14      -11600.0       2  \n",
       "15       41800.0       1  \n",
       "16       48200.0       2  \n",
       "17      -12900.0       2  \n",
       "18        2300.0       2  \n",
       "19       14900.0       2  \n",
       "20      -28600.0       2  \n",
       "21     1654800.0       2  \n",
       "22     -824800.0       2  \n",
       "23     -304700.0       2  \n",
       "24      -81300.0       2  \n",
       "25     -249700.0       2  \n",
       "26       89500.0       2  \n",
       "27      -30200.0       0  \n",
       "28      -72200.0       2  \n",
       "29      -95400.0       2  \n",
       "\n",
       "[30 rows x 429 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "\n",
    "np.random.seed(2)\n",
    "symbol = 'XLE'\n",
    "# use the path printed in above output cell after running stock_cnn.py. It's in below format\n",
    "df = pd.read_csv(\"data/df_\"+symbol+\".csv\")\n",
    "df = df[df['timestamp'] < '2016-01-01']  # For backtesting the last 5 years will be excluded from training.\n",
    "df['labels'] = df['labels'].astype(np.int8)\n",
    "if 'dividend_amount' in df.columns:\n",
    "    df.drop(columns=['dividend_amount', 'split_coefficient'], inplace=True)\n",
    "display(df.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into Training, Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features 425\n",
      "train_split = 0.8\n",
      "Shape of x, y train/cv/test (2721, 425) (2721,) (681, 425) (681,) (851, 425) (851,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "list_features = list(df.loc[:, 'open':'eom_26'].columns)\n",
    "print('Total number of features', len(list_features))\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.loc[:, 'open':'eom_26'].values, df['labels'].values, train_size=0.8, \n",
    "                                                    test_size=0.2, random_state=2, shuffle=True, stratify=df['labels'].values)\n",
    "\n",
    "# smote = RandomOverSampler(random_state=42, sampling_strategy='not majority')\n",
    "# x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "# print('Resampled dataset shape %s' % Counter(y_train))\n",
    "\n",
    "if 0.7*x_train.shape[0] < 2500:\n",
    "    train_split = 0.8\n",
    "else:\n",
    "    train_split = 0.7\n",
    "# train_split = 0.7\n",
    "print('train_split =',train_split)\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, train_size=train_split, test_size=1-train_split, \n",
    "                                                random_state=2, shuffle=True, stratify=y_train)\n",
    "mm_scaler = MinMaxScaler(feature_range=(0, 1)) # or StandardScaler?\n",
    "x_train = mm_scaler.fit_transform(x_train)\n",
    "x_cv = mm_scaler.transform(x_cv)\n",
    "x_test = mm_scaler.transform(x_test)\n",
    "\n",
    "x_main = x_train.copy()\n",
    "print(\"Shape of x, y train/cv/test {} {} {} {} {} {}\".format(x_train.shape, y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of total 441+ features select top 'N' features (let's include base features like close, adjusted_close etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 225  # should be a perfect square\n",
    "selection_method = 'all'\n",
    "topk = 320 if selection_method == 'all' else num_features\n",
    "# if train_split >= 0.8:\n",
    "#     topk = 400\n",
    "# else:\n",
    "#     topk = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('open', 'high', 'low', 'close', 'volume', 'rsi_6', 'rsi_7', 'rsi_8', 'rsi_9', 'rsi_10', 'rsi_11', 'rsi_12', 'rsi_13', 'rsi_14', 'rsi_15', 'rsi_16', 'rsi_17', 'rsi_18', 'rsi_19', 'rsi_20', 'rsi_21', 'rsi_22', 'rsi_23', 'rsi_24', 'rsi_25', 'rsi_26', 'wr_6', 'wr_7', 'wr_8', 'wr_9', 'wr_10', 'wr_11', 'wr_12', 'wr_13', 'wr_14', 'wr_15', 'wr_16', 'wr_17', 'wr_18', 'wr_19', 'wr_20', 'wr_21', 'wr_22', 'wr_23', 'wr_24', 'wr_25', 'wr_26', 'mfi_6', 'mfi_7', 'mfi_8', 'mfi_9', 'mfi_10', 'mfi_11', 'mfi_12', 'mfi_13', 'mfi_14', 'mfi_15', 'mfi_16', 'mfi_17', 'mfi_18', 'mfi_19', 'mfi_20', 'mfi_21', 'mfi_22', 'mfi_23', 'mfi_24', 'mfi_25', 'mfi_26', 'roc_6', 'roc_7', 'roc_8', 'roc_9', 'roc_10', 'roc_11', 'roc_12', 'roc_13', 'roc_14', 'roc_15', 'roc_16', 'roc_17', 'roc_18', 'roc_19', 'roc_20', 'roc_21', 'roc_22', 'roc_23', 'roc_24', 'roc_25', 'roc_26', 'cmf_6', 'cmf_7', 'cmf_8', 'cmf_9', 'cmf_10', 'cmf_11', 'cmf_12', 'cmf_13', 'cmf_14', 'cmf_15', 'cmf_16', 'cmf_17', 'cmf_18', 'cmf_19', 'cmf_20', 'cmf_21', 'cmf_22', 'cmf_23', 'cmf_24', 'cmf_25', 'cmf_26', 'cmo_6', 'cmo_7', 'cmo_8', 'cmo_9', 'cmo_10', 'cmo_11', 'cmo_12', 'cmo_13', 'cmo_14', 'cmo_15', 'cmo_16', 'cmo_17', 'cmo_18', 'cmo_19', 'cmo_20', 'cmo_21', 'cmo_22', 'cmo_23', 'cmo_24', 'cmo_25', 'cmo_26', 'trix_6', 'trix_7', 'trix_8', 'trix_9', 'trix_10', 'trix_11', 'trix_12', 'trix_13', 'trix_14', 'trix_15', 'trix_16', 'trix_17', 'trix_18', 'trix_19', 'trix_20', 'trix_21', 'trix_22', 'trix_23', 'trix_24', 'trix_25', 'trix_26', 'cci_6', 'cci_7', 'cci_8', 'cci_9', 'cci_10', 'cci_11', 'cci_12', 'cci_13', 'cci_14', 'cci_15', 'cci_16', 'cci_17', 'cci_18', 'cci_19', 'cci_20', 'cci_21', 'cci_22', 'cci_23', 'cci_24', 'cci_25', 'cci_26', 'dpo_6', 'dpo_7', 'dpo_8', 'dpo_9', 'dpo_10', 'dpo_11', 'dpo_12', 'dpo_13', 'dpo_14', 'dpo_15', 'dpo_16', 'dpo_17', 'dpo_18', 'dpo_19', 'dpo_20', 'dpo_21', 'dpo_22', 'dpo_23', 'dpo_24', 'dpo_25', 'dpo_26', 'kst_6', 'kst_7', 'kst_8', 'kst_9', 'kst_10', 'kst_11', 'kst_12', 'kst_13', 'kst_14', 'kst_15', 'kst_16', 'kst_17', 'kst_18', 'kst_19', 'kst_20', 'kst_21', 'kst_22', 'kst_23', 'kst_24', 'kst_25', 'kst_26', 'dmi_6', 'dmi_7', 'dmi_8', 'dmi_9', 'dmi_10', 'dmi_11', 'dmi_12', 'dmi_13', 'dmi_14', 'dmi_15', 'dmi_16', 'dmi_17', 'dmi_18', 'dmi_19', 'dmi_20', 'dmi_21', 'dmi_22', 'dmi_23', 'dmi_24', 'dmi_25', 'dmi_26', 'fi_6', 'fi_7', 'fi_8', 'fi_9', 'fi_10', 'fi_11', 'fi_12', 'fi_13', 'fi_14', 'fi_15', 'fi_16', 'fi_17', 'fi_18', 'fi_19', 'fi_20', 'fi_21', 'fi_22', 'fi_23', 'fi_24', 'fi_25', 'fi_26', 'rsv_6', 'kdjk_6', 'rsv_7', 'kdjk_7', 'rsv_8', 'kdjk_8', 'rsv_9', 'kdjk_9', 'rsv_10', 'kdjk_10', 'rsv_11', 'kdjk_11', 'rsv_12', 'kdjk_12', 'rsv_13', 'kdjk_13', 'rsv_14', 'kdjk_14', 'rsv_15', 'kdjk_15', 'rsv_16', 'kdjk_16', 'rsv_17', 'kdjk_17', 'rsv_18', 'kdjk_18', 'rsv_19', 'kdjk_19', 'rsv_20', 'kdjk_20', 'rsv_21', 'kdjk_21', 'rsv_22', 'kdjk_22', 'rsv_23', 'kdjk_23', 'rsv_24', 'kdjk_24', 'rsv_25', 'kdjk_25', 'rsv_26', 'kdjk_26', 'eom_6', 'eom_7', 'eom_8', 'eom_9', 'eom_10', 'eom_11', 'eom_12', 'eom_13', 'eom_14', 'eom_15', 'eom_16', 'eom_17', 'eom_18', 'eom_19', 'eom_20', 'eom_21', 'eom_22', 'eom_23', 'eom_24', 'eom_25', 'eom_26')\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 215 216 217 218 219 220 221 222 223 224 225 226 227\n",
      " 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245\n",
      " 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263\n",
      " 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281\n",
      " 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299\n",
      " 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317\n",
      " 318 319 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356\n",
      " 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374\n",
      " 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392\n",
      " 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410\n",
      " 411 412 413 414 415 416 417 418 419 420 421 422 423 424]\n",
      "****************************************\n",
      "320 ('close', 'volume', 'rsi_6', 'rsi_7', 'rsi_8', 'rsi_9', 'rsi_10', 'rsi_11', 'rsi_12', 'rsi_13', 'rsi_14', 'rsi_15', 'rsi_16', 'rsi_17', 'rsi_18', 'rsi_19', 'rsi_20', 'rsi_21', 'rsi_22', 'rsi_24', 'rsi_26', 'wr_6', 'wr_7', 'wr_8', 'wr_9', 'wr_10', 'wr_11', 'wr_12', 'wr_13', 'wr_14', 'wr_15', 'wr_16', 'wr_17', 'wr_18', 'wr_19', 'wr_20', 'wr_21', 'wr_22', 'wr_23', 'wr_24', 'wr_25', 'wr_26', 'mfi_6', 'mfi_7', 'mfi_8', 'mfi_9', 'mfi_10', 'mfi_11', 'mfi_12', 'mfi_13', 'mfi_14', 'mfi_15', 'mfi_16', 'mfi_17', 'mfi_18', 'mfi_19', 'mfi_20', 'mfi_21', 'mfi_22', 'mfi_23', 'mfi_24', 'mfi_25', 'mfi_26', 'roc_6', 'roc_7', 'roc_8', 'roc_9', 'roc_10', 'roc_11', 'roc_12', 'roc_13', 'roc_14', 'roc_15', 'roc_16', 'roc_17', 'roc_18', 'roc_19', 'roc_20', 'roc_21', 'roc_22', 'roc_23', 'roc_24', 'roc_25', 'roc_26', 'cmf_6', 'cmf_7', 'cmf_8', 'cmf_9', 'cmf_10', 'cmf_11', 'cmf_12', 'cmf_13', 'cmf_14', 'cmf_15', 'cmf_16', 'cmf_17', 'cmf_18', 'cmf_19', 'cmf_20', 'cmf_22', 'cmf_23', 'cmf_24', 'cmf_25', 'cmf_26', 'cmo_6', 'cmo_7', 'cmo_8', 'cmo_9', 'cmo_10', 'cmo_11', 'cmo_12', 'cmo_13', 'cmo_14', 'cmo_15', 'cmo_16', 'cmo_17', 'cmo_18', 'cmo_19', 'cmo_20', 'cmo_21', 'cmo_22', 'cmo_23', 'cmo_25', 'wma_24', 'hma_0', 'hma_1', 'hma_8', 'hma_9', 'trix_6', 'trix_7', 'trix_9', 'trix_10', 'trix_13', 'trix_14', 'trix_16', 'trix_18', 'trix_21', 'trix_24', 'cci_6', 'cci_7', 'cci_8', 'cci_9', 'cci_10', 'cci_11', 'cci_12', 'cci_13', 'cci_14', 'cci_15', 'cci_16', 'cci_17', 'cci_18', 'cci_19', 'cci_20', 'cci_21', 'cci_22', 'cci_23', 'cci_24', 'cci_25', 'cci_26', 'dpo_6', 'dpo_7', 'dpo_8', 'dpo_9', 'dpo_10', 'dpo_11', 'dpo_12', 'dpo_13', 'dpo_14', 'dpo_17', 'dpo_19', 'dpo_21', 'dpo_23', 'dpo_24', 'dpo_25', 'dpo_26', 'kst_8', 'kst_9', 'kst_10', 'kst_11', 'kst_12', 'kst_13', 'kst_14', 'kst_15', 'kst_16', 'kst_17', 'kst_18', 'kst_19', 'kst_20', 'kst_21', 'kst_22', 'kst_23', 'kst_24', 'kst_25', 'kst_26', 'dmi_6', 'dmi_7', 'dmi_8', 'dmi_9', 'dmi_10', 'dmi_11', 'dmi_12', 'dmi_13', 'dmi_14', 'dmi_15', 'dmi_16', 'dmi_17', 'dmi_18', 'dmi_19', 'dmi_20', 'dmi_21', 'dmi_22', 'dmi_23', 'dmi_24', 'dmi_25', 'dmi_26', 'bb_6', 'bb_7', 'bb_8', 'bb_9', 'bb_10', 'bb_11', 'bb_12', 'bb_13', 'bb_14', 'bb_15', 'bb_16', 'bb_17', 'bb_18', 'bb_19', 'bb_20', 'bb_21', 'bb_22', 'bb_23', 'bb_24', 'bb_25', 'bb_26', 'fi_6', 'fi_7', 'fi_8', 'fi_9', 'fi_10', 'fi_11', 'fi_12', 'fi_13', 'fi_14', 'fi_15', 'fi_16', 'fi_17', 'fi_18', 'fi_19', 'fi_20', 'fi_21', 'fi_22', 'fi_23', 'fi_24', 'fi_25', 'fi_26', 'rsv_6', 'kdjk_6', 'rsv_7', 'kdjk_7', 'rsv_8', 'kdjk_8', 'rsv_9', 'kdjk_9', 'rsv_10', 'kdjk_10', 'rsv_11', 'kdjk_11', 'rsv_12', 'kdjk_12', 'rsv_13', 'kdjk_13', 'rsv_14', 'kdjk_14', 'rsv_15', 'kdjk_15', 'rsv_16', 'kdjk_16', 'rsv_17', 'kdjk_17', 'rsv_18', 'kdjk_18', 'rsv_19', 'kdjk_19', 'rsv_20', 'kdjk_20', 'rsv_21', 'kdjk_21', 'rsv_22', 'kdjk_22', 'rsv_23', 'kdjk_23', 'rsv_24', 'kdjk_24', 'rsv_25', 'kdjk_25', 'rsv_26', 'kdjk_26', 'eom_6', 'eom_7', 'eom_8', 'eom_9', 'eom_10', 'eom_11', 'eom_12', 'eom_13', 'eom_14', 'eom_15', 'eom_16', 'eom_17', 'eom_18', 'eom_19', 'eom_20', 'eom_21', 'eom_22', 'eom_23', 'eom_24', 'eom_25', 'eom_26')\n",
      "[  3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20\n",
      "  21  23  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40\n",
      "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
      "  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76\n",
      "  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94\n",
      "  95  96  97  98  99 100 101 102 103 105 106 107 108 109 110 111 112 113\n",
      " 114 115 116 117 118 119 120 121 122 123 124 125 126 127 129 191 194 195\n",
      " 202 203 215 216 218 219 222 223 225 227 230 233 236 237 238 239 240 241\n",
      " 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259\n",
      " 260 261 262 263 264 265 268 270 272 274 275 276 277 280 281 282 283 284\n",
      " 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302\n",
      " 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320\n",
      " 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338\n",
      " 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356\n",
      " 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374\n",
      " 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392\n",
      " 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410\n",
      " 411 412 413 414 415 416 417 418 419 420 421 422 423 424]\n",
      "CPU times: user 12.1 s, sys: 221 ms, total: 12.3 s\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from operator import itemgetter\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "if selection_method == 'anova' or selection_method == 'all':\n",
    "    select_k_best = SelectKBest(f_classif, k=topk)\n",
    "    if selection_method != 'all':\n",
    "        x_train = select_k_best.fit_transform(x_main, y_train)\n",
    "        x_cv = select_k_best.transform(x_cv)\n",
    "        x_test = select_k_best.transform(x_test)\n",
    "    else:\n",
    "        select_k_best.fit(x_main, y_train)\n",
    "    \n",
    "    selected_features_anova = itemgetter(*select_k_best.get_support(indices=True))(list_features)\n",
    "    print(selected_features_anova)\n",
    "    print(select_k_best.get_support(indices=True))\n",
    "    print(\"****************************************\")\n",
    "    \n",
    "if selection_method == 'mutual_info' or selection_method == 'all':\n",
    "    select_k_best = SelectKBest(mutual_info_classif, k=topk)\n",
    "    if selection_method != 'all':\n",
    "        x_train = select_k_best.fit_transform(x_main, y_train)\n",
    "        x_cv = select_k_best.transform(x_cv)\n",
    "        x_test = select_k_best.transform(x_test)\n",
    "    else:\n",
    "        select_k_best.fit(x_main, y_train)\n",
    "\n",
    "    selected_features_mic = itemgetter(*select_k_best.get_support(indices=True))(list_features)\n",
    "    print(len(selected_features_mic), selected_features_mic)\n",
    "    print(select_k_best.get_support(indices=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common selected featues 294 ['cmf_16', 'rsv_23', 'mfi_11', 'rsi_17', 'cmo_8', 'rsv_6', 'wr_6', 'mfi_17', 'fi_11', 'fi_14', 'kst_14', 'fi_24', 'rsv_21', 'eom_6', 'mfi_26', 'trix_21', 'kst_15', 'cci_16', 'dmi_8', 'mfi_21', 'cmf_8', 'fi_10', 'cmf_13', 'rsi_20', 'cmf_15', 'fi_18', 'eom_22', 'cci_10', 'dmi_18', 'roc_10', 'eom_24', 'cci_12', 'kst_8', 'kst_21', 'rsv_18', 'eom_15', 'mfi_25', 'mfi_16', 'cci_26', 'roc_6', 'rsi_15', 'dpo_9', 'cmo_20', 'wr_24', 'mfi_23', 'roc_11', 'rsv_25', 'roc_7', 'dmi_7', 'kst_16', 'dpo_17', 'kdjk_23', 'roc_13', 'close', 'wr_18', 'roc_25', 'cci_18', 'mfi_7', 'dmi_6', 'cmo_25', 'kdjk_12', 'cmo_10', 'kst_17', 'kdjk_13', 'rsi_6', 'eom_23', 'rsv_7', 'roc_22', 'kst_24', 'cmo_9', 'fi_16', 'cci_6', 'cci_19', 'kdjk_20', 'wr_17', 'roc_26', 'eom_7', 'fi_21', 'eom_11', 'kdjk_19', 'rsv_19', 'volume', 'mfi_19', 'roc_9', 'trix_9', 'rsv_20', 'rsi_21', 'wr_10', 'dpo_14', 'kdjk_17', 'kdjk_8', 'mfi_12', 'dpo_6', 'dmi_12', 'cmf_18', 'mfi_10', 'eom_26', 'cci_21', 'cmf_19', 'kst_9', 'dmi_9', 'kst_23', 'eom_16', 'wr_20', 'cmf_14', 'cci_17', 'wr_16', 'fi_6', 'fi_12', 'cci_25', 'kdjk_18', 'cmf_6', 'rsv_16', 'cmf_20', 'cci_15', 'roc_16', 'dmi_23', 'dmi_15', 'cmf_24', 'rsv_15', 'kst_11', 'cmo_17', 'kdjk_25', 'roc_23', 'dpo_26', 'dpo_25', 'rsi_12', 'eom_9', 'cci_13', 'mfi_13', 'cmo_21', 'wr_25', 'cmo_15', 'dpo_13', 'dpo_19', 'cmf_22', 'kdjk_15', 'fi_17', 'dmi_25', 'eom_25', 'trix_16', 'wr_14', 'mfi_15', 'wr_13', 'mfi_22', 'dmi_10', 'wr_12', 'trix_24', 'fi_9', 'wr_7', 'mfi_14', 'fi_26', 'cmo_6', 'rsv_13', 'rsv_24', 'kdjk_16', 'trix_18', 'dpo_7', 'rsv_12', 'kdjk_14', 'trix_13', 'cci_8', 'rsi_19', 'dpo_23', 'cci_14', 'rsv_9', 'wr_23', 'roc_18', 'rsv_8', 'eom_14', 'kdjk_22', 'dmi_13', 'wr_9', 'cci_22', 'eom_8', 'cmo_22', 'dmi_11', 'wr_26', 'dmi_16', 'rsv_14', 'kst_18', 'rsv_10', 'rsi_14', 'kdjk_26', 'cmf_11', 'wr_11', 'trix_7', 'kst_25', 'dpo_21', 'cmo_23', 'rsi_24', 'cci_9', 'dmi_26', 'dmi_22', 'cmf_9', 'mfi_18', 'mfi_24', 'eom_18', 'eom_19', 'eom_13', 'roc_24', 'dpo_24', 'rsv_11', 'eom_20', 'wr_21', 'dpo_11', 'cmo_19', 'rsi_11', 'fi_13', 'fi_20', 'kst_13', 'rsi_9', 'rsv_26', 'mfi_8', 'wr_19', 'cci_24', 'roc_19', 'dmi_21', 'dmi_24', 'fi_25', 'rsi_22', 'cmo_7', 'kst_26', 'roc_8', 'fi_8', 'rsi_7', 'rsi_13', 'fi_22', 'kst_22', 'eom_10', 'cmo_12', 'trix_10', 'trix_6', 'dmi_19', 'kdjk_9', 'dpo_8', 'fi_23', 'trix_14', 'rsi_10', 'rsi_26', 'cmf_25', 'cmf_10', 'eom_21', 'cmo_18', 'cmo_14', 'mfi_6', 'kst_19', 'cci_23', 'eom_12', 'kdjk_10', 'cmf_17', 'cmo_13', 'dmi_20', 'kst_10', 'cci_11', 'fi_15', 'mfi_9', 'dmi_14', 'cmf_12', 'wr_8', 'cci_7', 'cci_20', 'kst_20', 'kdjk_7', 'fi_7', 'rsi_16', 'dpo_10', 'fi_19', 'kdjk_21', 'cmf_26', 'dmi_17', 'rsv_22', 'wr_22', 'kst_12', 'dpo_12', 'roc_17', 'cmo_11', 'roc_15', 'kdjk_11', 'roc_20', 'cmf_7', 'roc_14', 'roc_12', 'cmo_16', 'kdjk_24', 'kdjk_6', 'roc_21', 'eom_17', 'rsv_17', 'mfi_20', 'cmf_23', 'wr_15', 'rsi_8', 'rsi_18']\n",
      "[3, 4, 5, 8, 10, 11, 13, 14, 16, 18, 19, 20, 21, 23, 26, 27, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 43, 44, 45, 46, 48, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 78, 80, 81, 84, 85, 86, 87, 88, 89, 91, 92, 94, 96, 97, 98, 99, 101, 102, 103, 105, 107, 110, 111, 112, 113, 114, 119, 121, 123, 124, 125, 126, 127, 129, 216, 218, 222, 225, 227, 230, 233, 236, 238, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 254, 255, 256, 257, 258, 260, 262, 264, 265, 268, 270, 272, 274, 275, 276, 277, 280, 281, 283, 285, 286, 287, 288, 289, 290, 293, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 308, 309, 311, 314, 315, 316, 317, 318, 319, 341, 343, 344, 345, 346, 347, 348, 349, 351, 352, 353, 355, 356, 359, 360, 361, 362, 364, 366, 367, 368, 370, 372, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 385, 386, 387, 388, 389, 390, 391, 392, 395, 396, 397, 398, 400, 401, 402, 403, 404, 405, 406, 407, 409, 411, 412, 413, 414, 416, 417, 418, 420, 421, 422, 423, 424]\n"
     ]
    }
   ],
   "source": [
    "if selection_method == 'all':\n",
    "    common = list(set(selected_features_anova).intersection(selected_features_mic))\n",
    "    print(\"common selected featues\", len(common), common)\n",
    "    if len(common) < num_features:\n",
    "        raise Exception('number of common features found {} < {} required features. Increase \"topk variable\"'.format(len(common), num_features))\n",
    "    feat_idx = []\n",
    "    for c in common:\n",
    "        feat_idx.append(list_features.index(c))\n",
    "    feat_idx = sorted(feat_idx[0:225])\n",
    "    print(feat_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x, y train/cv/test (2721, 225) (2721,) (681, 225) (681,) (851, 225) (851,)\n"
     ]
    }
   ],
   "source": [
    "if selection_method == 'all':\n",
    "    x_train = x_train[:, feat_idx]\n",
    "    x_cv = x_cv[:, feat_idx]\n",
    "    x_test = x_test[:, feat_idx]\n",
    "\n",
    "print(\"Shape of x, y train/cv/test {} {} {} {} {} {}\".format(x_train.shape, \n",
    "                                                             y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of class 0 = 5.990444689452407, class 1 = 5.806688717383315\n"
     ]
    }
   ],
   "source": [
    "_labels, _counts = np.unique(y_train, return_counts=True)\n",
    "print(\"percentage of class 0 = {}, class 1 = {}\".format(_counts[0]/len(y_train) * 100, _counts[1]/len(y_train) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "def get_sample_weights(y):\n",
    "    \"\"\"\n",
    "    calculate the sample weights based on class weights. Used for models with\n",
    "    imbalanced data and one hot encoding prediction.\n",
    "\n",
    "    params:\n",
    "        y: class labels as integers\n",
    "    \"\"\"\n",
    "\n",
    "    y = y.astype(int)  # compute_class_weight needs int labels\n",
    "    class_weights = compute_class_weight('balanced', np.unique(y), y)\n",
    "    \n",
    "    print(\"real class weights are {}\".format(class_weights), np.unique(y))\n",
    "    print(\"value_counts\", np.unique(y, return_counts=True))\n",
    "    sample_weights = y.copy().astype(float)\n",
    "    for i in np.unique(y):\n",
    "        sample_weights[sample_weights == i] = class_weights[i]  # if i == 2 else 0.8 * class_weights[i]\n",
    "        # sample_weights = np.where(sample_weights == i, class_weights[int(i)], y_)\n",
    "\n",
    "    return sample_weights\n",
    "\n",
    "def reshape_as_image(x, img_width, img_height):\n",
    "    x_temp = np.zeros((len(x), img_height, img_width))\n",
    "    for i in range(x.shape[0]):\n",
    "        # print(type(x), type(x_temp), x.shape)\n",
    "        x_temp[i] = np.reshape(x[i], (img_height, img_width))\n",
    "\n",
    "    return x_temp\n",
    "\n",
    "def f1_weighted(y_true, y_pred):\n",
    "    y_true_class = tf.math.argmax(y_true, axis=1, output_type=tf.dtypes.int32)\n",
    "    y_pred_class = tf.math.argmax(y_pred, axis=1, output_type=tf.dtypes.int32)\n",
    "    conf_mat = tf.math.confusion_matrix(y_true_class, y_pred_class)  # can use conf_mat[0, :], tf.slice()\n",
    "    # precision = TP/TP+FP, recall = TP/TP+FN\n",
    "    rows, cols = conf_mat.get_shape()\n",
    "    size = y_true_class.get_shape()[0]\n",
    "    precision = tf.constant([0, 0, 0])  # change this to use rows/cols as size\n",
    "    recall = tf.constant([0, 0, 0])\n",
    "    class_counts = tf.constant([0, 0, 0])\n",
    "\n",
    "    def get_precision(i, conf_mat):\n",
    "        print(\"prec check\", conf_mat, conf_mat[i, i], tf.reduce_sum(conf_mat[:, i]))\n",
    "        precision[i].assign(conf_mat[i, i] / tf.reduce_sum(conf_mat[:, i]))\n",
    "        recall[i].assign(conf_mat[i, i] / tf.reduce_sum(conf_mat[i, :]))\n",
    "        tf.add(i, 1)\n",
    "        return i, conf_mat, precision, recall\n",
    "\n",
    "    def tf_count(i):\n",
    "        elements_equal_to_value = tf.equal(y_true_class, i)\n",
    "        as_ints = tf.cast(elements_equal_to_value, tf.int32)\n",
    "        count = tf.reduce_sum(as_ints)\n",
    "        class_counts[i].assign(count)\n",
    "        tf.add(i, 1)\n",
    "        return count\n",
    "\n",
    "    def condition(i, conf_mat):\n",
    "        return tf.less(i, 3)\n",
    "\n",
    "    i = tf.constant(3)\n",
    "    i, conf_mat = tf.while_loop(condition, get_precision, [i, conf_mat])\n",
    "\n",
    "    i = tf.constant(3)\n",
    "    c = lambda i: tf.less(i, 3)\n",
    "    b = tf_count(i)\n",
    "    tf.while_loop(c, b, [i])\n",
    "\n",
    "    weights = tf.math.divide(class_counts, size)\n",
    "    numerators = tf.math.multiply(tf.math.multiply(precision, recall), tf.constant(2))\n",
    "    denominators = tf.math.add(precision, recall)\n",
    "    f1s = tf.math.divide(numerators, denominators)\n",
    "    weighted_f1 = tf.reduce_sum(f.math.multiply(f1s, weights))\n",
    "    return weighted_f1\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    this calculates precision & recall \n",
    "    \"\"\"\n",
    "\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))  # mistake: y_pred of 0.3 is also considered 1\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    # y_true_class = tf.math.argmax(y_true, axis=1, output_type=tf.dtypes.int32)\n",
    "    # y_pred_class = tf.math.argmax(y_pred, axis=1, output_type=tf.dtypes.int32)\n",
    "    # conf_mat = tf.math.confusion_matrix(y_true_class, y_pred_class)\n",
    "    # tf.Print(conf_mat, [conf_mat], \"confusion_matrix\")\n",
    "\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "get_custom_objects().update({\"f1_metric\": f1_metric, \"f1_weighted\": f1_weighted})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real class weights are [5.56441718 5.74050633 0.37791667] [0 1 2]\n",
      "value_counts (array([0, 1, 2]), array([ 163,  158, 2400]))\n",
      "Test sample_weights\n",
      "[2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 0]\n",
      "[0.37791667 0.37791667 0.37791667 0.37791667 5.56441718 0.37791667\n",
      " 0.37791667 0.37791667 0.37791667 0.37791667 0.37791667 0.37791667\n",
      " 0.37791667 0.37791667 0.37791667 0.37791667 0.37791667 0.37791667\n",
      " 0.37791667 0.37791667 0.37791667 5.56441718 0.37791667 0.37791667\n",
      " 0.37791667 0.37791667 0.37791667 0.37791667 0.37791667 5.56441718]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass classes=[0 1 2], y=[0 1 2 ... 2 2 2] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "sample_weights = get_sample_weights(y_train)\n",
    "print(\"Test sample_weights\")\n",
    "rand_idx = np.random.randint(0, 1000, 30)\n",
    "print(y_train[rand_idx])\n",
    "print(sample_weights[rand_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train (2721, 3)\n"
     ]
    }
   ],
   "source": [
    "one_hot_enc = OneHotEncoder(sparse=False, categories='auto')  # , categories='auto'\n",
    "y_train = one_hot_enc.fit_transform(y_train.reshape(-1, 1))\n",
    "print(\"y_train\",y_train.shape)\n",
    "y_cv = one_hot_enc.transform(y_cv.reshape(-1, 1))\n",
    "y_test = one_hot_enc.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final shape of x, y train/test (2721, 15, 15, 3) (2721, 3) (851, 15, 15, 3) (851, 3)\n"
     ]
    }
   ],
   "source": [
    "dim = int(np.sqrt(num_features))\n",
    "x_train = reshape_as_image(x_train, dim, dim)\n",
    "x_cv = reshape_as_image(x_cv, dim, dim)\n",
    "x_test = reshape_as_image(x_test, dim, dim)\n",
    "# adding a 1-dim for channels (3)\n",
    "x_train = np.stack((x_train,) * 3, axis=-1)\n",
    "x_test = np.stack((x_test,) * 3, axis=-1)\n",
    "x_cv = np.stack((x_cv,) * 3, axis=-1)\n",
    "print(\"final shape of x, y train/test {} {} {} {}\".format(x_train.shape, y_train.shape, x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "columns = rows = 3\n",
    "for i in range(1, columns*rows +1):\n",
    "    index = np.random.randint(len(x_train))\n",
    "    img = x_train[index]\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title('image_'+str(index)+'_class_'+str(np.argmax(y_train[index])), fontsize=10)\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, LeakyReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger, Callback\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from tensorflow.keras.initializers import RandomUniform, RandomNormal\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "params = {'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.2, 'conv2d_filters_1': 32, 'conv2d_kernel_size_1': 3, 'conv2d_mp_1': 0, \n",
    "                                               'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.0, 'conv2d_do_2': 0.3, \n",
    "                                               'conv2d_filters_2': 64, 'conv2d_kernel_size_2': 3, 'conv2d_mp_2': 2, 'conv2d_strides_2': 1, \n",
    "                                               'kernel_regularizer_2': 0.0, 'layers': 'two'}, \n",
    "           'dense_layers': {'dense_do_1': 0.3, 'dense_nodes_1': 128, 'kernel_regularizer_1': 0.0, 'layers': 'one'},\n",
    "           'epochs': 50, 'lr': 0.001, 'optimizer': 'adam'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import *\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "def f1_custom(y_true, y_pred):\n",
    "    y_t = np.argmax(y_true, axis=1)\n",
    "    y_p = np.argmax(y_pred, axis=1)\n",
    "    f1_score(y_t, y_p, labels=None, average='weighted', sample_weight=None, zero_division='warn')\n",
    "\n",
    "def create_model_cnn(params):\n",
    "    model = Sequential()\n",
    "\n",
    "    print(\"Training with params {}\".format(params))\n",
    "    \n",
    "    conv2d_layer1 = Conv2D(params[\"conv2d_layers\"][\"conv2d_filters_1\"],\n",
    "                           params[\"conv2d_layers\"][\"conv2d_kernel_size_1\"],\n",
    "                           strides=params[\"conv2d_layers\"][\"conv2d_strides_1\"],\n",
    "                           kernel_regularizer=regularizers.l2(params[\"conv2d_layers\"][\"kernel_regularizer_1\"]), \n",
    "                           padding='same',activation=\"relu\", use_bias=True,\n",
    "                           kernel_initializer='glorot_uniform',\n",
    "                           input_shape=(x_train[0].shape[0],\n",
    "                                        x_train[0].shape[1], x_train[0].shape[2]))\n",
    "    model.add(conv2d_layer1)\n",
    "    if params[\"conv2d_layers\"]['conv2d_mp_1'] > 1:\n",
    "        model.add(MaxPool2D(pool_size=params[\"conv2d_layers\"]['conv2d_mp_1']))\n",
    "        \n",
    "    model.add(Dropout(params['conv2d_layers']['conv2d_do_1']))\n",
    "    if params[\"conv2d_layers\"]['layers'] == 'two':\n",
    "        conv2d_layer2 = Conv2D(params[\"conv2d_layers\"][\"conv2d_filters_2\"],\n",
    "                               params[\"conv2d_layers\"][\"conv2d_kernel_size_2\"],\n",
    "                               strides=params[\"conv2d_layers\"][\"conv2d_strides_2\"],\n",
    "                               kernel_regularizer=regularizers.l2(params[\"conv2d_layers\"][\"kernel_regularizer_2\"]),\n",
    "                               padding='same',activation=\"relu\", use_bias=True,\n",
    "                               kernel_initializer='glorot_uniform')\n",
    "        model.add(conv2d_layer2)\n",
    "        \n",
    "        if params[\"conv2d_layers\"]['conv2d_mp_2'] > 1:\n",
    "            model.add(MaxPool2D(pool_size=params[\"conv2d_layers\"]['conv2d_mp_2']))\n",
    "        \n",
    "        model.add(Dropout(params['conv2d_layers']['conv2d_do_2']))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(params['dense_layers'][\"dense_nodes_1\"], activation='relu'))\n",
    "    model.add(Dropout(params['dense_layers']['dense_do_1']))\n",
    "\n",
    "    if params['dense_layers'][\"layers\"] == 'two':\n",
    "        model.add(Dense(params['dense_layers'][\"dense_nodes_2\"], activation='relu', \n",
    "                        kernel_regularizer=params['dense_layers'][\"kernel_regularizer_1\"]))\n",
    "        model.add(Dropout(params['dense_layers']['dense_do_2']))\n",
    "\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    if params[\"optimizer\"] == 'rmsprop':\n",
    "        optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "    elif params[\"optimizer\"] == 'sgd':\n",
    "        optimizer = optimizers.SGD(lr=params[\"lr\"], decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    elif params[\"optimizer\"] == 'adam':\n",
    "        optimizer = optimizers.Adam(learning_rate=params[\"lr\"], beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', f1_metric])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def check_baseline(pred, y_test):\n",
    "    print(\"size of test set\", len(y_test))\n",
    "    e = np.equal(pred, y_test)\n",
    "    print(\"TP class counts\", np.unique(y_test[e], return_counts=True))\n",
    "    print(\"True class counts\", np.unique(y_test, return_counts=True))\n",
    "    print(\"Pred class counts\", np.unique(pred, return_counts=True))\n",
    "    holds = np.unique(y_test, return_counts=True)[1][2]  # number 'hold' predictions\n",
    "    print(\"baseline acc:\", (holds/len(y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params {'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.2, 'conv2d_filters_1': 32, 'conv2d_kernel_size_1': 3, 'conv2d_mp_1': 0, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.0, 'conv2d_do_2': 0.3, 'conv2d_filters_2': 64, 'conv2d_kernel_size_2': 3, 'conv2d_mp_2': 2, 'conv2d_strides_2': 1, 'kernel_regularizer_2': 0.0, 'layers': 'two'}, 'dense_layers': {'dense_do_1': 0.3, 'dense_nodes_1': 128, 'kernel_regularizer_1': 0.0, 'layers': 'one'}, 'epochs': 50, 'lr': 0.001, 'optimizer': 'adam'}\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import plot_model\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "model = create_model_cnn(params)\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=False)\n",
    "\n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "best_model_path = os.path.join('.', 'best_model_keras')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                   patience=100, min_delta=0.0001)\n",
    "# csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'log_training_batch.log'), append=True)\n",
    "rlp = ReduceLROnPlateau(monitor='val_loss', factor=0.02, patience=20, verbose=1, mode='min',\n",
    "                        min_delta=0.001, cooldown=1, min_lr=0.0001)\n",
    "mcp = ModelCheckpoint(best_model_path, monitor='val_f1_metric', verbose=1,\n",
    "                      save_best_only=True, save_weights_only=False, mode='max', period=1)  # val_f1_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "43/43 [==============================] - 5s 70ms/step - loss: 0.9079 - accuracy: 0.2984 - f1_metric: 0.1463 - val_loss: 1.0272 - val_accuracy: 0.2482 - val_f1_metric: 0.1195\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.11948, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 2/50\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 0.7017 - accuracy: 0.3679 - f1_metric: 0.2450 - val_loss: 1.0806 - val_accuracy: 0.3084 - val_f1_metric: 0.2063\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.11948 to 0.20626, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 3/50\n",
      "43/43 [==============================] - 3s 75ms/step - loss: 0.6414 - accuracy: 0.4300 - f1_metric: 0.3917 - val_loss: 1.0021 - val_accuracy: 0.3186 - val_f1_metric: 0.2492\n",
      "\n",
      "Epoch 00003: val_f1_metric improved from 0.20626 to 0.24924, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 4/50\n",
      "43/43 [==============================] - 3s 80ms/step - loss: 0.6109 - accuracy: 0.4377 - f1_metric: 0.4169 - val_loss: 0.9755 - val_accuracy: 0.4097 - val_f1_metric: 0.3792\n",
      "\n",
      "Epoch 00004: val_f1_metric improved from 0.24924 to 0.37915, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 5/50\n",
      "43/43 [==============================] - 3s 64ms/step - loss: 0.5992 - accuracy: 0.4605 - f1_metric: 0.4417 - val_loss: 1.1233 - val_accuracy: 0.3054 - val_f1_metric: 0.2852\n",
      "\n",
      "Epoch 00005: val_f1_metric did not improve from 0.37915\n",
      "Epoch 6/50\n",
      "43/43 [==============================] - 3s 73ms/step - loss: 0.5948 - accuracy: 0.4498 - f1_metric: 0.4408 - val_loss: 0.8952 - val_accuracy: 0.4288 - val_f1_metric: 0.4200\n",
      "\n",
      "Epoch 00006: val_f1_metric improved from 0.37915 to 0.42003, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 7/50\n",
      "43/43 [==============================] - 3s 80ms/step - loss: 0.5766 - accuracy: 0.4410 - f1_metric: 0.4285 - val_loss: 0.9791 - val_accuracy: 0.4023 - val_f1_metric: 0.3896\n",
      "\n",
      "Epoch 00007: val_f1_metric did not improve from 0.42003\n",
      "Epoch 8/50\n",
      "43/43 [==============================] - 3s 77ms/step - loss: 0.5598 - accuracy: 0.4781 - f1_metric: 0.4719 - val_loss: 0.9249 - val_accuracy: 0.4875 - val_f1_metric: 0.4857\n",
      "\n",
      "Epoch 00008: val_f1_metric improved from 0.42003 to 0.48573, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 9/50\n",
      "43/43 [==============================] - 3s 63ms/step - loss: 0.5442 - accuracy: 0.5031 - f1_metric: 0.4976 - val_loss: 1.0172 - val_accuracy: 0.4023 - val_f1_metric: 0.3996\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.48573\n",
      "Epoch 10/50\n",
      "43/43 [==============================] - 3s 65ms/step - loss: 0.5500 - accuracy: 0.4980 - f1_metric: 0.4932 - val_loss: 0.8425 - val_accuracy: 0.4537 - val_f1_metric: 0.4454\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.48573\n",
      "Epoch 11/50\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 0.5405 - accuracy: 0.4932 - f1_metric: 0.4906 - val_loss: 0.7752 - val_accuracy: 0.5286 - val_f1_metric: 0.5302\n",
      "\n",
      "Epoch 00011: val_f1_metric improved from 0.48573 to 0.53015, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 12/50\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.5370 - accuracy: 0.4969 - f1_metric: 0.4932 - val_loss: 0.7655 - val_accuracy: 0.5110 - val_f1_metric: 0.5060\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.53015\n",
      "Epoch 13/50\n",
      "43/43 [==============================] - 3s 70ms/step - loss: 0.5289 - accuracy: 0.5200 - f1_metric: 0.5177 - val_loss: 0.9159 - val_accuracy: 0.5051 - val_f1_metric: 0.5055\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.53015\n",
      "Epoch 14/50\n",
      "43/43 [==============================] - 3s 68ms/step - loss: 0.5253 - accuracy: 0.5186 - f1_metric: 0.5158 - val_loss: 0.7308 - val_accuracy: 0.5551 - val_f1_metric: 0.5543\n",
      "\n",
      "Epoch 00014: val_f1_metric improved from 0.53015 to 0.55426, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 15/50\n",
      "43/43 [==============================] - 3s 76ms/step - loss: 0.5265 - accuracy: 0.5075 - f1_metric: 0.5054 - val_loss: 0.8996 - val_accuracy: 0.4376 - val_f1_metric: 0.4395\n",
      "\n",
      "Epoch 00015: val_f1_metric did not improve from 0.55426\n",
      "Epoch 16/50\n",
      "43/43 [==============================] - 3s 81ms/step - loss: 0.5262 - accuracy: 0.5175 - f1_metric: 0.5154 - val_loss: 0.9502 - val_accuracy: 0.4214 - val_f1_metric: 0.4175\n",
      "\n",
      "Epoch 00016: val_f1_metric did not improve from 0.55426\n",
      "Epoch 17/50\n",
      "43/43 [==============================] - 3s 73ms/step - loss: 0.5311 - accuracy: 0.4998 - f1_metric: 0.4987 - val_loss: 0.8542 - val_accuracy: 0.4684 - val_f1_metric: 0.4665\n",
      "\n",
      "Epoch 00017: val_f1_metric did not improve from 0.55426\n",
      "Epoch 18/50\n",
      "43/43 [==============================] - 3s 59ms/step - loss: 0.5026 - accuracy: 0.5116 - f1_metric: 0.5126 - val_loss: 0.8508 - val_accuracy: 0.4963 - val_f1_metric: 0.4984\n",
      "\n",
      "Epoch 00018: val_f1_metric did not improve from 0.55426\n",
      "Epoch 19/50\n",
      "43/43 [==============================] - 3s 76ms/step - loss: 0.5022 - accuracy: 0.5171 - f1_metric: 0.5149 - val_loss: 0.8845 - val_accuracy: 0.4684 - val_f1_metric: 0.4702\n",
      "\n",
      "Epoch 00019: val_f1_metric did not improve from 0.55426\n",
      "Epoch 20/50\n",
      "43/43 [==============================] - 4s 82ms/step - loss: 0.4931 - accuracy: 0.5355 - f1_metric: 0.5332 - val_loss: 0.8672 - val_accuracy: 0.4905 - val_f1_metric: 0.4920\n",
      "\n",
      "Epoch 00020: val_f1_metric did not improve from 0.55426\n",
      "Epoch 21/50\n",
      "43/43 [==============================] - 3s 81ms/step - loss: 0.4904 - accuracy: 0.5516 - f1_metric: 0.5507 - val_loss: 0.7353 - val_accuracy: 0.5477 - val_f1_metric: 0.5476\n",
      "\n",
      "Epoch 00021: val_f1_metric did not improve from 0.55426\n",
      "Epoch 22/50\n",
      "43/43 [==============================] - 4s 81ms/step - loss: 0.4778 - accuracy: 0.5513 - f1_metric: 0.5491 - val_loss: 0.8630 - val_accuracy: 0.4743 - val_f1_metric: 0.4758\n",
      "\n",
      "Epoch 00022: val_f1_metric did not improve from 0.55426\n",
      "Epoch 23/50\n",
      "43/43 [==============================] - 3s 75ms/step - loss: 0.4789 - accuracy: 0.5586 - f1_metric: 0.5589 - val_loss: 0.8857 - val_accuracy: 0.4655 - val_f1_metric: 0.4662\n",
      "\n",
      "Epoch 00023: val_f1_metric did not improve from 0.55426\n",
      "Epoch 24/50\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.4734 - accuracy: 0.5307 - f1_metric: 0.5304 - val_loss: 0.7925 - val_accuracy: 0.5507 - val_f1_metric: 0.5502\n",
      "\n",
      "Epoch 00024: val_f1_metric did not improve from 0.55426\n",
      "Epoch 25/50\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.4701 - accuracy: 0.5630 - f1_metric: 0.5623 - val_loss: 0.9068 - val_accuracy: 0.4317 - val_f1_metric: 0.4312\n",
      "\n",
      "Epoch 00025: val_f1_metric did not improve from 0.55426\n",
      "Epoch 26/50\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 0.4827 - accuracy: 0.5314 - f1_metric: 0.5315 - val_loss: 0.7669 - val_accuracy: 0.5477 - val_f1_metric: 0.5466\n",
      "\n",
      "Epoch 00026: val_f1_metric did not improve from 0.55426\n",
      "Epoch 27/50\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.4654 - accuracy: 0.5521 - f1_metric: 0.55 - 2s 50ms/step - loss: 0.4692 - accuracy: 0.5520 - f1_metric: 0.5522 - val_loss: 0.8002 - val_accuracy: 0.5374 - val_f1_metric: 0.5374\n",
      "\n",
      "Epoch 00027: val_f1_metric did not improve from 0.55426\n",
      "Epoch 28/50\n",
      "43/43 [==============================] - 3s 64ms/step - loss: 0.4752 - accuracy: 0.5410 - f1_metric: 0.5414 - val_loss: 0.7648 - val_accuracy: 0.5051 - val_f1_metric: 0.5046\n",
      "\n",
      "Epoch 00028: val_f1_metric did not improve from 0.55426\n",
      "Epoch 29/50\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 0.4657 - accuracy: 0.5601 - f1_metric: 0.5614 - val_loss: 0.6413 - val_accuracy: 0.6094 - val_f1_metric: 0.6059\n",
      "\n",
      "Epoch 00029: val_f1_metric improved from 0.55426 to 0.60587, saving model to ./best_model_keras\n",
      "INFO:tensorflow:Assets written to: ./best_model_keras/assets\n",
      "Epoch 30/50\n",
      "43/43 [==============================] - 3s 74ms/step - loss: 0.4707 - accuracy: 0.5759 - f1_metric: 0.5747 - val_loss: 0.7920 - val_accuracy: 0.5213 - val_f1_metric: 0.5214\n",
      "\n",
      "Epoch 00030: val_f1_metric did not improve from 0.60587\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 3s 78ms/step - loss: 0.4480 - accuracy: 0.5748 - f1_metric: 0.5769 - val_loss: 0.7681 - val_accuracy: 0.5727 - val_f1_metric: 0.5707\n",
      "\n",
      "Epoch 00031: val_f1_metric did not improve from 0.60587\n",
      "Epoch 32/50\n",
      "43/43 [==============================] - 3s 71ms/step - loss: 0.4642 - accuracy: 0.5652 - f1_metric: 0.5654 - val_loss: 0.7773 - val_accuracy: 0.5286 - val_f1_metric: 0.5273\n",
      "\n",
      "Epoch 00032: val_f1_metric did not improve from 0.60587\n",
      "Epoch 33/50\n",
      "43/43 [==============================] - 3s 70ms/step - loss: 0.4486 - accuracy: 0.5877 - f1_metric: 0.5885 - val_loss: 0.7598 - val_accuracy: 0.5742 - val_f1_metric: 0.5713\n",
      "\n",
      "Epoch 00033: val_f1_metric did not improve from 0.60587\n",
      "Epoch 34/50\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 0.4529 - accuracy: 0.5741 - f1_metric: 0.5751 - val_loss: 0.6804 - val_accuracy: 0.5668 - val_f1_metric: 0.5650\n",
      "\n",
      "Epoch 00034: val_f1_metric did not improve from 0.60587\n",
      "Epoch 35/50\n",
      "43/43 [==============================] - 3s 71ms/step - loss: 0.4443 - accuracy: 0.5641 - f1_metric: 0.5662 - val_loss: 0.6924 - val_accuracy: 0.5609 - val_f1_metric: 0.5601\n",
      "\n",
      "Epoch 00035: val_f1_metric did not improve from 0.60587\n",
      "Epoch 36/50\n",
      " 2/43 [>.............................] - ETA: 2s - loss: 0.3990 - accuracy: 0.5938 - f1_metric: 0.5937"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<timed exec>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1182\u001B[0m                 _r=1):\n\u001B[1;32m   1183\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1184\u001B[0;31m               \u001B[0mtmp_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1185\u001B[0m               \u001B[0;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1186\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    883\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    884\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 885\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    886\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    887\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    915\u001B[0m       \u001B[0;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    916\u001B[0m       \u001B[0;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 917\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=not-callable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    918\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    919\u001B[0m       \u001B[0;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3037\u001B[0m       (graph_function,\n\u001B[1;32m   3038\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[0;32m-> 3039\u001B[0;31m     return graph_function._call_flat(\n\u001B[0m\u001B[1;32m   3040\u001B[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[1;32m   3041\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1961\u001B[0m         and executing_eagerly):\n\u001B[1;32m   1962\u001B[0m       \u001B[0;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1963\u001B[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0m\u001B[1;32m   1964\u001B[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[1;32m   1965\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    589\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0m_InterpolateFunctionError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    590\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcancellation_manager\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 591\u001B[0;31m           outputs = execute.execute(\n\u001B[0m\u001B[1;32m    592\u001B[0m               \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msignature\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    593\u001B[0m               \u001B[0mnum_outputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_outputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     57\u001B[0m   \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 59\u001B[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0m\u001B[1;32m     60\u001B[0m                                         inputs, attrs, num_outputs)\n\u001B[1;32m     61\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(x_train, y_train, epochs=params['epochs'], verbose=1,\n",
    "                            batch_size=64, shuffle=True,\n",
    "                            # validation_split=0.3,\n",
    "                            validation_data=(x_cv, y_cv),\n",
    "                            callbacks=[mcp, rlp, es]\n",
    "                            , sample_weight=sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x149c59ee0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 545, in __del__\n",
      "    gen_dataset_ops.delete_iterator(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1263, in delete_iterator\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-21-d02416954039>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfigure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhistory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'loss'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhistory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'val_loss'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhistory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'f1_metric'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['f1_metric'])\n",
    "plt.plot(history.history['val_f1_metric'])\n",
    "\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train_loss', 'val_loss', 'f1', 'val_f1'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "\n",
    "model = load_model(best_model_path)\n",
    "test_res = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"keras evaluate=\", test_res)\n",
    "pred = model.predict(x_test)\n",
    "pred_classes = np.argmax(pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "check_baseline(pred_classes, y_test_classes)\n",
    "conf_mat = confusion_matrix(y_test_classes, pred_classes)\n",
    "print(conf_mat)\n",
    "labels = [0,1,2]\n",
    "\n",
    "f1_weighted = f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='weighted', sample_weight=None)\n",
    "print(\"F1 score (weighted)\", f1_weighted)\n",
    "print(\"F1 score (macro)\", f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='macro', sample_weight=None))\n",
    "print(\"F1 score (micro)\", f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='micro', sample_weight=None))  # weighted and micro preferred in case of imbalance\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-s-kappa --> supports multiclass; ref: https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english\n",
    "print(\"cohen's Kappa\", cohen_kappa_score(y_test_classes, pred_classes))\n",
    "\n",
    "recall = []\n",
    "for i, row in enumerate(conf_mat):\n",
    "    recall.append(np.round(row[i]/np.sum(row), 2))\n",
    "    print(\"Recall of class {} = {}\".format(i, recall[i]))\n",
    "print(\"Recall avg\", sum(recall)/len(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda uninstall pydot\n",
    "# !conda uninstall pydotplus\n",
    "# !conda uninstall graphviz\n",
    "\n",
    "#!conda install pydot\n",
    "#!conda install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}