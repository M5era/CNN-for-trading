{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below line of code if you want to calculate features and save dataframe\n",
    "# this script prints the path at which dataframe with calculated features is saved.\n",
    "# train.py calls the DataGenerator class to \n",
    "\n",
    "# %run ./main.py WMT original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "",
    "_uuid": "",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unnamed: 0</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi_6</th>\n",
       "      <th>rsi_7</th>\n",
       "      <th>rsi_8</th>\n",
       "      <th>...</th>\n",
       "      <th>eom_18</th>\n",
       "      <th>eom_19</th>\n",
       "      <th>eom_20</th>\n",
       "      <th>eom_21</th>\n",
       "      <th>eom_22</th>\n",
       "      <th>eom_23</th>\n",
       "      <th>eom_24</th>\n",
       "      <th>eom_25</th>\n",
       "      <th>eom_26</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>1999-02-08</td>\n",
       "      <td>13.327940</td>\n",
       "      <td>13.454873</td>\n",
       "      <td>13.309807</td>\n",
       "      <td>13.454873</td>\n",
       "      <td>31100</td>\n",
       "      <td>76.190708</td>\n",
       "      <td>64.331059</td>\n",
       "      <td>58.823622</td>\n",
       "      <td>...</td>\n",
       "      <td>135.334375</td>\n",
       "      <td>135.334375</td>\n",
       "      <td>135.334375</td>\n",
       "      <td>135.334375</td>\n",
       "      <td>135.334375</td>\n",
       "      <td>135.334375</td>\n",
       "      <td>135.334375</td>\n",
       "      <td>135.334375</td>\n",
       "      <td>135.334375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>1999-02-09</td>\n",
       "      <td>13.473011</td>\n",
       "      <td>13.473011</td>\n",
       "      <td>13.201013</td>\n",
       "      <td>13.201013</td>\n",
       "      <td>64300</td>\n",
       "      <td>90.415574</td>\n",
       "      <td>77.880480</td>\n",
       "      <td>66.723212</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.174450</td>\n",
       "      <td>-19.174450</td>\n",
       "      <td>-19.174450</td>\n",
       "      <td>-19.174450</td>\n",
       "      <td>-19.174450</td>\n",
       "      <td>-19.174450</td>\n",
       "      <td>-19.174450</td>\n",
       "      <td>-19.174450</td>\n",
       "      <td>-19.174450</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>1999-02-10</td>\n",
       "      <td>13.255406</td>\n",
       "      <td>13.382339</td>\n",
       "      <td>13.164740</td>\n",
       "      <td>13.237273</td>\n",
       "      <td>69300</td>\n",
       "      <td>71.428397</td>\n",
       "      <td>73.333248</td>\n",
       "      <td>64.924132</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.930040</td>\n",
       "      <td>-19.930040</td>\n",
       "      <td>-19.930040</td>\n",
       "      <td>-19.930040</td>\n",
       "      <td>-19.930040</td>\n",
       "      <td>-19.930040</td>\n",
       "      <td>-19.930040</td>\n",
       "      <td>-19.930040</td>\n",
       "      <td>-19.930040</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>1999-02-11</td>\n",
       "      <td>13.182880</td>\n",
       "      <td>13.182880</td>\n",
       "      <td>12.929014</td>\n",
       "      <td>13.128480</td>\n",
       "      <td>216800</td>\n",
       "      <td>75.221443</td>\n",
       "      <td>69.231060</td>\n",
       "      <td>70.902750</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.479398</td>\n",
       "      <td>-25.479398</td>\n",
       "      <td>-25.479398</td>\n",
       "      <td>-25.479398</td>\n",
       "      <td>-25.479398</td>\n",
       "      <td>-25.479398</td>\n",
       "      <td>-25.479398</td>\n",
       "      <td>-25.479398</td>\n",
       "      <td>-25.479398</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>1999-02-12</td>\n",
       "      <td>12.983404</td>\n",
       "      <td>13.128470</td>\n",
       "      <td>12.856471</td>\n",
       "      <td>12.856471</td>\n",
       "      <td>242300</td>\n",
       "      <td>61.679370</td>\n",
       "      <td>63.882715</td>\n",
       "      <td>59.599109</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.125683</td>\n",
       "      <td>-7.125683</td>\n",
       "      <td>-7.125683</td>\n",
       "      <td>-7.125683</td>\n",
       "      <td>-7.125683</td>\n",
       "      <td>-7.125683</td>\n",
       "      <td>-7.125683</td>\n",
       "      <td>-7.125683</td>\n",
       "      <td>-7.125683</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>36</td>\n",
       "      <td>1999-02-16</td>\n",
       "      <td>12.956205</td>\n",
       "      <td>13.019672</td>\n",
       "      <td>12.638873</td>\n",
       "      <td>12.765806</td>\n",
       "      <td>36600</td>\n",
       "      <td>54.858716</td>\n",
       "      <td>62.045943</td>\n",
       "      <td>63.978263</td>\n",
       "      <td>...</td>\n",
       "      <td>-169.796291</td>\n",
       "      <td>-169.796291</td>\n",
       "      <td>-169.796291</td>\n",
       "      <td>-169.796291</td>\n",
       "      <td>-169.796291</td>\n",
       "      <td>-169.796291</td>\n",
       "      <td>-169.796291</td>\n",
       "      <td>-169.796291</td>\n",
       "      <td>-169.796291</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>37</td>\n",
       "      <td>1999-02-17</td>\n",
       "      <td>12.693273</td>\n",
       "      <td>12.820206</td>\n",
       "      <td>12.620740</td>\n",
       "      <td>12.638873</td>\n",
       "      <td>41700</td>\n",
       "      <td>38.718484</td>\n",
       "      <td>43.523222</td>\n",
       "      <td>51.047172</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.042783</td>\n",
       "      <td>-52.042783</td>\n",
       "      <td>-52.042783</td>\n",
       "      <td>-52.042783</td>\n",
       "      <td>-52.042783</td>\n",
       "      <td>-52.042783</td>\n",
       "      <td>-52.042783</td>\n",
       "      <td>-52.042783</td>\n",
       "      <td>-52.042783</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>38</td>\n",
       "      <td>1999-02-18</td>\n",
       "      <td>12.620747</td>\n",
       "      <td>12.747680</td>\n",
       "      <td>12.493814</td>\n",
       "      <td>12.675147</td>\n",
       "      <td>83000</td>\n",
       "      <td>14.337356</td>\n",
       "      <td>37.826525</td>\n",
       "      <td>42.387705</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.502236</td>\n",
       "      <td>-30.502236</td>\n",
       "      <td>-30.502236</td>\n",
       "      <td>-30.502236</td>\n",
       "      <td>-30.502236</td>\n",
       "      <td>-30.502236</td>\n",
       "      <td>-30.502236</td>\n",
       "      <td>-30.502236</td>\n",
       "      <td>-30.502236</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>39</td>\n",
       "      <td>1999-02-19</td>\n",
       "      <td>12.729545</td>\n",
       "      <td>12.747679</td>\n",
       "      <td>12.493813</td>\n",
       "      <td>12.602612</td>\n",
       "      <td>130300</td>\n",
       "      <td>13.283954</td>\n",
       "      <td>24.193748</td>\n",
       "      <td>43.276130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40</td>\n",
       "      <td>1999-02-22</td>\n",
       "      <td>12.693278</td>\n",
       "      <td>12.765811</td>\n",
       "      <td>12.584479</td>\n",
       "      <td>12.729545</td>\n",
       "      <td>79500</td>\n",
       "      <td>13.824787</td>\n",
       "      <td>10.843216</td>\n",
       "      <td>22.122103</td>\n",
       "      <td>...</td>\n",
       "      <td>12.407965</td>\n",
       "      <td>12.407965</td>\n",
       "      <td>12.407965</td>\n",
       "      <td>12.407965</td>\n",
       "      <td>12.407965</td>\n",
       "      <td>12.407965</td>\n",
       "      <td>12.407965</td>\n",
       "      <td>12.407965</td>\n",
       "      <td>12.407965</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>41</td>\n",
       "      <td>1999-02-23</td>\n",
       "      <td>12.765813</td>\n",
       "      <td>12.820212</td>\n",
       "      <td>12.620747</td>\n",
       "      <td>12.620747</td>\n",
       "      <td>309600</td>\n",
       "      <td>24.657494</td>\n",
       "      <td>22.377750</td>\n",
       "      <td>17.745788</td>\n",
       "      <td>...</td>\n",
       "      <td>2.920773</td>\n",
       "      <td>2.920773</td>\n",
       "      <td>2.920773</td>\n",
       "      <td>2.920773</td>\n",
       "      <td>2.920773</td>\n",
       "      <td>2.920773</td>\n",
       "      <td>2.920773</td>\n",
       "      <td>2.920773</td>\n",
       "      <td>2.920773</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>42</td>\n",
       "      <td>1999-02-24</td>\n",
       "      <td>12.657005</td>\n",
       "      <td>12.820204</td>\n",
       "      <td>12.566339</td>\n",
       "      <td>12.566339</td>\n",
       "      <td>34500</td>\n",
       "      <td>25.510128</td>\n",
       "      <td>19.999735</td>\n",
       "      <td>18.567534</td>\n",
       "      <td>...</td>\n",
       "      <td>-20.020984</td>\n",
       "      <td>-20.020984</td>\n",
       "      <td>-20.020984</td>\n",
       "      <td>-20.020984</td>\n",
       "      <td>-20.020984</td>\n",
       "      <td>-20.020984</td>\n",
       "      <td>-20.020984</td>\n",
       "      <td>-20.020984</td>\n",
       "      <td>-20.020984</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>43</td>\n",
       "      <td>1999-02-25</td>\n",
       "      <td>12.557281</td>\n",
       "      <td>12.711414</td>\n",
       "      <td>12.548214</td>\n",
       "      <td>12.629814</td>\n",
       "      <td>44600</td>\n",
       "      <td>23.202131</td>\n",
       "      <td>22.514360</td>\n",
       "      <td>18.087854</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.220081</td>\n",
       "      <td>-23.220081</td>\n",
       "      <td>-23.220081</td>\n",
       "      <td>-23.220081</td>\n",
       "      <td>-23.220081</td>\n",
       "      <td>-23.220081</td>\n",
       "      <td>-23.220081</td>\n",
       "      <td>-23.220081</td>\n",
       "      <td>-23.220081</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>44</td>\n",
       "      <td>1999-02-26</td>\n",
       "      <td>12.675143</td>\n",
       "      <td>12.765809</td>\n",
       "      <td>12.548210</td>\n",
       "      <td>12.548210</td>\n",
       "      <td>24900</td>\n",
       "      <td>50.425129</td>\n",
       "      <td>35.462112</td>\n",
       "      <td>34.221570</td>\n",
       "      <td>...</td>\n",
       "      <td>23.765926</td>\n",
       "      <td>23.765926</td>\n",
       "      <td>23.765926</td>\n",
       "      <td>23.765926</td>\n",
       "      <td>23.765926</td>\n",
       "      <td>23.765926</td>\n",
       "      <td>23.765926</td>\n",
       "      <td>23.765926</td>\n",
       "      <td>23.765926</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>45</td>\n",
       "      <td>1999-03-01</td>\n",
       "      <td>12.575414</td>\n",
       "      <td>12.584480</td>\n",
       "      <td>12.385014</td>\n",
       "      <td>12.421281</td>\n",
       "      <td>13300</td>\n",
       "      <td>45.081909</td>\n",
       "      <td>40.825047</td>\n",
       "      <td>30.117380</td>\n",
       "      <td>...</td>\n",
       "      <td>-258.349327</td>\n",
       "      <td>-258.349327</td>\n",
       "      <td>-258.349327</td>\n",
       "      <td>-258.349327</td>\n",
       "      <td>-258.349327</td>\n",
       "      <td>-258.349327</td>\n",
       "      <td>-258.349327</td>\n",
       "      <td>-258.349327</td>\n",
       "      <td>-258.349327</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>46</td>\n",
       "      <td>1999-03-02</td>\n",
       "      <td>12.439406</td>\n",
       "      <td>12.493806</td>\n",
       "      <td>12.239940</td>\n",
       "      <td>12.294340</td>\n",
       "      <td>55100</td>\n",
       "      <td>27.272201</td>\n",
       "      <td>37.287533</td>\n",
       "      <td>34.323674</td>\n",
       "      <td>...</td>\n",
       "      <td>-54.308840</td>\n",
       "      <td>-54.308840</td>\n",
       "      <td>-54.308840</td>\n",
       "      <td>-54.308840</td>\n",
       "      <td>-54.308840</td>\n",
       "      <td>-54.308840</td>\n",
       "      <td>-54.308840</td>\n",
       "      <td>-54.308840</td>\n",
       "      <td>-54.308840</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>47</td>\n",
       "      <td>1999-03-03</td>\n",
       "      <td>12.403139</td>\n",
       "      <td>12.620738</td>\n",
       "      <td>12.312473</td>\n",
       "      <td>12.566339</td>\n",
       "      <td>103300</td>\n",
       "      <td>28.074402</td>\n",
       "      <td>26.694452</td>\n",
       "      <td>36.434791</td>\n",
       "      <td>...</td>\n",
       "      <td>29.761936</td>\n",
       "      <td>29.761936</td>\n",
       "      <td>29.761936</td>\n",
       "      <td>29.761936</td>\n",
       "      <td>29.761936</td>\n",
       "      <td>29.761936</td>\n",
       "      <td>29.761936</td>\n",
       "      <td>29.761936</td>\n",
       "      <td>29.761936</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>48</td>\n",
       "      <td>1999-03-04</td>\n",
       "      <td>12.675147</td>\n",
       "      <td>13.092212</td>\n",
       "      <td>12.675147</td>\n",
       "      <td>13.019679</td>\n",
       "      <td>90400</td>\n",
       "      <td>48.039377</td>\n",
       "      <td>51.376323</td>\n",
       "      <td>49.236795</td>\n",
       "      <td>...</td>\n",
       "      <td>192.419153</td>\n",
       "      <td>192.419153</td>\n",
       "      <td>192.419153</td>\n",
       "      <td>192.419153</td>\n",
       "      <td>192.419153</td>\n",
       "      <td>192.419153</td>\n",
       "      <td>192.419153</td>\n",
       "      <td>192.419153</td>\n",
       "      <td>192.419153</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>49</td>\n",
       "      <td>1999-03-05</td>\n",
       "      <td>13.346074</td>\n",
       "      <td>13.364207</td>\n",
       "      <td>13.128475</td>\n",
       "      <td>13.264474</td>\n",
       "      <td>92700</td>\n",
       "      <td>76.273364</td>\n",
       "      <td>70.932191</td>\n",
       "      <td>71.893802</td>\n",
       "      <td>...</td>\n",
       "      <td>92.223204</td>\n",
       "      <td>92.223204</td>\n",
       "      <td>92.223204</td>\n",
       "      <td>92.223204</td>\n",
       "      <td>92.223204</td>\n",
       "      <td>92.223204</td>\n",
       "      <td>92.223204</td>\n",
       "      <td>92.223204</td>\n",
       "      <td>92.223204</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>50</td>\n",
       "      <td>1999-03-08</td>\n",
       "      <td>13.337007</td>\n",
       "      <td>13.491140</td>\n",
       "      <td>13.282607</td>\n",
       "      <td>13.454873</td>\n",
       "      <td>107600</td>\n",
       "      <td>78.947101</td>\n",
       "      <td>73.508095</td>\n",
       "      <td>68.363783</td>\n",
       "      <td>...</td>\n",
       "      <td>27.235713</td>\n",
       "      <td>27.235713</td>\n",
       "      <td>27.235713</td>\n",
       "      <td>27.235713</td>\n",
       "      <td>27.235713</td>\n",
       "      <td>27.235713</td>\n",
       "      <td>27.235713</td>\n",
       "      <td>27.235713</td>\n",
       "      <td>27.235713</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>51</td>\n",
       "      <td>1999-03-09</td>\n",
       "      <td>13.309810</td>\n",
       "      <td>13.527409</td>\n",
       "      <td>13.255410</td>\n",
       "      <td>13.309810</td>\n",
       "      <td>79000</td>\n",
       "      <td>75.036079</td>\n",
       "      <td>77.227639</td>\n",
       "      <td>72.032042</td>\n",
       "      <td>...</td>\n",
       "      <td>1.561737</td>\n",
       "      <td>1.561737</td>\n",
       "      <td>1.561737</td>\n",
       "      <td>1.561737</td>\n",
       "      <td>1.561737</td>\n",
       "      <td>1.561737</td>\n",
       "      <td>1.561737</td>\n",
       "      <td>1.561737</td>\n",
       "      <td>1.561737</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>52</td>\n",
       "      <td>1999-03-10</td>\n",
       "      <td>13.364203</td>\n",
       "      <td>14.107666</td>\n",
       "      <td>13.364203</td>\n",
       "      <td>14.071400</td>\n",
       "      <td>1733800</td>\n",
       "      <td>82.860979</td>\n",
       "      <td>76.551557</td>\n",
       "      <td>78.461322</td>\n",
       "      <td>...</td>\n",
       "      <td>14.773437</td>\n",
       "      <td>14.773437</td>\n",
       "      <td>14.773437</td>\n",
       "      <td>14.773437</td>\n",
       "      <td>14.773437</td>\n",
       "      <td>14.773437</td>\n",
       "      <td>14.773437</td>\n",
       "      <td>14.773437</td>\n",
       "      <td>14.773437</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>53</td>\n",
       "      <td>1999-03-11</td>\n",
       "      <td>14.506605</td>\n",
       "      <td>14.651671</td>\n",
       "      <td>14.071407</td>\n",
       "      <td>14.270873</td>\n",
       "      <td>909000</td>\n",
       "      <td>97.028336</td>\n",
       "      <td>91.785533</td>\n",
       "      <td>88.194405</td>\n",
       "      <td>...</td>\n",
       "      <td>39.935758</td>\n",
       "      <td>39.935758</td>\n",
       "      <td>39.935758</td>\n",
       "      <td>39.935758</td>\n",
       "      <td>39.935758</td>\n",
       "      <td>39.935758</td>\n",
       "      <td>39.935758</td>\n",
       "      <td>39.935758</td>\n",
       "      <td>39.935758</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>54</td>\n",
       "      <td>1999-03-12</td>\n",
       "      <td>14.434066</td>\n",
       "      <td>14.542865</td>\n",
       "      <td>14.071401</td>\n",
       "      <td>14.216467</td>\n",
       "      <td>604300</td>\n",
       "      <td>94.551081</td>\n",
       "      <td>93.157702</td>\n",
       "      <td>88.012342</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.244679</td>\n",
       "      <td>-4.244679</td>\n",
       "      <td>-4.244679</td>\n",
       "      <td>-4.244679</td>\n",
       "      <td>-4.244679</td>\n",
       "      <td>-4.244679</td>\n",
       "      <td>-4.244679</td>\n",
       "      <td>-4.244679</td>\n",
       "      <td>-4.244679</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>55</td>\n",
       "      <td>1999-03-15</td>\n",
       "      <td>14.252735</td>\n",
       "      <td>14.289001</td>\n",
       "      <td>13.935403</td>\n",
       "      <td>14.107669</td>\n",
       "      <td>523000</td>\n",
       "      <td>85.123868</td>\n",
       "      <td>86.977845</td>\n",
       "      <td>85.862738</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.179211</td>\n",
       "      <td>-13.179211</td>\n",
       "      <td>-13.179211</td>\n",
       "      <td>-13.179211</td>\n",
       "      <td>-13.179211</td>\n",
       "      <td>-13.179211</td>\n",
       "      <td>-13.179211</td>\n",
       "      <td>-13.179211</td>\n",
       "      <td>-13.179211</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>56</td>\n",
       "      <td>1999-03-16</td>\n",
       "      <td>13.980746</td>\n",
       "      <td>14.107679</td>\n",
       "      <td>13.926346</td>\n",
       "      <td>14.035146</td>\n",
       "      <td>273300</td>\n",
       "      <td>66.000366</td>\n",
       "      <td>75.458126</td>\n",
       "      <td>78.072137</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.315769</td>\n",
       "      <td>-6.315769</td>\n",
       "      <td>-6.315769</td>\n",
       "      <td>-6.315769</td>\n",
       "      <td>-6.315769</td>\n",
       "      <td>-6.315769</td>\n",
       "      <td>-6.315769</td>\n",
       "      <td>-6.315769</td>\n",
       "      <td>-6.315769</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>57</td>\n",
       "      <td>1999-03-17</td>\n",
       "      <td>14.107673</td>\n",
       "      <td>14.415938</td>\n",
       "      <td>14.071407</td>\n",
       "      <td>14.397805</td>\n",
       "      <td>362800</td>\n",
       "      <td>70.924883</td>\n",
       "      <td>70.523173</td>\n",
       "      <td>78.169163</td>\n",
       "      <td>...</td>\n",
       "      <td>21.524699</td>\n",
       "      <td>21.524699</td>\n",
       "      <td>21.524699</td>\n",
       "      <td>21.524699</td>\n",
       "      <td>21.524699</td>\n",
       "      <td>21.524699</td>\n",
       "      <td>21.524699</td>\n",
       "      <td>21.524699</td>\n",
       "      <td>21.524699</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>58</td>\n",
       "      <td>1999-03-18</td>\n",
       "      <td>14.651666</td>\n",
       "      <td>14.687933</td>\n",
       "      <td>14.470334</td>\n",
       "      <td>14.579133</td>\n",
       "      <td>332600</td>\n",
       "      <td>78.985576</td>\n",
       "      <td>77.978485</td>\n",
       "      <td>77.582789</td>\n",
       "      <td>...</td>\n",
       "      <td>21.947044</td>\n",
       "      <td>21.947044</td>\n",
       "      <td>21.947044</td>\n",
       "      <td>21.947044</td>\n",
       "      <td>21.947044</td>\n",
       "      <td>21.947044</td>\n",
       "      <td>21.947044</td>\n",
       "      <td>21.947044</td>\n",
       "      <td>21.947044</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>59</td>\n",
       "      <td>1999-03-19</td>\n",
       "      <td>14.704340</td>\n",
       "      <td>14.722538</td>\n",
       "      <td>14.340371</td>\n",
       "      <td>14.431363</td>\n",
       "      <td>260400</td>\n",
       "      <td>78.110990</td>\n",
       "      <td>78.580307</td>\n",
       "      <td>77.708054</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.997375</td>\n",
       "      <td>-6.997375</td>\n",
       "      <td>-6.997375</td>\n",
       "      <td>-6.997375</td>\n",
       "      <td>-6.997375</td>\n",
       "      <td>-6.997375</td>\n",
       "      <td>-6.997375</td>\n",
       "      <td>-6.997375</td>\n",
       "      <td>-6.997375</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>60</td>\n",
       "      <td>1999-03-22</td>\n",
       "      <td>14.558749</td>\n",
       "      <td>14.631543</td>\n",
       "      <td>14.404062</td>\n",
       "      <td>14.522352</td>\n",
       "      <td>165000</td>\n",
       "      <td>50.808390</td>\n",
       "      <td>72.841781</td>\n",
       "      <td>73.503843</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.882148</td>\n",
       "      <td>-1.882148</td>\n",
       "      <td>-1.882148</td>\n",
       "      <td>-1.882148</td>\n",
       "      <td>-1.882148</td>\n",
       "      <td>-1.882148</td>\n",
       "      <td>-1.882148</td>\n",
       "      <td>-1.882148</td>\n",
       "      <td>-1.882148</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows Ã— 428 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unnamed: 0   timestamp       open       high        low      close  \\\n",
       "0           31  1999-02-08  13.327940  13.454873  13.309807  13.454873   \n",
       "1           32  1999-02-09  13.473011  13.473011  13.201013  13.201013   \n",
       "2           33  1999-02-10  13.255406  13.382339  13.164740  13.237273   \n",
       "3           34  1999-02-11  13.182880  13.182880  12.929014  13.128480   \n",
       "4           35  1999-02-12  12.983404  13.128470  12.856471  12.856471   \n",
       "5           36  1999-02-16  12.956205  13.019672  12.638873  12.765806   \n",
       "6           37  1999-02-17  12.693273  12.820206  12.620740  12.638873   \n",
       "7           38  1999-02-18  12.620747  12.747680  12.493814  12.675147   \n",
       "8           39  1999-02-19  12.729545  12.747679  12.493813  12.602612   \n",
       "9           40  1999-02-22  12.693278  12.765811  12.584479  12.729545   \n",
       "10          41  1999-02-23  12.765813  12.820212  12.620747  12.620747   \n",
       "11          42  1999-02-24  12.657005  12.820204  12.566339  12.566339   \n",
       "12          43  1999-02-25  12.557281  12.711414  12.548214  12.629814   \n",
       "13          44  1999-02-26  12.675143  12.765809  12.548210  12.548210   \n",
       "14          45  1999-03-01  12.575414  12.584480  12.385014  12.421281   \n",
       "15          46  1999-03-02  12.439406  12.493806  12.239940  12.294340   \n",
       "16          47  1999-03-03  12.403139  12.620738  12.312473  12.566339   \n",
       "17          48  1999-03-04  12.675147  13.092212  12.675147  13.019679   \n",
       "18          49  1999-03-05  13.346074  13.364207  13.128475  13.264474   \n",
       "19          50  1999-03-08  13.337007  13.491140  13.282607  13.454873   \n",
       "20          51  1999-03-09  13.309810  13.527409  13.255410  13.309810   \n",
       "21          52  1999-03-10  13.364203  14.107666  13.364203  14.071400   \n",
       "22          53  1999-03-11  14.506605  14.651671  14.071407  14.270873   \n",
       "23          54  1999-03-12  14.434066  14.542865  14.071401  14.216467   \n",
       "24          55  1999-03-15  14.252735  14.289001  13.935403  14.107669   \n",
       "25          56  1999-03-16  13.980746  14.107679  13.926346  14.035146   \n",
       "26          57  1999-03-17  14.107673  14.415938  14.071407  14.397805   \n",
       "27          58  1999-03-18  14.651666  14.687933  14.470334  14.579133   \n",
       "28          59  1999-03-19  14.704340  14.722538  14.340371  14.431363   \n",
       "29          60  1999-03-22  14.558749  14.631543  14.404062  14.522352   \n",
       "\n",
       "     volume      rsi_6      rsi_7      rsi_8  ...      eom_18      eom_19  \\\n",
       "0     31100  76.190708  64.331059  58.823622  ...  135.334375  135.334375   \n",
       "1     64300  90.415574  77.880480  66.723212  ...  -19.174450  -19.174450   \n",
       "2     69300  71.428397  73.333248  64.924132  ...  -19.930040  -19.930040   \n",
       "3    216800  75.221443  69.231060  70.902750  ...  -25.479398  -25.479398   \n",
       "4    242300  61.679370  63.882715  59.599109  ...   -7.125683   -7.125683   \n",
       "5     36600  54.858716  62.045943  63.978263  ... -169.796291 -169.796291   \n",
       "6     41700  38.718484  43.523222  51.047172  ...  -52.042783  -52.042783   \n",
       "7     83000  14.337356  37.826525  42.387705  ...  -30.502236  -30.502236   \n",
       "8    130300  13.283954  24.193748  43.276130  ...   -0.000299   -0.000299   \n",
       "9     79500  13.824787  10.843216  22.122103  ...   12.407965   12.407965   \n",
       "10   309600  24.657494  22.377750  17.745788  ...    2.920773    2.920773   \n",
       "11    34500  25.510128  19.999735  18.567534  ...  -20.020984  -20.020984   \n",
       "12    44600  23.202131  22.514360  18.087854  ...  -23.220081  -23.220081   \n",
       "13    24900  50.425129  35.462112  34.221570  ...   23.765926   23.765926   \n",
       "14    13300  45.081909  40.825047  30.117380  ... -258.349327 -258.349327   \n",
       "15    55100  27.272201  37.287533  34.323674  ...  -54.308840  -54.308840   \n",
       "16   103300  28.074402  26.694452  36.434791  ...   29.761936   29.761936   \n",
       "17    90400  48.039377  51.376323  49.236795  ...  192.419153  192.419153   \n",
       "18    92700  76.273364  70.932191  71.893802  ...   92.223204   92.223204   \n",
       "19   107600  78.947101  73.508095  68.363783  ...   27.235713   27.235713   \n",
       "20    79000  75.036079  77.227639  72.032042  ...    1.561737    1.561737   \n",
       "21  1733800  82.860979  76.551557  78.461322  ...   14.773437   14.773437   \n",
       "22   909000  97.028336  91.785533  88.194405  ...   39.935758   39.935758   \n",
       "23   604300  94.551081  93.157702  88.012342  ...   -4.244679   -4.244679   \n",
       "24   523000  85.123868  86.977845  85.862738  ...  -13.179211  -13.179211   \n",
       "25   273300  66.000366  75.458126  78.072137  ...   -6.315769   -6.315769   \n",
       "26   362800  70.924883  70.523173  78.169163  ...   21.524699   21.524699   \n",
       "27   332600  78.985576  77.978485  77.582789  ...   21.947044   21.947044   \n",
       "28   260400  78.110990  78.580307  77.708054  ...   -6.997375   -6.997375   \n",
       "29   165000  50.808390  72.841781  73.503843  ...   -1.882148   -1.882148   \n",
       "\n",
       "        eom_20      eom_21      eom_22      eom_23      eom_24      eom_25  \\\n",
       "0   135.334375  135.334375  135.334375  135.334375  135.334375  135.334375   \n",
       "1   -19.174450  -19.174450  -19.174450  -19.174450  -19.174450  -19.174450   \n",
       "2   -19.930040  -19.930040  -19.930040  -19.930040  -19.930040  -19.930040   \n",
       "3   -25.479398  -25.479398  -25.479398  -25.479398  -25.479398  -25.479398   \n",
       "4    -7.125683   -7.125683   -7.125683   -7.125683   -7.125683   -7.125683   \n",
       "5  -169.796291 -169.796291 -169.796291 -169.796291 -169.796291 -169.796291   \n",
       "6   -52.042783  -52.042783  -52.042783  -52.042783  -52.042783  -52.042783   \n",
       "7   -30.502236  -30.502236  -30.502236  -30.502236  -30.502236  -30.502236   \n",
       "8    -0.000299   -0.000299   -0.000299   -0.000299   -0.000299   -0.000299   \n",
       "9    12.407965   12.407965   12.407965   12.407965   12.407965   12.407965   \n",
       "10    2.920773    2.920773    2.920773    2.920773    2.920773    2.920773   \n",
       "11  -20.020984  -20.020984  -20.020984  -20.020984  -20.020984  -20.020984   \n",
       "12  -23.220081  -23.220081  -23.220081  -23.220081  -23.220081  -23.220081   \n",
       "13   23.765926   23.765926   23.765926   23.765926   23.765926   23.765926   \n",
       "14 -258.349327 -258.349327 -258.349327 -258.349327 -258.349327 -258.349327   \n",
       "15  -54.308840  -54.308840  -54.308840  -54.308840  -54.308840  -54.308840   \n",
       "16   29.761936   29.761936   29.761936   29.761936   29.761936   29.761936   \n",
       "17  192.419153  192.419153  192.419153  192.419153  192.419153  192.419153   \n",
       "18   92.223204   92.223204   92.223204   92.223204   92.223204   92.223204   \n",
       "19   27.235713   27.235713   27.235713   27.235713   27.235713   27.235713   \n",
       "20    1.561737    1.561737    1.561737    1.561737    1.561737    1.561737   \n",
       "21   14.773437   14.773437   14.773437   14.773437   14.773437   14.773437   \n",
       "22   39.935758   39.935758   39.935758   39.935758   39.935758   39.935758   \n",
       "23   -4.244679   -4.244679   -4.244679   -4.244679   -4.244679   -4.244679   \n",
       "24  -13.179211  -13.179211  -13.179211  -13.179211  -13.179211  -13.179211   \n",
       "25   -6.315769   -6.315769   -6.315769   -6.315769   -6.315769   -6.315769   \n",
       "26   21.524699   21.524699   21.524699   21.524699   21.524699   21.524699   \n",
       "27   21.947044   21.947044   21.947044   21.947044   21.947044   21.947044   \n",
       "28   -6.997375   -6.997375   -6.997375   -6.997375   -6.997375   -6.997375   \n",
       "29   -1.882148   -1.882148   -1.882148   -1.882148   -1.882148   -1.882148   \n",
       "\n",
       "        eom_26  labels  \n",
       "0   135.334375       0  \n",
       "1   -19.174450       2  \n",
       "2   -19.930040       2  \n",
       "3   -25.479398       2  \n",
       "4    -7.125683       2  \n",
       "5  -169.796291       2  \n",
       "6   -52.042783       2  \n",
       "7   -30.502236       2  \n",
       "8    -0.000299       2  \n",
       "9    12.407965       2  \n",
       "10    2.920773       2  \n",
       "11  -20.020984       2  \n",
       "12  -23.220081       2  \n",
       "13   23.765926       2  \n",
       "14 -258.349327       2  \n",
       "15  -54.308840       1  \n",
       "16   29.761936       2  \n",
       "17  192.419153       2  \n",
       "18   92.223204       2  \n",
       "19   27.235713       2  \n",
       "20    1.561737       2  \n",
       "21   14.773437       2  \n",
       "22   39.935758       2  \n",
       "23   -4.244679       2  \n",
       "24  -13.179211       2  \n",
       "25   -6.315769       2  \n",
       "26   21.524699       2  \n",
       "27   21.947044       0  \n",
       "28   -6.997375       2  \n",
       "29   -1.882148       2  \n",
       "\n",
       "[30 rows x 428 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "\n",
    "np.random.seed(2)\n",
    "symbol = 'XLE'\n",
    "# use the path printed in above output cell after running stock_cnn.py. It's in below format\n",
    "df = pd.read_csv(\"data/df_\"+symbol+\".csv\")\n",
    "df = df[df['timestamp'] < '2016-01-01']  # For backtesting the last 5 years will be excluded from training.\n",
    "df['labels'] = df['labels'].astype(np.int8)\n",
    "if 'dividend_amount' in df.columns:\n",
    "    df.drop(columns=['dividend_amount', 'split_coefficient'], inplace=True)\n",
    "display(df.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into Training, Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features 425\n",
      "train_split = 0.8\n",
      "Shape of x, y train/cv/test (2721, 425) (2721,) (681, 425) (681,) (851, 425) (851,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "list_features = list(df.loc[:, 'open':'eom_26'].columns)\n",
    "print('Total number of features', len(list_features))\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.loc[:, 'open':'eom_26'].values, df['labels'].values, train_size=0.8, \n",
    "                                                    test_size=0.2, random_state=2, shuffle=True, stratify=df['labels'].values)\n",
    "\n",
    "# smote = RandomOverSampler(random_state=42, sampling_strategy='not majority')\n",
    "# x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "# print('Resampled dataset shape %s' % Counter(y_train))\n",
    "\n",
    "if 0.7*x_train.shape[0] < 2500:\n",
    "    train_split = 0.8\n",
    "else:\n",
    "    train_split = 0.7\n",
    "# train_split = 0.7\n",
    "print('train_split =',train_split)\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, train_size=train_split, test_size=1-train_split, \n",
    "                                                random_state=2, shuffle=True, stratify=y_train)\n",
    "mm_scaler = MinMaxScaler(feature_range=(0, 1)) # or StandardScaler?\n",
    "x_train = mm_scaler.fit_transform(x_train)\n",
    "x_cv = mm_scaler.transform(x_cv)\n",
    "x_test = mm_scaler.transform(x_test)\n",
    "\n",
    "x_main = x_train.copy()\n",
    "print(\"Shape of x, y train/cv/test {} {} {} {} {} {}\".format(x_train.shape, y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of total 441+ features select top 'N' features (let's include base features like close, adjusted_close etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 225  # should be a perfect square\n",
    "selection_method = 'all'\n",
    "topk = 320 if selection_method == 'all' else num_features\n",
    "# if train_split >= 0.8:\n",
    "#     topk = 400\n",
    "# else:\n",
    "#     topk = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('open', 'high', 'low', 'close', 'volume', 'rsi_6', 'rsi_7', 'rsi_8', 'rsi_9', 'rsi_10', 'rsi_11', 'rsi_12', 'rsi_13', 'rsi_14', 'rsi_15', 'rsi_16', 'rsi_17', 'rsi_18', 'rsi_19', 'rsi_20', 'rsi_21', 'rsi_22', 'rsi_23', 'rsi_24', 'rsi_25', 'rsi_26', 'wr_6', 'wr_7', 'wr_8', 'wr_9', 'wr_10', 'wr_11', 'wr_12', 'wr_13', 'wr_14', 'wr_15', 'wr_16', 'wr_17', 'wr_18', 'wr_19', 'wr_20', 'wr_21', 'wr_22', 'wr_23', 'wr_24', 'wr_25', 'wr_26', 'mfi_6', 'mfi_7', 'mfi_8', 'mfi_9', 'mfi_10', 'mfi_11', 'mfi_12', 'mfi_13', 'mfi_14', 'mfi_15', 'mfi_16', 'mfi_17', 'mfi_18', 'mfi_19', 'mfi_20', 'mfi_21', 'mfi_22', 'mfi_23', 'mfi_24', 'mfi_25', 'mfi_26', 'roc_6', 'roc_7', 'roc_8', 'roc_9', 'roc_10', 'roc_11', 'roc_12', 'roc_13', 'roc_14', 'roc_15', 'roc_16', 'roc_17', 'roc_18', 'roc_19', 'roc_20', 'roc_21', 'roc_22', 'roc_23', 'roc_24', 'roc_25', 'roc_26', 'cmf_6', 'cmf_7', 'cmf_8', 'cmf_9', 'cmf_10', 'cmf_11', 'cmf_12', 'cmf_13', 'cmf_14', 'cmf_15', 'cmf_16', 'cmf_17', 'cmf_18', 'cmf_19', 'cmf_20', 'cmf_21', 'cmf_22', 'cmf_23', 'cmf_24', 'cmf_25', 'cmf_26', 'cmo_6', 'cmo_7', 'cmo_8', 'cmo_9', 'cmo_10', 'cmo_11', 'cmo_12', 'cmo_13', 'cmo_14', 'cmo_15', 'cmo_16', 'cmo_17', 'cmo_18', 'cmo_19', 'cmo_20', 'cmo_21', 'cmo_22', 'cmo_23', 'cmo_24', 'cmo_25', 'cmo_26', 'trix_6', 'trix_7', 'trix_8', 'trix_9', 'trix_10', 'trix_11', 'trix_12', 'trix_13', 'trix_14', 'trix_15', 'trix_16', 'trix_17', 'trix_18', 'trix_19', 'trix_20', 'trix_21', 'trix_22', 'trix_23', 'trix_24', 'trix_25', 'trix_26', 'cci_6', 'cci_7', 'cci_8', 'cci_9', 'cci_10', 'cci_11', 'cci_12', 'cci_13', 'cci_14', 'cci_15', 'cci_16', 'cci_17', 'cci_18', 'cci_19', 'cci_20', 'cci_21', 'cci_22', 'cci_23', 'cci_24', 'cci_25', 'cci_26', 'dpo_6', 'dpo_7', 'dpo_8', 'dpo_9', 'dpo_10', 'dpo_11', 'dpo_12', 'dpo_13', 'dpo_14', 'dpo_15', 'dpo_16', 'dpo_17', 'dpo_18', 'dpo_19', 'dpo_20', 'dpo_21', 'dpo_22', 'dpo_23', 'dpo_24', 'dpo_25', 'dpo_26', 'kst_6', 'kst_7', 'kst_8', 'kst_9', 'kst_10', 'kst_11', 'kst_12', 'kst_13', 'kst_14', 'kst_15', 'kst_16', 'kst_17', 'kst_18', 'kst_19', 'kst_20', 'kst_21', 'kst_22', 'kst_23', 'kst_24', 'kst_25', 'kst_26', 'dmi_6', 'dmi_7', 'dmi_8', 'dmi_9', 'dmi_10', 'dmi_11', 'dmi_12', 'dmi_13', 'dmi_14', 'dmi_15', 'dmi_16', 'dmi_17', 'dmi_18', 'dmi_19', 'dmi_20', 'dmi_21', 'dmi_22', 'dmi_23', 'dmi_24', 'dmi_25', 'dmi_26', 'fi_6', 'fi_7', 'fi_8', 'fi_9', 'fi_10', 'fi_11', 'fi_12', 'fi_13', 'fi_14', 'fi_15', 'fi_16', 'fi_17', 'fi_18', 'fi_19', 'fi_20', 'fi_21', 'fi_22', 'fi_23', 'fi_24', 'fi_25', 'fi_26', 'rsv_6', 'kdjk_6', 'rsv_7', 'kdjk_7', 'rsv_8', 'kdjk_8', 'rsv_9', 'kdjk_9', 'rsv_10', 'kdjk_10', 'rsv_11', 'kdjk_11', 'rsv_12', 'kdjk_12', 'rsv_13', 'kdjk_13', 'rsv_14', 'kdjk_14', 'rsv_15', 'kdjk_15', 'rsv_16', 'kdjk_16', 'rsv_17', 'kdjk_17', 'rsv_18', 'kdjk_18', 'rsv_19', 'kdjk_19', 'rsv_20', 'kdjk_20', 'rsv_21', 'kdjk_21', 'rsv_22', 'kdjk_22', 'rsv_23', 'kdjk_23', 'rsv_24', 'kdjk_24', 'rsv_25', 'kdjk_25', 'rsv_26', 'kdjk_26', 'eom_6', 'eom_7', 'eom_8', 'eom_9', 'eom_10', 'eom_11', 'eom_12', 'eom_13', 'eom_14', 'eom_15', 'eom_16', 'eom_17', 'eom_18', 'eom_19', 'eom_20', 'eom_21', 'eom_22', 'eom_23', 'eom_24', 'eom_25', 'eom_26')\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 215 216 217 218 219 220 221 222 223 224 225 226 227\n",
      " 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245\n",
      " 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263\n",
      " 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281\n",
      " 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299\n",
      " 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317\n",
      " 318 319 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356\n",
      " 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374\n",
      " 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392\n",
      " 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410\n",
      " 411 412 413 414 415 416 417 418 419 420 421 422 423 424]\n",
      "****************************************\n",
      "320 ('close', 'volume', 'rsi_6', 'rsi_7', 'rsi_8', 'rsi_9', 'rsi_10', 'rsi_11', 'rsi_12', 'rsi_13', 'rsi_14', 'rsi_15', 'rsi_16', 'rsi_17', 'rsi_18', 'rsi_19', 'rsi_20', 'rsi_21', 'rsi_22', 'rsi_24', 'rsi_26', 'wr_6', 'wr_7', 'wr_8', 'wr_9', 'wr_10', 'wr_11', 'wr_12', 'wr_13', 'wr_14', 'wr_15', 'wr_16', 'wr_17', 'wr_18', 'wr_19', 'wr_20', 'wr_21', 'wr_22', 'wr_23', 'wr_24', 'wr_25', 'wr_26', 'mfi_6', 'mfi_7', 'mfi_8', 'mfi_9', 'mfi_10', 'mfi_11', 'mfi_12', 'mfi_13', 'mfi_14', 'mfi_15', 'mfi_16', 'mfi_17', 'mfi_18', 'mfi_19', 'mfi_20', 'mfi_21', 'mfi_22', 'mfi_23', 'mfi_24', 'mfi_25', 'mfi_26', 'roc_6', 'roc_7', 'roc_8', 'roc_9', 'roc_10', 'roc_11', 'roc_12', 'roc_13', 'roc_14', 'roc_15', 'roc_16', 'roc_17', 'roc_18', 'roc_19', 'roc_20', 'roc_21', 'roc_22', 'roc_23', 'roc_24', 'roc_25', 'roc_26', 'cmf_6', 'cmf_7', 'cmf_8', 'cmf_9', 'cmf_10', 'cmf_11', 'cmf_12', 'cmf_13', 'cmf_14', 'cmf_15', 'cmf_16', 'cmf_17', 'cmf_18', 'cmf_19', 'cmf_20', 'cmf_22', 'cmf_23', 'cmf_24', 'cmf_25', 'cmf_26', 'cmo_6', 'cmo_7', 'cmo_8', 'cmo_9', 'cmo_10', 'cmo_11', 'cmo_12', 'cmo_13', 'cmo_14', 'cmo_15', 'cmo_16', 'cmo_17', 'cmo_18', 'cmo_19', 'cmo_20', 'cmo_21', 'cmo_22', 'cmo_23', 'cmo_25', 'wma_24', 'hma_0', 'hma_1', 'hma_8', 'hma_9', 'trix_6', 'trix_7', 'trix_9', 'trix_10', 'trix_13', 'trix_14', 'trix_16', 'trix_18', 'trix_21', 'trix_24', 'cci_6', 'cci_7', 'cci_8', 'cci_9', 'cci_10', 'cci_11', 'cci_12', 'cci_13', 'cci_14', 'cci_15', 'cci_16', 'cci_17', 'cci_18', 'cci_19', 'cci_20', 'cci_21', 'cci_22', 'cci_23', 'cci_24', 'cci_25', 'cci_26', 'dpo_6', 'dpo_7', 'dpo_8', 'dpo_9', 'dpo_10', 'dpo_11', 'dpo_12', 'dpo_13', 'dpo_14', 'dpo_17', 'dpo_19', 'dpo_21', 'dpo_23', 'dpo_24', 'dpo_25', 'dpo_26', 'kst_8', 'kst_9', 'kst_10', 'kst_11', 'kst_12', 'kst_13', 'kst_14', 'kst_15', 'kst_16', 'kst_17', 'kst_18', 'kst_19', 'kst_20', 'kst_21', 'kst_22', 'kst_23', 'kst_24', 'kst_25', 'kst_26', 'dmi_6', 'dmi_7', 'dmi_8', 'dmi_9', 'dmi_10', 'dmi_11', 'dmi_12', 'dmi_13', 'dmi_14', 'dmi_15', 'dmi_16', 'dmi_17', 'dmi_18', 'dmi_19', 'dmi_20', 'dmi_21', 'dmi_22', 'dmi_23', 'dmi_24', 'dmi_25', 'dmi_26', 'bb_6', 'bb_7', 'bb_8', 'bb_9', 'bb_10', 'bb_11', 'bb_12', 'bb_13', 'bb_14', 'bb_15', 'bb_16', 'bb_17', 'bb_18', 'bb_19', 'bb_20', 'bb_21', 'bb_22', 'bb_23', 'bb_24', 'bb_25', 'bb_26', 'fi_6', 'fi_7', 'fi_8', 'fi_9', 'fi_10', 'fi_11', 'fi_12', 'fi_13', 'fi_14', 'fi_15', 'fi_16', 'fi_17', 'fi_18', 'fi_19', 'fi_20', 'fi_21', 'fi_22', 'fi_23', 'fi_24', 'fi_25', 'fi_26', 'rsv_6', 'kdjk_6', 'rsv_7', 'kdjk_7', 'rsv_8', 'kdjk_8', 'rsv_9', 'kdjk_9', 'rsv_10', 'kdjk_10', 'rsv_11', 'kdjk_11', 'rsv_12', 'kdjk_12', 'rsv_13', 'kdjk_13', 'rsv_14', 'kdjk_14', 'rsv_15', 'kdjk_15', 'rsv_16', 'kdjk_16', 'rsv_17', 'kdjk_17', 'rsv_18', 'kdjk_18', 'rsv_19', 'kdjk_19', 'rsv_20', 'kdjk_20', 'rsv_21', 'kdjk_21', 'rsv_22', 'kdjk_22', 'rsv_23', 'kdjk_23', 'rsv_24', 'kdjk_24', 'rsv_25', 'kdjk_25', 'rsv_26', 'kdjk_26', 'eom_6', 'eom_7', 'eom_8', 'eom_9', 'eom_10', 'eom_11', 'eom_12', 'eom_13', 'eom_14', 'eom_15', 'eom_16', 'eom_17', 'eom_18', 'eom_19', 'eom_20', 'eom_21', 'eom_22', 'eom_23', 'eom_24', 'eom_25', 'eom_26')\n",
      "[  3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20\n",
      "  21  23  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40\n",
      "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
      "  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76\n",
      "  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94\n",
      "  95  96  97  98  99 100 101 102 103 105 106 107 108 109 110 111 112 113\n",
      " 114 115 116 117 118 119 120 121 122 123 124 125 126 127 129 191 194 195\n",
      " 202 203 215 216 218 219 222 223 225 227 230 233 236 237 238 239 240 241\n",
      " 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259\n",
      " 260 261 262 263 264 265 268 270 272 274 275 276 277 280 281 282 283 284\n",
      " 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302\n",
      " 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320\n",
      " 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338\n",
      " 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356\n",
      " 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374\n",
      " 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392\n",
      " 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410\n",
      " 411 412 413 414 415 416 417 418 419 420 421 422 423 424]\n",
      "CPU times: user 10.8 s, sys: 170 ms, total: 11 s\n",
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from operator import itemgetter\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "if selection_method == 'anova' or selection_method == 'all':\n",
    "    select_k_best = SelectKBest(f_classif, k=topk)\n",
    "    if selection_method != 'all':\n",
    "        x_train = select_k_best.fit_transform(x_main, y_train)\n",
    "        x_cv = select_k_best.transform(x_cv)\n",
    "        x_test = select_k_best.transform(x_test)\n",
    "    else:\n",
    "        select_k_best.fit(x_main, y_train)\n",
    "    \n",
    "    selected_features_anova = itemgetter(*select_k_best.get_support(indices=True))(list_features)\n",
    "    print(selected_features_anova)\n",
    "    print(select_k_best.get_support(indices=True))\n",
    "    print(\"****************************************\")\n",
    "    \n",
    "if selection_method == 'mutual_info' or selection_method == 'all':\n",
    "    select_k_best = SelectKBest(mutual_info_classif, k=topk)\n",
    "    if selection_method != 'all':\n",
    "        x_train = select_k_best.fit_transform(x_main, y_train)\n",
    "        x_cv = select_k_best.transform(x_cv)\n",
    "        x_test = select_k_best.transform(x_test)\n",
    "    else:\n",
    "        select_k_best.fit(x_main, y_train)\n",
    "\n",
    "    selected_features_mic = itemgetter(*select_k_best.get_support(indices=True))(list_features)\n",
    "    print(len(selected_features_mic), selected_features_mic)\n",
    "    print(select_k_best.get_support(indices=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common selected featues 294 ['mfi_7', 'dpo_26', 'rsi_17', 'kst_15', 'rsv_24', 'trix_21', 'fi_15', 'cmf_23', 'fi_11', 'fi_12', 'eom_7', 'eom_19', 'kst_20', 'rsi_22', 'cci_25', 'rsv_25', 'rsv_22', 'mfi_11', 'dpo_8', 'cmo_25', 'eom_11', 'trix_7', 'dpo_10', 'dpo_13', 'kdjk_11', 'rsv_14', 'cci_11', 'dpo_11', 'cmf_8', 'cmf_17', 'roc_8', 'mfi_23', 'cci_12', 'fi_19', 'kdjk_18', 'mfi_26', 'roc_6', 'roc_16', 'rsi_15', 'cci_15', 'trix_13', 'fi_8', 'roc_10', 'volume', 'fi_25', 'roc_12', 'trix_18', 'fi_13', 'eom_9', 'mfi_15', 'fi_9', 'cmf_19', 'wr_12', 'wr_14', 'dmi_14', 'trix_6', 'kdjk_15', 'eom_12', 'kdjk_17', 'eom_20', 'rsv_10', 'dmi_24', 'kdjk_23', 'rsi_8', 'eom_22', 'roc_9', 'cci_10', 'mfi_14', 'roc_11', 'cmf_13', 'kst_10', 'dmi_11', 'roc_13', 'kst_22', 'rsv_20', 'cmo_13', 'cci_6', 'dmi_6', 'roc_24', 'eom_24', 'eom_25', 'wr_7', 'roc_20', 'cmo_10', 'dmi_26', 'dmi_22', 'fi_7', 'rsv_7', 'mfi_12', 'cci_22', 'dpo_19', 'cmf_11', 'dpo_25', 'fi_23', 'mfi_13', 'dpo_21', 'rsv_16', 'dmi_12', 'rsv_19', 'kst_19', 'kst_11', 'kdjk_7', 'mfi_20', 'kst_23', 'cmf_12', 'kdjk_16', 'dmi_16', 'dmi_8', 'mfi_18', 'kst_14', 'kst_24', 'rsv_13', 'kst_13', 'trix_16', 'cci_8', 'roc_26', 'dmi_18', 'rsi_12', 'cci_7', 'wr_10', 'dpo_12', 'kst_16', 'dmi_17', 'kdjk_22', 'fi_24', 'kst_8', 'dmi_20', 'mfi_16', 'rsv_15', 'wr_17', 'cmo_12', 'rsi_16', 'wr_22', 'kdjk_6', 'kdjk_24', 'roc_22', 'cmf_25', 'cmo_6', 'wr_26', 'cci_26', 'mfi_17', 'cci_19', 'rsi_14', 'rsi_10', 'cmo_19', 'roc_18', 'cci_24', 'rsi_18', 'roc_15', 'cci_13', 'dpo_24', 'eom_13', 'cmo_11', 'eom_18', 'rsv_21', 'fi_10', 'rsv_6', 'mfi_25', 'roc_14', 'roc_23', 'dpo_9', 'wr_20', 'kst_9', 'rsi_9', 'wr_18', 'rsi_21', 'fi_22', 'eom_16', 'cmf_15', 'cci_23', 'dpo_7', 'rsi_6', 'dmi_23', 'rsv_26', 'fi_14', 'cmf_10', 'mfi_19', 'wr_23', 'roc_17', 'wr_11', 'cmf_6', 'kst_17', 'wr_15', 'rsi_19', 'cci_21', 'kdjk_25', 'fi_17', 'rsi_20', 'close', 'wr_21', 'wr_25', 'rsi_13', 'roc_19', 'cci_16', 'kst_18', 'eom_21', 'cmo_9', 'cmo_14', 'dmi_15', 'rsv_8', 'dmi_10', 'trix_9', 'cmo_20', 'kdjk_9', 'roc_7', 'fi_20', 'eom_26', 'dmi_19', 'rsi_24', 'kdjk_13', 'mfi_8', 'kdjk_12', 'cmo_16', 'dpo_23', 'cmf_24', 'fi_16', 'rsi_26', 'kdjk_19', 'wr_6', 'fi_26', 'wr_8', 'dmi_13', 'eom_14', 'mfi_10', 'wr_9', 'wr_24', 'cmo_8', 'cmo_17', 'cmf_7', 'mfi_24', 'kst_21', 'eom_15', 'kst_25', 'fi_6', 'cmo_18', 'kst_26', 'fi_18', 'wr_16', 'dmi_9', 'cmf_26', 'mfi_6', 'kdjk_26', 'mfi_9', 'cmo_22', 'dpo_14', 'cci_9', 'dmi_25', 'kdjk_20', 'dmi_21', 'cmf_16', 'eom_6', 'eom_8', 'trix_14', 'kdjk_8', 'rsv_17', 'trix_24', 'wr_19', 'mfi_22', 'cmo_23', 'cmf_9', 'cmf_18', 'cmo_15', 'cmo_21', 'rsv_18', 'roc_25', 'eom_17', 'rsv_9', 'rsv_11', 'cci_14', 'wr_13', 'mfi_21', 'rsi_11', 'cmf_22', 'cmo_7', 'cmf_14', 'cci_17', 'dmi_7', 'dpo_6', 'kdjk_10', 'kdjk_21', 'cci_18', 'rsv_23', 'eom_10', 'eom_23', 'rsi_7', 'cci_20', 'rsv_12', 'cmf_20', 'kdjk_14', 'roc_21', 'kst_12', 'fi_21', 'dpo_17', 'trix_10']\n",
      "[3, 4, 5, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 37, 38, 40, 41, 42, 43, 45, 46, 48, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 88, 89, 91, 93, 94, 95, 96, 98, 100, 102, 106, 107, 108, 110, 113, 114, 115, 116, 117, 118, 120, 123, 124, 129, 215, 216, 218, 222, 225, 227, 230, 236, 237, 238, 240, 241, 242, 243, 245, 246, 249, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 270, 272, 274, 275, 276, 277, 280, 281, 282, 283, 285, 286, 287, 288, 289, 290, 291, 292, 294, 295, 296, 299, 301, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 319, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 354, 355, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 369, 370, 373, 375, 376, 377, 378, 380, 381, 382, 383, 385, 387, 388, 389, 390, 392, 394, 395, 397, 398, 399, 400, 401, 402, 405, 407, 409, 410, 411, 412, 414, 416, 417, 418, 419, 420, 422, 423, 424]\n"
     ]
    }
   ],
   "source": [
    "if selection_method == 'all':\n",
    "    common = list(set(selected_features_anova).intersection(selected_features_mic))\n",
    "    print(\"common selected featues\", len(common), common)\n",
    "    if len(common) < num_features:\n",
    "        raise Exception('number of common features found {} < {} required features. Increase \"topk variable\"'.format(len(common), num_features))\n",
    "    feat_idx = []\n",
    "    for c in common:\n",
    "        feat_idx.append(list_features.index(c))\n",
    "    feat_idx = sorted(feat_idx[0:225])\n",
    "    print(feat_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x, y train/cv/test (2721, 225) (2721,) (681, 225) (681,) (851, 225) (851,)\n"
     ]
    }
   ],
   "source": [
    "if selection_method == 'all':\n",
    "    x_train = x_train[:, feat_idx]\n",
    "    x_cv = x_cv[:, feat_idx]\n",
    "    x_test = x_test[:, feat_idx]\n",
    "\n",
    "print(\"Shape of x, y train/cv/test {} {} {} {} {} {}\".format(x_train.shape, \n",
    "                                                             y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of class 0 = 5.990444689452407, class 1 = 5.806688717383315\n"
     ]
    }
   ],
   "source": [
    "_labels, _counts = np.unique(y_train, return_counts=True)\n",
    "print(\"percentage of class 0 = {}, class 1 = {}\".format(_counts[0]/len(y_train) * 100, _counts[1]/len(y_train) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "def get_sample_weights(y):\n",
    "    \"\"\"\n",
    "    calculate the sample weights based on class weights. Used for models with\n",
    "    imbalanced data and one hot encoding prediction.\n",
    "\n",
    "    params:\n",
    "        y: class labels as integers\n",
    "    \"\"\"\n",
    "\n",
    "    y = y.astype(int)  # compute_class_weight needs int labels\n",
    "    class_weights = compute_class_weight('balanced', np.unique(y), y)\n",
    "    \n",
    "    print(\"real class weights are {}\".format(class_weights), np.unique(y))\n",
    "    print(\"value_counts\", np.unique(y, return_counts=True))\n",
    "    sample_weights = y.copy().astype(float)\n",
    "    for i in np.unique(y):\n",
    "        sample_weights[sample_weights == i] = class_weights[i]  # if i == 2 else 0.8 * class_weights[i]\n",
    "        # sample_weights = np.where(sample_weights == i, class_weights[int(i)], y_)\n",
    "\n",
    "    return sample_weights\n",
    "\n",
    "def reshape_as_image(x, img_width, img_height):\n",
    "    x_temp = np.zeros((len(x), img_height, img_width))\n",
    "    for i in range(x.shape[0]):\n",
    "        # print(type(x), type(x_temp), x.shape)\n",
    "        x_temp[i] = np.reshape(x[i], (img_height, img_width))\n",
    "\n",
    "    return x_temp\n",
    "\n",
    "def f1_weighted(y_true, y_pred):\n",
    "    y_true_class = tf.math.argmax(y_true, axis=1, output_type=tf.dtypes.int32)\n",
    "    y_pred_class = tf.math.argmax(y_pred, axis=1, output_type=tf.dtypes.int32)\n",
    "    conf_mat = tf.math.confusion_matrix(y_true_class, y_pred_class)  # can use conf_mat[0, :], tf.slice()\n",
    "    # precision = TP/TP+FP, recall = TP/TP+FN\n",
    "    rows, cols = conf_mat.get_shape()\n",
    "    size = y_true_class.get_shape()[0]\n",
    "    precision = tf.constant([0, 0, 0])  # change this to use rows/cols as size\n",
    "    recall = tf.constant([0, 0, 0])\n",
    "    class_counts = tf.constant([0, 0, 0])\n",
    "\n",
    "    def get_precision(i, conf_mat):\n",
    "        print(\"prec check\", conf_mat, conf_mat[i, i], tf.reduce_sum(conf_mat[:, i]))\n",
    "        precision[i].assign(conf_mat[i, i] / tf.reduce_sum(conf_mat[:, i]))\n",
    "        recall[i].assign(conf_mat[i, i] / tf.reduce_sum(conf_mat[i, :]))\n",
    "        tf.add(i, 1)\n",
    "        return i, conf_mat, precision, recall\n",
    "\n",
    "    def tf_count(i):\n",
    "        elements_equal_to_value = tf.equal(y_true_class, i)\n",
    "        as_ints = tf.cast(elements_equal_to_value, tf.int32)\n",
    "        count = tf.reduce_sum(as_ints)\n",
    "        class_counts[i].assign(count)\n",
    "        tf.add(i, 1)\n",
    "        return count\n",
    "\n",
    "    def condition(i, conf_mat):\n",
    "        return tf.less(i, 3)\n",
    "\n",
    "    i = tf.constant(3)\n",
    "    i, conf_mat = tf.while_loop(condition, get_precision, [i, conf_mat])\n",
    "\n",
    "    i = tf.constant(3)\n",
    "    c = lambda i: tf.less(i, 3)\n",
    "    b = tf_count(i)\n",
    "    tf.while_loop(c, b, [i])\n",
    "\n",
    "    weights = tf.math.divide(class_counts, size)\n",
    "    numerators = tf.math.multiply(tf.math.multiply(precision, recall), tf.constant(2))\n",
    "    denominators = tf.math.add(precision, recall)\n",
    "    f1s = tf.math.divide(numerators, denominators)\n",
    "    weighted_f1 = tf.reduce_sum(f.math.multiply(f1s, weights))\n",
    "    return weighted_f1\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    this calculates precision & recall \n",
    "    \"\"\"\n",
    "\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))  # mistake: y_pred of 0.3 is also considered 1\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    # y_true_class = tf.math.argmax(y_true, axis=1, output_type=tf.dtypes.int32)\n",
    "    # y_pred_class = tf.math.argmax(y_pred, axis=1, output_type=tf.dtypes.int32)\n",
    "    # conf_mat = tf.math.confusion_matrix(y_true_class, y_pred_class)\n",
    "    # tf.Print(conf_mat, [conf_mat], \"confusion_matrix\")\n",
    "\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "get_custom_objects().update({\"f1_metric\": f1_metric, \"f1_weighted\": f1_weighted})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real class weights are [5.56441718 5.74050633 0.37791667] [0 1 2]\n",
      "value_counts (array([0, 1, 2]), array([ 163,  158, 2400]))\n",
      "Test sample_weights\n",
      "[2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 0]\n",
      "[0.37791667 0.37791667 0.37791667 0.37791667 5.56441718 0.37791667\n",
      " 0.37791667 0.37791667 0.37791667 0.37791667 0.37791667 0.37791667\n",
      " 0.37791667 0.37791667 0.37791667 0.37791667 0.37791667 0.37791667\n",
      " 0.37791667 0.37791667 0.37791667 5.56441718 0.37791667 0.37791667\n",
      " 0.37791667 0.37791667 0.37791667 0.37791667 0.37791667 5.56441718]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass classes=[0 1 2], y=[0 1 2 ... 2 2 2] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "sample_weights = get_sample_weights(y_train)\n",
    "print(\"Test sample_weights\")\n",
    "rand_idx = np.random.randint(0, 1000, 30)\n",
    "print(y_train[rand_idx])\n",
    "print(sample_weights[rand_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train (2721, 3)\n"
     ]
    }
   ],
   "source": [
    "one_hot_enc = OneHotEncoder(sparse=False, categories='auto')  # , categories='auto'\n",
    "y_train = one_hot_enc.fit_transform(y_train.reshape(-1, 1))\n",
    "print(\"y_train\",y_train.shape)\n",
    "y_cv = one_hot_enc.transform(y_cv.reshape(-1, 1))\n",
    "y_test = one_hot_enc.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final shape of x, y train/test (2721, 15, 15, 3) (2721, 3) (851, 15, 15, 3) (851, 3)\n"
     ]
    }
   ],
   "source": [
    "dim = int(np.sqrt(num_features))\n",
    "x_train = reshape_as_image(x_train, dim, dim)\n",
    "x_cv = reshape_as_image(x_cv, dim, dim)\n",
    "x_test = reshape_as_image(x_test, dim, dim)\n",
    "# adding a 1-dim for channels (3)\n",
    "x_train = np.stack((x_train,) * 3, axis=-1)\n",
    "x_test = np.stack((x_test,) * 3, axis=-1)\n",
    "x_cv = np.stack((x_cv,) * 3, axis=-1)\n",
    "print(\"final shape of x, y train/test {} {} {} {}\".format(x_train.shape, y_train.shape, x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "columns = rows = 3\n",
    "for i in range(1, columns*rows +1):\n",
    "    index = np.random.randint(len(x_train))\n",
    "    img = x_train[index]\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title('image_'+str(index)+'_class_'+str(np.argmax(y_train[index])), fontsize=10)\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, LeakyReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger, Callback\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from tensorflow.keras.initializers import RandomUniform, RandomNormal\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "params = {'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.2, 'conv2d_filters_1': 32, 'conv2d_kernel_size_1': 3, 'conv2d_mp_1': 0, \n",
    "                                               'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.0, 'conv2d_do_2': 0.3, \n",
    "                                               'conv2d_filters_2': 64, 'conv2d_kernel_size_2': 3, 'conv2d_mp_2': 2, 'conv2d_strides_2': 1, \n",
    "                                               'kernel_regularizer_2': 0.0, 'layers': 'two'}, \n",
    "           'dense_layers': {'dense_do_1': 0.3, 'dense_nodes_1': 128, 'kernel_regularizer_1': 0.0, 'layers': 'one'},\n",
    "           'epochs': 50, 'lr': 0.001, 'optimizer': 'adam'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import *\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "def f1_custom(y_true, y_pred):\n",
    "    y_t = np.argmax(y_true, axis=1)\n",
    "    y_p = np.argmax(y_pred, axis=1)\n",
    "    f1_score(y_t, y_p, labels=None, average='weighted', sample_weight=None, zero_division='warn')\n",
    "\n",
    "def create_model_cnn(params):\n",
    "    model = Sequential()\n",
    "\n",
    "    print(\"Training with params {}\".format(params))\n",
    "    \n",
    "    conv2d_layer1 = Conv2D(params[\"conv2d_layers\"][\"conv2d_filters_1\"],\n",
    "                           params[\"conv2d_layers\"][\"conv2d_kernel_size_1\"],\n",
    "                           strides=params[\"conv2d_layers\"][\"conv2d_strides_1\"],\n",
    "                           kernel_regularizer=regularizers.l2(params[\"conv2d_layers\"][\"kernel_regularizer_1\"]), \n",
    "                           padding='same',activation=\"relu\", use_bias=True,\n",
    "                           kernel_initializer='glorot_uniform',\n",
    "                           input_shape=(x_train[0].shape[0],\n",
    "                                        x_train[0].shape[1], x_train[0].shape[2]))\n",
    "    model.add(conv2d_layer1)\n",
    "    if params[\"conv2d_layers\"]['conv2d_mp_1'] > 1:\n",
    "        model.add(MaxPool2D(pool_size=params[\"conv2d_layers\"]['conv2d_mp_1']))\n",
    "        \n",
    "    model.add(Dropout(params['conv2d_layers']['conv2d_do_1']))\n",
    "    if params[\"conv2d_layers\"]['layers'] == 'two':\n",
    "        conv2d_layer2 = Conv2D(params[\"conv2d_layers\"][\"conv2d_filters_2\"],\n",
    "                               params[\"conv2d_layers\"][\"conv2d_kernel_size_2\"],\n",
    "                               strides=params[\"conv2d_layers\"][\"conv2d_strides_2\"],\n",
    "                               kernel_regularizer=regularizers.l2(params[\"conv2d_layers\"][\"kernel_regularizer_2\"]),\n",
    "                               padding='same',activation=\"relu\", use_bias=True,\n",
    "                               kernel_initializer='glorot_uniform')\n",
    "        model.add(conv2d_layer2)\n",
    "        \n",
    "        if params[\"conv2d_layers\"]['conv2d_mp_2'] > 1:\n",
    "            model.add(MaxPool2D(pool_size=params[\"conv2d_layers\"]['conv2d_mp_2']))\n",
    "        \n",
    "        model.add(Dropout(params['conv2d_layers']['conv2d_do_2']))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(params['dense_layers'][\"dense_nodes_1\"], activation='relu'))\n",
    "    model.add(Dropout(params['dense_layers']['dense_do_1']))\n",
    "\n",
    "    if params['dense_layers'][\"layers\"] == 'two':\n",
    "        model.add(Dense(params['dense_layers'][\"dense_nodes_2\"], activation='relu', \n",
    "                        kernel_regularizer=params['dense_layers'][\"kernel_regularizer_1\"]))\n",
    "        model.add(Dropout(params['dense_layers']['dense_do_2']))\n",
    "\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    if params[\"optimizer\"] == 'rmsprop':\n",
    "        optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "    elif params[\"optimizer\"] == 'sgd':\n",
    "        optimizer = optimizers.SGD(lr=params[\"lr\"], decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    elif params[\"optimizer\"] == 'adam':\n",
    "        optimizer = optimizers.Adam(learning_rate=params[\"lr\"], beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', f1_metric])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def check_baseline(pred, y_test):\n",
    "    print(\"size of test set\", len(y_test))\n",
    "    e = np.equal(pred, y_test)\n",
    "    print(\"TP class counts\", np.unique(y_test[e], return_counts=True))\n",
    "    print(\"True class counts\", np.unique(y_test, return_counts=True))\n",
    "    print(\"Pred class counts\", np.unique(pred, return_counts=True))\n",
    "    holds = np.unique(y_test, return_counts=True)[1][2]  # number 'hold' predictions\n",
    "    print(\"baseline acc:\", (holds/len(y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import plot_model\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "model = create_model_cnn(params)\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=False)\n",
    "\n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "best_model_path = os.path.join('.', 'best_model_keras')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                   patience=100, min_delta=0.0001)\n",
    "# csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'log_training_batch.log'), append=True)\n",
    "rlp = ReduceLROnPlateau(monitor='val_loss', factor=0.02, patience=20, verbose=1, mode='min',\n",
    "                        min_delta=0.001, cooldown=1, min_lr=0.0001)\n",
    "mcp = ModelCheckpoint(best_model_path, monitor='val_f1_metric', verbose=1,\n",
    "                      save_best_only=True, save_weights_only=False, mode='max', period=1)  # val_f1_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(x_train, y_train, epochs=params['epochs'], verbose=1,\n",
    "                            batch_size=64, shuffle=True,\n",
    "                            # validation_split=0.3,\n",
    "                            validation_data=(x_cv, y_cv),\n",
    "                            callbacks=[mcp, rlp, es]\n",
    "                            , sample_weight=sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['f1_metric'])\n",
    "plt.plot(history.history['val_f1_metric'])\n",
    "\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train_loss', 'val_loss', 'f1', 'val_f1'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "\n",
    "model = load_model(best_model_path)\n",
    "test_res = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"keras evaluate=\", test_res)\n",
    "pred = model.predict(x_test)\n",
    "pred_classes = np.argmax(pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "check_baseline(pred_classes, y_test_classes)\n",
    "conf_mat = confusion_matrix(y_test_classes, pred_classes)\n",
    "print(conf_mat)\n",
    "labels = [0,1,2]\n",
    "\n",
    "f1_weighted = f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='weighted', sample_weight=None)\n",
    "print(\"F1 score (weighted)\", f1_weighted)\n",
    "print(\"F1 score (macro)\", f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='macro', sample_weight=None))\n",
    "print(\"F1 score (micro)\", f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='micro', sample_weight=None))  # weighted and micro preferred in case of imbalance\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-s-kappa --> supports multiclass; ref: https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english\n",
    "print(\"cohen's Kappa\", cohen_kappa_score(y_test_classes, pred_classes))\n",
    "\n",
    "recall = []\n",
    "for i, row in enumerate(conf_mat):\n",
    "    recall.append(np.round(row[i]/np.sum(row), 2))\n",
    "    print(\"Recall of class {} = {}\".format(i, recall[i]))\n",
    "print(\"Recall avg\", sum(recall)/len(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda uninstall pydot\n",
    "# !conda uninstall pydotplus\n",
    "# !conda uninstall graphviz\n",
    "\n",
    "#!conda install pydot\n",
    "#!conda install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
